[
  {
    "objectID": "lectures/01/01-welcome.html#teaching-team",
    "href": "lectures/01/01-welcome.html#teaching-team",
    "title": "Welcome to STA 101",
    "section": "Teaching team",
    "text": "Teaching team\n\n\nInstructor\nDr. Mine Çetinkaya-Rundel\nOld Chem 213\nmc301@duke.edu\n\nTeaching assistants\n\n\n\nShuo Wang\nHead + Lab TA\n\n\nSylvia Vincent\nLab TA\n\n\nJohn Gillen\nLab TA\n\n\nChris Oswald\nLab TA\n\n\nMinh Anh To\nTA\n\n\nHao Wang\nTA\n\n\nNoah Obuya\nTA\n\n\nMeghna Katyal\nTA\n\n\nAvery Hodges\nTA"
  },
  {
    "objectID": "lectures/01/01-welcome.html#timetable",
    "href": "lectures/01/01-welcome.html#timetable",
    "title": "Welcome to STA 101",
    "section": "Timetable",
    "text": "Timetable\n\nLectures at Gross Hall 103: Mon + Wed 1:25 - 2:40 pm\nLabs at Perkins LINK 087 (Classroom 3)\n\nLab 1: Fri 8:30 - 9:45 am\nLab 2: Fri 10:05 - 11:20 am\nLab 3: Fri 11:45 am - 1:00 pm\nLab 4: Fri 1:25 - 2:40 pm"
  },
  {
    "objectID": "lectures/01/01-welcome.html#learning-objectives",
    "href": "lectures/01/01-welcome.html#learning-objectives",
    "title": "Welcome to STA 101",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nRecognize the importance of data collection, identify limitations in data collection methods, and determine how they affect the scope of inference.\nUse statistical software to summarize data numerically and visually, and to perform data analysis.\nHave a conceptual understanding of the unified nature of statistical inference.\nApply estimation and testing methods to analyze single variables or the relationship between two variables in order to understand natural phenomena and make data-based decisions.\nModel numerical response variables using a single or multiple explanatory variables.\nInterpret results correctly, effectively, and in context without relying on statistical jargon.\nCritique data-based claims and evaluate data-based decisions.\nComplete research projects demonstrating mastery of statistical data analysis from exploratory analysis to inference to modeling."
  },
  {
    "objectID": "lectures/01/01-welcome.html#lets-play-a-game",
    "href": "lectures/01/01-welcome.html#lets-play-a-game",
    "title": "Welcome to STA 101",
    "section": "Let’s play a game!",
    "text": "Let’s play a game!\n\n\nForm a small group (2-4 people) with people sitting around you\nFirst, introduce yourselves to each other – name (and proper pronunciation of name), year, major, where are you from, etc.\nPlay the game: https://nyti.ms/3suUJHG"
  },
  {
    "objectID": "lectures/01/01-welcome.html#course-website",
    "href": "lectures/01/01-welcome.html#course-website",
    "title": "Welcome to STA 101",
    "section": "Course website",
    "text": "Course website\n\nsta101-f23.github.io\n\n\naka “the one link to rule them all”"
  },
  {
    "objectID": "lectures/01/01-welcome.html#lectures",
    "href": "lectures/01/01-welcome.html#lectures",
    "title": "Welcome to STA 101",
    "section": "Lectures",
    "text": "Lectures\n\nIn person\nAttendance is required (as long as you’re healthy!)\nA little bit of everything:\n\nTraditional lecture\nLive coding + demos\nShort exercises + solution discussion\n\nRecordings will be posted after class – to be used for review + make-up if you can’t make it to class due to health reasons, they’re not an alternative to class attendance"
  },
  {
    "objectID": "lectures/01/01-welcome.html#labs",
    "href": "lectures/01/01-welcome.html#labs",
    "title": "Welcome to STA 101",
    "section": "Labs",
    "text": "Labs\n\nAttendance is required (as long as you’re healthy!)\nOpportunity to work on course assignments with TA support\nOpportunity to work with teammates on projects"
  },
  {
    "objectID": "lectures/01/01-welcome.html#announcements",
    "href": "lectures/01/01-welcome.html#announcements",
    "title": "Welcome to STA 101",
    "section": "Announcements",
    "text": "Announcements\n\nPosted on Canvas (Announcements) and sent via email, be sure to check both regularly\nI’ll assume that you’ve read an announcement by the next “business” day"
  },
  {
    "objectID": "lectures/01/01-welcome.html#diversity-and-inclusion",
    "href": "lectures/01/01-welcome.html#diversity-and-inclusion",
    "title": "Welcome to STA 101",
    "section": "Diversity and inclusion",
    "text": "Diversity and inclusion\nIt is my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength and benefit.\n\nIf you have a name that differs from those that appear in your official Duke records, please let me know! Add your name pronunciation to your Canvas and Slack profiles.\nPlease let me know your preferred pronouns and add these to your Canvas and Slack profiles.\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don’t hesitate to come and talk with me. I want to be a resource for you. If you prefer to speak with someone outside of the course, your advisers and deans are excellent resources.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to me about it."
  },
  {
    "objectID": "lectures/01/01-welcome.html#accessibility",
    "href": "lectures/01/01-welcome.html#accessibility",
    "title": "Welcome to STA 101",
    "section": "Accessibility",
    "text": "Accessibility\n\nThe Student Disability Access Office (SDAO) is available to ensure that students are able to engage with their courses and related assignments.\nWe will have in class exams. If you need special accommodations, please book the testing center ASAP!\nI am committed to making all course materials accessible and I’m always learning how to do this better. If any course component is not accessible to you in any way, please don’t hesitate to let me know."
  },
  {
    "objectID": "lectures/01/01-welcome.html#attendance-participation-5",
    "href": "lectures/01/01-welcome.html#attendance-participation-5",
    "title": "Welcome to STA 101",
    "section": "Attendance + participation (5%)",
    "text": "Attendance + participation (5%)\n\nRequired throughout the semester in lecture and lab\nStudents who attend at least 80% of the lectures and participate regularly in lecture and/or other course venues (lab + Slack) will receive full credit for this portion of their grade\nParticipation in labs as well as on Slack will also count towards this component\n\n\n\n\n\n\n\nTip\n\n\nIf you attend at least 80% of the classes, you’ll get all available points for this component."
  },
  {
    "objectID": "lectures/01/01-welcome.html#interactive-tutorials-5",
    "href": "lectures/01/01-welcome.html#interactive-tutorials-5",
    "title": "Welcome to STA 101",
    "section": "Interactive tutorials (5%)",
    "text": "Interactive tutorials (5%)\n\nOnline, individual, can discuss with classmates\nCover reading that is due since the previous quiz and up to and including the deadline for the given quiz\nMake sure to fill out your name and Net ID prior to generating the hash to submit (more info on this coming soon)\nDue by 5 pm ET (on the indicated day on the course schedule\n\n\n\n\n\n\n\nTip\n\n\nIf you complete at least 80% of the tutorials, you’ll get all available points for this component."
  },
  {
    "objectID": "lectures/01/01-welcome.html#labs-25",
    "href": "lectures/01/01-welcome.html#labs-25",
    "title": "Welcome to STA 101",
    "section": "Labs (25%)",
    "text": "Labs (25%)\n\nSubmitted on Gradescope, individual, can discuss with classmates\nLab sessions allocated to working on assignments and getting feedback from TAs\nDue by 5 pm ET on the indicated day on the course schedule\nWeekly deadlines to keep you on track, hard deadlines by exams or end of class\n\n\n\n\n\n\n\nTip\n\n\nLowest lab score is dropped, whether it’s an actual low score or a 0 from not turning it in."
  },
  {
    "objectID": "lectures/01/01-welcome.html#exams",
    "href": "lectures/01/01-welcome.html#exams",
    "title": "Welcome to STA 101",
    "section": "Exams",
    "text": "Exams\n\nTwo exams, each 20%\nEach exam comprised of two parts:\n\nIn class: 75 minute in-class exam. Closed book, one sheet of notes (“cheat sheet”, no larger than 8 1/2 x 11, both sides, must be prepared by you) – 70% of the grade\nTake home: 48 hours to complete the take home portion. The take home portion will follow from the in class exam and focus on the analysis of a dataset introduced in the take home exam – 30% of the grade\n\n\n\n\n\n\n\n\nCaution\n\n\nExam dates cannot be changed and no make-up exams will be given. If you can’t take the exams on these dates, you should drop this class."
  },
  {
    "objectID": "lectures/01/01-welcome.html#projects",
    "href": "lectures/01/01-welcome.html#projects",
    "title": "Welcome to STA 101",
    "section": "Projects",
    "text": "Projects\n\nProject 1 (10%)\n\nSame data, regression\nWrite-up only\n\nProject 2 (15%)\n\nDataset of your choice, method of your choice\nNew team\nPresentation and write-up\nPresentations on the final exam date\n\nInterim deadlines, peer review on content, peer evaluation for team contribution\nSome lab sessions allocated to working on projects, doing peer review, getting feedback from TAs\n\n\n\n\n\n\n\nCaution\n\n\nFinal presentation date cannot be changed. If you can’t present on that date, you should drop this class."
  },
  {
    "objectID": "lectures/01/01-welcome.html#teams",
    "href": "lectures/01/01-welcome.html#teams",
    "title": "Welcome to STA 101",
    "section": "Teams",
    "text": "Teams\n\nTeamwork\n\nProjects (required), in class exercises (recommended)\nAssigned different teams for each project\nPeer evaluation during teamwork and after completion\n\nExpectations and roles\n\nEveryone is expected to contribute equal effort\nEveryone is expected to understand all code turned in\nIndividual contribution evaluated by peer evaluation, commits, etc."
  },
  {
    "objectID": "lectures/01/01-welcome.html#covid-policies",
    "href": "lectures/01/01-welcome.html#covid-policies",
    "title": "Welcome to STA 101",
    "section": "COVID policies",
    "text": "COVID policies\n\nWear a mask if the university requires\nStay home if you’re sick and follow guidance\nRead and follow university guidance"
  },
  {
    "objectID": "lectures/01/01-welcome.html#late-work-policy",
    "href": "lectures/01/01-welcome.html#late-work-policy",
    "title": "Welcome to STA 101",
    "section": "Late work policy",
    "text": "Late work policy\n\nInteractive tutorials: Late submissions past the hard deadlines not accepted\nLabs:\n\nLate, but within 24 hours of deadline: -20% of available points\nAny later: No credit, and we will not provide written feedback\nNote that lowest lab score will be dropped, even if that score is a 0\n\nProject write-ups:\n\nLate, but within 24 hours of deadline: -20% of available points\nAny later: No credit, and we will not provide written feedback\nTwo days late or later: No credit, and we will not provide written feedback\n\nProject presentation: Late submissions not accepted\nPeer evaluation:\n\nLate submissions not accepted\nMust turn in peer evaluation if you want your own score from others"
  },
  {
    "objectID": "lectures/01/01-welcome.html#collaboration-policy",
    "href": "lectures/01/01-welcome.html#collaboration-policy",
    "title": "Welcome to STA 101",
    "section": "Collaboration policy",
    "text": "Collaboration policy\n\nOnly work that is clearly assigned as team work should be completed collaboratively (projects)\nExams must be completed individually, you may not discuss answers with teammates, clarification questions should only be asked to myself and the TAs\nLabs must be completed individually. You may not directly share answers / code with others, however you are welcome to discuss the problems in general and ask for advice"
  },
  {
    "objectID": "lectures/01/01-welcome.html#sharing-reusing-code-policy",
    "href": "lectures/01/01-welcome.html#sharing-reusing-code-policy",
    "title": "Welcome to STA 101",
    "section": "Sharing / reusing code policy",
    "text": "Sharing / reusing code policy\n\nWe are aware that a huge volume of code is available on the web, and many tasks may have solutions posted\nUnless explicitly stated otherwise, this course’s policy is that you may make use of any online resources (e.g., StackOverflow) but you must explicitly cite where you obtained any code you directly use or use as inspiration in your solution(s)\nAny recycled code that is discovered and is not explicitly cited will be treated as plagiarism, regardless of source"
  },
  {
    "objectID": "lectures/01/01-welcome.html#generative-ai-policy",
    "href": "lectures/01/01-welcome.html#generative-ai-policy",
    "title": "Welcome to STA 101",
    "section": "Generative AI policy",
    "text": "Generative AI policy\nYou should treat generative AI, such as ChatGPT, the same as other online resources. There are two guiding principles that govern how you can use AI in this course:1\n(1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate—rather than hinder—learning.\n(2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.\n\n✅ AI tools for code: You may make use of the technology for coding examples on assignments; if you do so, you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism.\n❌ AI tools for narrative: Unless instructed otherwise, you may not use generative AI to write narrative on assignments. In general, you may use generative AI as a resource as you complete assignments but not to answer the exercises for you. You are ultimately responsible for the work you turn in; it should reflect your understanding of the course content.\n\nThese guiding principles are based on Course Policies related to ChatGPT and other AI Tools developed by Joel Gladd, Ph.D."
  },
  {
    "objectID": "lectures/01/01-welcome.html#academic-integrity",
    "href": "lectures/01/01-welcome.html#academic-integrity",
    "title": "Welcome to STA 101",
    "section": "Academic integrity",
    "text": "Academic integrity\nTo uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\n\n\nmost importantly:\nask if you’re not sure if something violates a policy!"
  },
  {
    "objectID": "lectures/01/01-welcome.html#office-hours",
    "href": "lectures/01/01-welcome.html#office-hours",
    "title": "Welcome to STA 101",
    "section": "Office hours",
    "text": "Office hours\n\nMine: Tuesdays 3:30 - 4:30 pm - Old Chem 213 + by appointment (on Zoom or in person depending on day/time)\nTAs: See the course team and course support pages on the course website. We have a total of 17 TA office hours per week!\n+ lots more resources listed on the syllabus!"
  },
  {
    "objectID": "lectures/01/01-welcome.html#wellness",
    "href": "lectures/01/01-welcome.html#wellness",
    "title": "Welcome to STA 101",
    "section": "Wellness",
    "text": "Wellness\nI want to make sure that you learn everything you were hoping to learn from this class. If this requires flexibility, please don’t hesitate to ask.\n\nYou never owe me personal information about your health (mental or physical) but you’re always welcome to talk to me. If I can’t help, I likely know someone who can.\nI want you to learn lots of things from this class, but I primarily want you to stay healthy, balanced, and grounded."
  },
  {
    "objectID": "lectures/01/01-welcome.html#rstudio",
    "href": "lectures/01/01-welcome.html#rstudio",
    "title": "Welcome to STA 101",
    "section": "RStudio",
    "text": "RStudio\n\nhttps://posit.cloud\n\n\nBrowser based RStudio instance(s) provided by Posit\nRequires internet connection to access\nProvides consistency in hardware and software environments\nLocal R installations are fine but we will not guarantee support"
  },
  {
    "objectID": "lectures/01/01-welcome.html#slack",
    "href": "lectures/01/01-welcome.html#slack",
    "title": "Welcome to STA 101",
    "section": "Slack",
    "text": "Slack\n\nOnline forum for asking and answering questions\nPrivate repo in the course organization\nYou will need to join the course organization for access\nAsk and answer questions related to course logistics, assignment, etc. here\nPersonal questions (e.g., extensions, illnesses, etc.) should be via email to me\nOnce you join, browse the channels to make sure you’re posting questions in the right channel, update your profile with your name, photo/avatar of you that matches your GitHub profile, and your pronouns\nUnfortunately Slack is not the best place to in-depth questions, but it’s a great place for real-time connection and collaboration"
  },
  {
    "objectID": "lectures/01/01-welcome.html#to-do-before-1",
    "href": "lectures/01/01-welcome.html#to-do-before-1",
    "title": "Welcome to STA 101",
    "section": "To do before…",
    "text": "To do before…\nwe move on\nSee course announcement (on Canvas or in your email) and click on the links to\n\nLog in to Posit Cloud – and update your profile\nLog in to Slack – and update your profile with your photo, pronouns, name pronunciation\n\nthe next class on Wednesday\n\nRead the syllabus\nComplete the Getting to know you survey on Canvas\nComplete the readings\n\nthe end of the week\n\nGet started on the lab assignment\nComplete the interactive tutorials"
  },
  {
    "objectID": "lectures/01/01-welcome.html#application-exercise-un-votes",
    "href": "lectures/01/01-welcome.html#application-exercise-un-votes",
    "title": "Welcome to STA 101",
    "section": "Application exercise: UN Votes",
    "text": "Application exercise: UN Votes\n\nGo to Posit Cloud and start the project called UN Votes. Render the document titled unvotes.qmd. Review the narrative and the data visualization you just created. Then, change “Turkey” to another country of your choice. Re-render the document. Show the plot you created to your neighbor and discuss (1) why you chose that country and (2) how this new visualization is different than the original (and what that says about country politics, if anything).\nTime permitting: How were these data collected?\n\n\n\n\n🔗 sta101-f23.github.io"
  },
  {
    "objectID": "course-support.html",
    "href": "course-support.html",
    "title": "Course support",
    "section": "",
    "text": "Most of you will need help at some point and we want to make sure you can identify when that is without getting too frustrated and feel comfortable seeking help."
  },
  {
    "objectID": "course-support.html#lectures-and-labs",
    "href": "course-support.html#lectures-and-labs",
    "title": "Course support",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone."
  },
  {
    "objectID": "course-support.html#office-hours",
    "href": "course-support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage each and every one of you to take advantage of this resource! Make a pledge to stop by office hours at least once during the first three weeks of class. If you truly have no questions to ask, just stop by and say hi and introduce yourself. You can find a list of everyone’s office hours here."
  },
  {
    "objectID": "course-support.html#slack",
    "href": "course-support.html#slack",
    "title": "Course support",
    "section": "Slack",
    "text": "Slack\nHave a question that can’t wait for office hours? Prefer to write out your question in detail rather than asking in person? The course Slack is the best venue for these! There is a chance another student has already asked a similar question, so please check the other posts on Slack before asking a new question. If you know the answer to a question that is posted, I encourage you to respond!"
  },
  {
    "objectID": "course-support.html#email",
    "href": "course-support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nPlease refrain from emailing any course content questions (those should go to Slack), and only use email for questions about personal matters that may not be appropriate for the public course forum (e.g., illness, accommodations, etc.). For such matters, you may email Dr. Mine Çetinkaya-Rundel at mc301@duke.edu.\nIf there is a question that’s not appropriate for the public forum, you are welcome to email me directly. If you email me, please include “STA 101” in the subject line. Barring extenuating circumstances, I will respond to STA 101 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday."
  },
  {
    "objectID": "course-support.html#academic-support",
    "href": "course-support.html#academic-support",
    "title": "Course support",
    "section": "Academic support",
    "text": "Academic support\nThere are times may need help with the class that is beyond what can be provided by the teaching team. In those instances, I encourage you to visit the Academic Resource Center. The Academic Resource Center (ARC) offers free services to all students during their undergraduate careers at Duke. Services include Learning Consultations, Peer Tutoring and Study Groups, ADHD/LD Coaching, Outreach Workshops, and more. Because learning is a process unique to every individual, they work with each student to discover and develop their own academic strategy for success at Duke. Contact the ARC to schedule an appointment. Undergraduates in any year, studying any discipline can benefit! Contact ARC@duke.edu, 919-684-5917."
  },
  {
    "objectID": "course-support.html#mental-health-and-wellness",
    "href": "course-support.html#mental-health-and-wellness",
    "title": "Course support",
    "section": "Mental health and wellness",
    "text": "Mental health and wellness\nStudent mental health and wellness are of primary importance at Duke, and the university offers resources to support students in managing daily stress and self-care. Duke offers several resources for students to seek assistance on coursework and to nurture daily habits that support overall well-being, some of which are listed below.\n\nThe Academic Resource Center: (919) 684-5917, theARC@duke.edu, or arc.duke.edu.\nDuWell: (919) 681-8421, provides Moments of Mindfulness (stress management and resilience building) and meditation programming to assist students in developing a daily emotional well-being practice. To see schedules for programs please see https://studentaffairs.duke.edu/duwell. All are welcome and no experience necessary.\n\nIf your mental health concerns and/or stressful events negatively affect your daily emotional state, academic performance, or ability to participate in your daily activities, many resources are available to help you through difficult times. Duke encourages all students to access these resources.\n\nDukeReach: Provides comprehensive outreach services to identify and support students in managing all aspects of well-being. If you have concerns about a student’s behavior or health visit the website for resources and assistance: http://studentaffairs.duke.edu/dukereach.\nCounseling and Psychological Services (CAPS): CAPS services include individual and group counseling services, psychiatric services, and workshops. To initiate services, walk-in/call-in 9-4 M,W,Th,F and 9-6 Tuesdays. CAPS also provides referral to off-campus resources for specialized care. (919) 660-1000 or https://students.duke.edu/wellness/caps.\nTimelyCare: (formerly known as Blue Devils Care) An online platform that is a convenient, confidential, and free way for Duke students to receive 24/7 mental health support through TalkNow and scheduled counseling: https://bluedevilscare.duke.edu."
  },
  {
    "objectID": "course-support.html#course-costs",
    "href": "course-support.html#course-costs",
    "title": "Course support",
    "section": "Course costs",
    "text": "Course costs\n\nTextbooks: The textbooks for this course are freely available on the web.\nLaptops: Each student is expected to have a laptop they can bring to each lecture and lab.\n\nIf you are having difficulty with costs associated with this course, here are some resources:\n\nContact the financial aid office (whether or not you are on aid). They have loans and resources for connecting students with programs on campus that might be able to help alleviate these costs.\nDukeLIFE offers course materials assistance for eligible students. Please note that students who are eligible for DukeLIFE benefits are notified prior to the start of the semester; program resources are limited.\nDuke Libraries offers textbook rentals through the Top Textbook Program, where you can rent out a textbook for 3 hours at a time.\nFor course-specific technology needs such as Digital Voice Recorder, HD Video Camera, TI-84 Plus CE, DSLR camera kit, Tripod, Shotgun Mic, iPad Mini 4, a Handheld Projector, or a GoPro, you can reserve rental equipment from the Link."
  },
  {
    "objectID": "course-support.html#assistance-with-canvas-and-zoom",
    "href": "course-support.html#assistance-with-canvas-and-zoom",
    "title": "Course support",
    "section": "Assistance with Canvas and Zoom",
    "text": "Assistance with Canvas and Zoom\nFor technical help with Canvas or Zoom, contact the Duke OIT Service Desk at oit.duke.edu/help. You can also access the self-service help documentation for Zoom here and for Canvas here.\nNote that we will be making minimal use of Canvas in this course (primarily for announcements and grade book). All assignment submission will take place on Gradescope and conversation on Slack.\nZoom will be used for online office hours as well as as a backup option should we need to hold the course online instead of in person."
  },
  {
    "objectID": "ae/ae4.html",
    "href": "ae/ae4.html",
    "title": "Exploratory Data Analysis II",
    "section": "",
    "text": "lab 1 due tonight at 11:59pm on Gradescope\nbe sure to complete prepare material (on the schedule) before each class"
  },
  {
    "objectID": "ae/ae4.html#bulletin",
    "href": "ae/ae4.html#bulletin",
    "title": "Exploratory Data Analysis II",
    "section": "",
    "text": "lab 1 due tonight at 11:59pm on Gradescope\nbe sure to complete prepare material (on the schedule) before each class"
  },
  {
    "objectID": "ae/ae4.html#today",
    "href": "ae/ae4.html#today",
    "title": "Exploratory Data Analysis II",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nLearn and practice the big 7 dplyr verbs"
  },
  {
    "objectID": "ae/ae4.html#getting-started",
    "href": "ae/ae4.html#getting-started",
    "title": "Exploratory Data Analysis II",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console (bottom left of screen)\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae4.qmd\",\n  destfile = \"ae4.qmd\")"
  },
  {
    "objectID": "ae/ae4.html#load-packages-and-data",
    "href": "ae/ae4.html#load-packages-and-data",
    "title": "Exploratory Data Analysis II",
    "section": "Load packages and data",
    "text": "Load packages and data\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\nType ?palmerpenguins to learn more about this package. Or better yet, check it out here.\n\ndata(penguins)"
  },
  {
    "objectID": "ae/ae4.html#a-package-within-a-package",
    "href": "ae/ae4.html#a-package-within-a-package",
    "title": "Exploratory Data Analysis II",
    "section": "A package within a package…",
    "text": "A package within a package…\nWhen we load the tidyverse library, dplyr is packaged with it.\ndplyr, a grammar of data manipulation offers intuitive ‘verb’ functions that describe actions we commonly want to perform with data. The big 7 we’ll cover today are:\n\nmutate() adds new variables (columns) that are functions of existing variables\nselect() picks variables based on their names.\nfilter() picks rows based on their values in specific columns.\ngroup_by() sets us up to summarize across groups\nsummarize() reduces multiple values down to a single summary.\narrange() changes the ordering of the rows.\nslice() select, remove and duplicate rows based on their index\n\n(as described at https://dplyr.tidyverse.org/)\n\nCheck out documentation with ?\n\n\nMutate\n\nmutate() adds new variables (columns) that are functions of existing variables\n\nApproximate bill area (in \\(m^2\\)) as bill length * bill depth:\n\npenguins %&gt;%\n  mutate(bill_area_mm2 = bill_length_mm * bill_depth_mm)\n\n# A tibble: 344 × 9\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 3 more variables: sex &lt;fct&gt;, year &lt;int&gt;, bill_area_mm2 &lt;dbl&gt;\n\n\n\n\nSelect\n\nselect() picks variables (columns) based on their names.\n\nIt’s hard to see bill length, depth and area in the same output, let’s select a smaller subset of the variables to look at.\n\n# Example 1\npenguins %&gt;%\n  mutate(bill_area_mm2 = bill_length_mm * bill_depth_mm) %&gt;%\n  select(-c(year, species, island, flipper_length_mm, body_mass_g, sex))\n\n# A tibble: 344 × 3\n   bill_length_mm bill_depth_mm bill_area_mm2\n            &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n 1           39.1          18.7          731.\n 2           39.5          17.4          687.\n 3           40.3          18            725.\n 4           NA            NA             NA \n 5           36.7          19.3          708.\n 6           39.3          20.6          810.\n 7           38.9          17.8          692.\n 8           39.2          19.6          768.\n 9           34.1          18.1          617.\n10           42            20.2          848.\n# ℹ 334 more rows\n\n# Example 2\npenguins %&gt;%\n  mutate(bill_area_mm2 = bill_length_mm * bill_depth_mm) %&gt;%\n  select(bill_length_mm, bill_depth_mm, bill_area_mm2)\n\n# A tibble: 344 × 3\n   bill_length_mm bill_depth_mm bill_area_mm2\n            &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n 1           39.1          18.7          731.\n 2           39.5          17.4          687.\n 3           40.3          18            725.\n 4           NA            NA             NA \n 5           36.7          19.3          708.\n 6           39.3          20.6          810.\n 7           38.9          17.8          692.\n 8           39.2          19.6          768.\n 9           34.1          18.1          617.\n10           42            20.2          848.\n# ℹ 334 more rows"
  },
  {
    "objectID": "ae/ae4.html#filter",
    "href": "ae/ae4.html#filter",
    "title": "Exploratory Data Analysis II",
    "section": "Filter",
    "text": "Filter\n\nfilter() picks rows based on their values in specific columns.\n\nLet’s just examine penguins on Dream island\n\npenguins %&gt;%\n  mutate(bill_area_mm2 = bill_length_mm * bill_depth_mm) %&gt;%\n  select(-year)\n# code here"
  },
  {
    "objectID": "ae/ae4.html#group-by-summarize",
    "href": "ae/ae4.html#group-by-summarize",
    "title": "Exploratory Data Analysis II",
    "section": "Group by + Summarize",
    "text": "Group by + Summarize\n\ngroup_by() sets us up to summarize across groups\nsummarize() reduces multiple values down to a single summary.\n\n\nExercise 2:\nFind mean bill area across sex. Fill in the blanks\n\npenguins %&gt;%\n  mutate(bill_area_mm2 = bill_length_mm * bill_depth_mm) %&gt;%\n  select(-year) %&gt;%\n  # filter for Dream\n  group_by(___) %&gt;%\n  summarize(mean_bill_area_mm2 = ___)"
  },
  {
    "objectID": "ae/ae4.html#arrange-slice",
    "href": "ae/ae4.html#arrange-slice",
    "title": "Exploratory Data Analysis II",
    "section": "Arrange + Slice",
    "text": "Arrange + Slice\n\narrange() changes the ordering of the rows.\nslice() select, remove and duplicate rows based on their index\n\nLet’s use arrange() and slice() to report the five penguins with the greatest bill area.\n\npenguins %&gt;%\n  mutate(bill_area_mm2 = bill_length_mm * bill_depth_mm) %&gt;%\n  select(bill_area_mm2, bill_length_mm) %&gt;%\n  arrange(desc(bill_area_mm2))\n\n# A tibble: 344 × 2\n   bill_area_mm2 bill_length_mm\n           &lt;dbl&gt;          &lt;dbl&gt;\n 1         1127.           54.2\n 2         1105.           55.8\n 3         1076.           52  \n 4         1065.           53.5\n 5         1056            52.8\n 6         1050.           51.7\n 7         1043.           52.7\n 8         1032.           58  \n 9         1021.           51.3\n10         1013.           59.6\n# ℹ 334 more rows\n\n\n\nExercise 3:\nAre these the same five penguins with the longest bills?\nOptional hint: if you want to be exactly precise about which penguins are which, you could add an ID column, e.g.\n\n penguins %&gt;%\n  mutate(id = seq(1:nrow(penguins)))\n\n# A tibble: 344 × 9\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 3 more variables: sex &lt;fct&gt;, year &lt;int&gt;, id &lt;int&gt;\n\n\nThis takes advantage of the nrow() function. Can you guess what it returns?\n\n\nExercise 4:\nCompute the average bill length, bill depth, flipper length and body mass across all islands.\n\n# code here\n\n\n\nOptional bonus:\nIs every species on every island?\n\n# code here"
  },
  {
    "objectID": "ae/ae25.html",
    "href": "ae/ae25.html",
    "title": "Cryptanalysis",
    "section": "",
    "text": "Office hours after class today\nLab 09 due tonight. Draft peer-report due Friday (in lab review)\ncourse evaluations open. \\(&gt;80\\%\\) response \\(\\rightarrow\\) +1pt final project"
  },
  {
    "objectID": "ae/ae25.html#bulletin",
    "href": "ae/ae25.html#bulletin",
    "title": "Cryptanalysis",
    "section": "",
    "text": "Office hours after class today\nLab 09 due tonight. Draft peer-report due Friday (in lab review)\ncourse evaluations open. \\(&gt;80\\%\\) response \\(\\rightarrow\\) +1pt final project"
  },
  {
    "objectID": "ae/ae25.html#getting-started",
    "href": "ae/ae25.html#getting-started",
    "title": "Cryptanalysis",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae25.qmd\",\ndestfile = \"ae25.qmd\")"
  },
  {
    "objectID": "ae/ae25.html#load-packages",
    "href": "ae/ae25.html#load-packages",
    "title": "Cryptanalysis",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(reshape2)"
  },
  {
    "objectID": "ae/ae25.html#background",
    "href": "ae/ae25.html#background",
    "title": "Cryptanalysis",
    "section": "Background",
    "text": "Background\nThis application exercise builds on the R code found here and The Markov Chain Monte Carlo Revolution by Persi Diaconis.\n\nLet’s load the data\n\nwarandpeace = readLines(\"https://sta101.github.io/static/appex/data/warandpeace.txt\")\nfrequency = read.table(\"https://sta101.github.io/static/appex/data/frequencies.txt\")\ncolnames(frequency) = c(toupper(letters), \"\") # edit column names\nsecret_message = readLines(\"https://sta101.github.io/static/appex/data/secret-message.txt\")"
  },
  {
    "objectID": "ae/ae25.html#exercise-1",
    "href": "ae/ae25.html#exercise-1",
    "title": "Cryptanalysis",
    "section": "Exercise 1",
    "text": "Exercise 1\nTake a look at secret-message. Each letter is a stand in for exactly one other letter. This sort of cipher is known as a “substitution cipher”.\n‘A’ could be encoded as one of 26 characters (A, B, C, …). Once the encoding for ‘A’ is chosen, ‘B’ has 25 possibilities and so on so there are, in total \\(26 \\times 25 \\times 24 \\times \\ldots \\times 3 \\times 2 \\times 1\\) possibilities.\n\nn = 26\ninput = n + 1\nkeys = gamma(input)\nkeys\n\n[1] 4.032915e+26\n\n\nThat’s over \\(4 \\times 10^{26}\\) possible keys! If you could check 10M keys per second, it would take approximately \\(1 \\times 10^{12}\\) (trillion) years to check every possible key. Trying every possible key is known as a “brute force” approach.\n\nChat with your neighbor and develop a strategy better than the brute force approach. Detail your strategy below."
  },
  {
    "objectID": "ae/ae25.html#exercise-2",
    "href": "ae/ae25.html#exercise-2",
    "title": "Cryptanalysis",
    "section": "Exercise 2",
    "text": "Exercise 2\nHere we determine how often one character follows another using the text from the very long book, War and Peace. We include a whitespace character as a 27th character in our alphabet.\nTo reduce computational demand, we will load the object created by this analysis but leave the code below for reference.\nThe result of the below analysis is in the object frequency.\nThe letter row denotes the first character in a 2 character sequence while columns determine the second character in the character sequence.\n\nWhat should be the sum of a row? Check this for the first row of the data frame.\n\n\n# code here\n\nCreate a heatmap of character frequency, where the y-axis is the first letter and the x-axis is the second letter in a two letter chain.\n\nUncomment and complete the code below.\n\nHint: We’ll use melt from the reshape2 package. Click here for an example.\n\nmelt_freq = melt(as.matrix(frequency))\n\n# melt_freq %&gt;%\n#   ggplot(aes(x = __, y = __, fill = __))"
  },
  {
    "objectID": "ae/ae25.html#mcmc",
    "href": "ae/ae25.html#mcmc",
    "title": "Cryptanalysis",
    "section": "MCMC",
    "text": "MCMC\nWe will use a famous statistical algorithm, Markov chain Monte Carlo (MCMC) to break the substitution cipher.\nMCMC is composed of three essential components:\n\nAbility. A way to propose any possible key.\nFeedback. A way to evaluate how good a given key is.\nCuriosity. A way to leave a good key for a worse key.\n\n\nThe analogy of the blind monkey.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere’s a monkey on an island with many ponds. The monkey has been tasked with finding the largest body of water on the island. The only trouble is, he is blind. In order to find the largest body of water, he throws rocks randomly and listens for a splash. If he hears a splash, he knows there’s a body of water where he last threw. He continues to throw more rocks in that direction to find out how big the pool is. Occasionally he gets bored and wanders off to look for another pond. In this way, he uses the MCMC algorithm to find the largest body of water.\n\n\nMaking the connection.\n\nAbility (throw a rock). The Monkey can walk around and throw the rock anywhere on the island, ensuring that given enough time, he will cover every inch of the island.\nFeedback (test for waters). Every time the Monkey throws the rock, he receives feedback by listening for a splash. A large splash means a deep pond and encourages him to continue throwing in that direction to figure out the perimeter of the pond.\nCuriosity (boredom). If the monkey finds the second largest pond on the island, he might get stuck throwing rocks in it for a long time. By occasionally walking away from a large pond, he will reach the largest pond quicker."
  },
  {
    "objectID": "ae/ae25.html#exercise-3",
    "href": "ae/ae25.html#exercise-3",
    "title": "Cryptanalysis",
    "section": "Exercise 3",
    "text": "Exercise 3\nLet’s write out together what MCMC looks like when decrpyting a secret message.\n\nAbility\nFeedback\nCuriosity"
  },
  {
    "objectID": "ae/ae25.html#exercise-4",
    "href": "ae/ae25.html#exercise-4",
    "title": "Cryptanalysis",
    "section": "Exercise 4",
    "text": "Exercise 4\nHere we load some functions that will help us decode the message.\nRun the code below to break the secret message. If the message is unintelligible after several iterations, you may try re-starting with a new seed. What is this equivalent to in the monkey analogy above?"
  },
  {
    "objectID": "ae/ae25.html#exercise-5",
    "href": "ae/ae25.html#exercise-5",
    "title": "Cryptanalysis",
    "section": "Exercise 5",
    "text": "Exercise 5\nTry your own message!\nCreate your own message in the code below and call mcmcAttack(coded) on your message to decode it!\n\nYou might think about what makes the message easy or difficult to attack, e.g. does length of the message affect its susceptibility to attack? What else might?"
  },
  {
    "objectID": "ae/ae22.html",
    "href": "ae/ae22.html",
    "title": "Regression + Inference",
    "section": "",
    "text": "Draft final project report due Friday December 2\nThis Friday is last lab before peer-review in two weeks"
  },
  {
    "objectID": "ae/ae22.html#bulletin",
    "href": "ae/ae22.html#bulletin",
    "title": "Regression + Inference",
    "section": "",
    "text": "Draft final project report due Friday December 2\nThis Friday is last lab before peer-review in two weeks"
  },
  {
    "objectID": "ae/ae22.html#today",
    "href": "ae/ae22.html#today",
    "title": "Regression + Inference",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nunderstand regression hypothesis testing\ninterpret p-values in a regression framework"
  },
  {
    "objectID": "ae/ae22.html#getting-started",
    "href": "ae/ae22.html#getting-started",
    "title": "Regression + Inference",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae22.qmd\",\ndestfile = \"ae22.qmd\")"
  },
  {
    "objectID": "ae/ae22.html#load-packages",
    "href": "ae/ae22.html#load-packages",
    "title": "Regression + Inference",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(viridis)"
  },
  {
    "objectID": "ae/ae22.html#load-data",
    "href": "ae/ae22.html#load-data",
    "title": "Regression + Inference",
    "section": "Load data",
    "text": "Load data\n\nSeoul_Bikes = read_csv(\"https://sta101.github.io/static/appex/data/Seoul_Bikes.csv\") \nSeoul_Calendar = read_csv(\"https://sta101.github.io/static/appex/data/Seoul_Calendar.csv\")\n\nbikes = left_join(Seoul_Bikes, Seoul_Calendar)\n\nThis data set was originally analyzed in two studies1 of predicting bike-rental usage in Seoul, South Korea. For this lecture, the data was sourced from UCI Machine Learning Repository.\nCode book:\n\nDate: the date\nrented_bikes: total number of bikes rented on a given day\ntemp_c: mean daily temperature (Celsius)\nhumidity_pct: mean daily humidity\nwind_speed: mean daily windspeed\nsnowfall_cm: mean daily snowfall (in cm)\nseason: the season\nholiday: whether or not the day is a holiday\n\n\nglimpse(bikes)\n\nRows: 365\nColumns: 8\n$ Date         &lt;chr&gt; \"1/1/18\", \"1/10/18\", \"1/11/18\", \"1/12/17\", \"1/2/18\", \"1/3…\n$ rented_bikes &lt;dbl&gt; 4290, 27909, 22964, 9539, 5377, 5132, 17388, 26820, 31928…\n$ temp_c       &lt;dbl&gt; -1.2833333, 15.4375000, 8.3458333, -2.4541667, -3.8666667…\n$ humidity_pct &lt;dbl&gt; 39.33333, 54.25000, 54.16667, 45.87500, 44.00000, 64.2083…\n$ wind_speed   &lt;dbl&gt; 1.4541667, 2.8250000, 1.2708333, 1.5375000, 1.6083333, 3.…\n$ snowfall_cm  &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.9041667, 0.…\n$ season       &lt;chr&gt; \"Winter\", \"Autumn\", \"Autumn\", \"Winter\", \"Winter\", \"Spring…\n$ holiday      &lt;chr&gt; \"Holiday\", \"No Holiday\", \"No Holiday\", \"No Holiday\", \"No …"
  },
  {
    "objectID": "ae/ae22.html#notes",
    "href": "ae/ae22.html#notes",
    "title": "Regression + Inference",
    "section": "Notes",
    "text": "Notes\n\nHypothesis testing in a regression framework\n\nbikes %&gt;%\n  ggplot(aes(x = temp_c, y = rented_bikes, color = holiday)) +\n  geom_point(alpha = 0.5) +\n  theme_bw() +\n  labs(x = \"Temperature\", y = \"Rented No. Bikes\", color = \"Holiday?\",\n       title = \"Rented Bicycles in Seoul\") +\n  scale_color_manual(values = c(\"red\", \"#00539B\")) +\n  geom_smooth(method = 'lm')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nExercise 1\nWrite the full model equation to match the figure above to predict the number of bikes rented on a given day based on the temperature outside and whether or not the day is a holiday.\n\\[\n\\text{Your equation here}\n\\]\n\n\nExercise 2\nFit the model above and examine the tidied output. What are the p-values associated with each predictor?\n\n# code here\n\nThe p-value output is associated with a typical hypothesis test… but what’s the null and alternative?\nThe main idea is that if a predictor (e.g. holiday) does not help us explain bike rental numbers then its associated \\(\\beta\\) might as well be 0. Within the framework of hypothesis testing:\n\\(H_0\\): \\(\\beta_{\\text{holiday}} = 0\\)\n\\(H_A:\\) \\(\\beta_{\\text{holiday}} \\neq 0\\)\nFor OLS regression, our test statistic is\n\\[\nT = \\frac{\\hat{\\beta} - 0}{\\text{SE}_{\\hat{\\beta}}} \\sim t_{n - 4}\n\\] We want to see if our observed statistic, \\(T\\), falls far in the tail under the null.\nR takes care of much of this behind the scenes with the tidy output and reports a p-value for each \\(\\beta\\) by default.\n\n\nExercise 3\nCalculate the p-value associated with \\(\\beta_{holiday}\\) manually using the equation above. Note: in a regression setting, the degrees of freedom is the number of observations minus the number of \\(\\beta\\)s.\nCompare the p-value to one reported in the tidy output.\n\n# code here\n\n\nIs \\(\\beta_{\\text{holiday}}\\) significant at the \\(\\alpha = 0.05\\) level? State your conclusion.\nLooking at the tidied output, are any of the \\(\\beta\\)s not significant at the \\(\\alpha = 0.05\\) level?\nChange the model from an interaction effects to a main effects model. What do you notice?"
  },
  {
    "objectID": "ae/ae22.html#footnotes",
    "href": "ae/ae22.html#footnotes",
    "title": "Regression + Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSathishkumar V E, Jangwoo Park, and Yongyun Cho. ‘Using data mining techniques for bike sharing demand prediction in metropolitan city.’ Computer Communications, Vol.153, pp.353-366, March, 2020; Sathishkumar V E and Yongyun Cho. ‘A rule-based model for Seoul Bike sharing demand prediction using weather data’ European Journal of Remote Sensing, pp. 1-18, Feb, 2020↩︎"
  },
  {
    "objectID": "ae/ae17.html",
    "href": "ae/ae17.html",
    "title": "CLT and Confidence Intervals",
    "section": "",
    "text": "Lab 6 due tonight\nProject proposal due Friday (tomorrow)"
  },
  {
    "objectID": "ae/ae17.html#bulletin",
    "href": "ae/ae17.html#bulletin",
    "title": "CLT and Confidence Intervals",
    "section": "",
    "text": "Lab 6 due tonight\nProject proposal due Friday (tomorrow)"
  },
  {
    "objectID": "ae/ae17.html#today",
    "href": "ae/ae17.html#today",
    "title": "CLT and Confidence Intervals",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nUse CLT to construct confidence intervals"
  },
  {
    "objectID": "ae/ae17.html#getting-started",
    "href": "ae/ae17.html#getting-started",
    "title": "CLT and Confidence Intervals",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae17.qmd\",\ndestfile = \"ae17.qmd\")"
  },
  {
    "objectID": "ae/ae17.html#load-packages",
    "href": "ae/ae17.html#load-packages",
    "title": "CLT and Confidence Intervals",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "ae/ae17.html#load-data-pokemon",
    "href": "ae/ae17.html#load-data-pokemon",
    "title": "CLT and Confidence Intervals",
    "section": "Load Data: Pokemon",
    "text": "Load Data: Pokemon\nWe will be using the pokemon data set, which contains information about 42 randomly selected Pokemon (from all generations). You may load in the data set with the following code:\n\npokemon = read_csv(\"https://sta101.github.io/static/appex/data/pokemon.csv\")\n\nIn this analysis, we will use CLT-based inference to draw conclusions about the mean height among all Pokemon species.\n\nExercise 1\nLet’s start by looking at the distribution of height_m, the typical height in meters for a Pokemon species, using a visualization and summary statistics.\n\nggplot(data = pokemon, aes(x = height_m)) +\n  geom_histogram(binwidth = 0.25, fill = \"steelblue\", color = \"black\") + \n  labs(x = \"Height (in meters)\", \n       y = \"Distributon of Pokemon heights\")\n\n\n\n\n\npokemon %&gt;%\n  summarise(mean_height = mean(height_m), \n            sd_height = sd(height_m), \n            n_pokemon = n())\n\n# A tibble: 1 × 3\n  mean_height sd_height n_pokemon\n        &lt;dbl&gt;     &lt;dbl&gt;     &lt;int&gt;\n1       0.929     0.497        42\n\n\nIn the previous lecture we were given the mean, \\(\\mu\\), and standard deviation, \\(\\sigma\\), of the population. That is unrealistic in practice (if we knew \\(\\mu\\) and \\(\\sigma\\), we wouldn’t need to do statistical inference!).\nToday we will start on using the Central Limit Theorem to draw conclusions about the \\(\\mu\\), the mean height in the population of Pokemon.\n\nWhat is the point estimate for \\(\\mu\\), i.e., the “best guess” for the mean height of all Pokemon?\nWhat is the point estimate for \\(\\sigma\\), i.e., the “best guess” for the standard deviation of the distribution of Pokemon heights?\n\n\n\nExercise 2\nBefore moving forward, let’s check the conditions required to apply the Central Limit Theorem. Are the following conditions met:\n\nIndependence?\nSample size/distribution?\n\n\n\nCentral limit theorem\nRemember, when the independence and sample size assumptions are met, the central limit theorem states\n\\[\n\\bar{x} \\sim N(\\mu, \\sigma / \\sqrt{n})\n\\] If we know \\(\\sigma\\), we can construct a symmetric confidence interval for the true mean easily using qnorm().\nFor example, if the true standard deviation in pokemon height is 0.4 meters, then to construct a 95% confidence interval:\n\nxbar = pokemon %&gt;%\n  summarize(xbar = mean(height_m)) %&gt;%\n  pull(xbar)\n\nqnorm(c(0.025, 0.975), mean = xbar, sd = 0.4)\n\n[1] 0.1445858 1.7125570\n\n\nThis can be equivalently expressed\n\nzscore = qnorm(0.025)\nxbar + zscore*0.4\n\n[1] 0.1445858\n\nxbar - zscore*0.4\n\n[1] 1.712557\n\n\nwhere we use the fact that we can write any normal distribution as a linear combination of a standard normal. For example,\nif \\(X \\sim N(0.928, .4)\\), then \\(X = .4Z + 0.928\\) where \\(Z\\) is standard normal, in other words \\(Z \\sim N(0, 1)\\).\nIn general, the confidence interval can be written as\n\\[\n\\bar{x} \\pm z^* \\times \\sigma\n\\] where \\(z^*\\) is the quantile of a standard normal distribution associated with our level of confidence.\nWhat about when we don’t know \\(\\sigma\\)?\n\n\nPractical confidence intervals\nWe don’t know the true population mean \\(\\mu\\) and standard deviation \\(\\sigma\\), how do we use CLT to construct a confidence interval?\nWe approximate \\(\\mu\\) by \\(\\bar{x}\\) and \\(\\sigma\\) by the same standard deviation \\(s\\). However \\(s\\) may be smaller than \\(\\sigma\\) and our confidence interval could be too narrow, for example, run the code below to compute the standard deviation of three draws from a standard normal.\n\nset.seed(6)\nsamples = rnorm(3, mean = 0, sd = 1)\nsd(samples)\n\n[1] 0.7543284\n\n\nThis was just for 1 random seed. If you remove the seed and repeat the simulation, you will find that \\(s\\) is sometimes above and sometimes below the true standard deviation.\nTo account for this uncertainty, we will use a distribution with thicker tails. This sampling distribution is called a t-distribution.\n\nggplot(data = data.frame(x = c(0 - 1*3, 0 + 1*3)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = 1),\n                color = \"black\") +\n  stat_function(fun = dt, args = list(df = 3),\n                color = \"red\",lty = 2) + theme_bw() +\n  labs(title = \"Black solid line = normal, Red dotted line = t-distribution\")\n\n\n\n\nThe t-distribution has a bell shape but the extra thick tails help us correct for the variability introduced by using \\(s\\) instead of \\(\\sigma\\).\nThe t-distribution, like the standard normal, is always centered at zero. Therefore, the t-distribution has only a single parameter: degrees of freedom. The degrees of freedom describes the precise form of the bell-shaped t-distribution. In general, we’ll use a t-distribution with \\(df=n−1\\) to model the sample mean when the sample size is \\(n\\).\nWe can use qt and pt to find quantiles and probabilities respectively under the t-distribution.\n\n\nConfidence interval\nTo construct our practical confidence interval (where we don’t know \\(\\sigma\\)) we use the t-distribution:\n\\[\n\\bar{x} \\pm t^*_{n-1} \\times \\frac{s}{\\sqrt{n}}\n\\]\n\nExercise 3\n\nCalculate the 95% confidence interval for pokemon height using the t-distribution.\n\n\n# code here\n\nHow does this compare to a 95% bootstrap confidence interval?\n\n# code here"
  },
  {
    "objectID": "ae/ae12.html",
    "href": "ae/ae12.html",
    "title": "Probability I",
    "section": "",
    "text": "Lab 04 due Friday October 7\nRegression project due Friday October 14"
  },
  {
    "objectID": "ae/ae12.html#bulletin",
    "href": "ae/ae12.html#bulletin",
    "title": "Probability I",
    "section": "",
    "text": "Lab 04 due Friday October 7\nRegression project due Friday October 14"
  },
  {
    "objectID": "ae/ae12.html#today",
    "href": "ae/ae12.html#today",
    "title": "Probability I",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nbe able to define random variables, probability, and distributions\nbe able to estimate probabilites from data\nsimulate from a binomial distribution"
  },
  {
    "objectID": "ae/ae12.html#getting-started",
    "href": "ae/ae12.html#getting-started",
    "title": "Probability I",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae12.qmd\",\ndestfile = \"ae12.qmd\")"
  },
  {
    "objectID": "ae/ae12.html#load-packages-and-data",
    "href": "ae/ae12.html#load-packages-and-data",
    "title": "Probability I",
    "section": "Load packages and data",
    "text": "Load packages and data\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(fivethirtyeight)\n\n\ndata(bob_ross)\nglimpse(bob_ross)\n\nRows: 403\nColumns: 71\n$ episode            &lt;chr&gt; \"S01E01\", \"S01E02\", \"S01E03\", \"S01E04\", \"S01E05\", \"…\n$ season             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, …\n$ episode_num        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1, 2, 3,…\n$ title              &lt;chr&gt; \"A WALK IN THE WOODS\", \"MT. MCKINLEY\", \"EBONY SUNSE…\n$ apple_frame        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ aurora_borealis    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ barn               &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ beach              &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ boat               &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ bridge             &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ building           &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ bushes             &lt;int&gt; 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, …\n$ cabin              &lt;int&gt; 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ cactus             &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ circle_frame       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ cirrus             &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ cliff              &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ clouds             &lt;int&gt; 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, …\n$ conifer            &lt;int&gt; 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, …\n$ cumulus            &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, …\n$ deciduous          &lt;int&gt; 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, …\n$ diane_andre        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ dock               &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ double_oval_frame  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ farm               &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ fence              &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ fire               &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ florida_frame      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ flowers            &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ fog                &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ framed             &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ grass              &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, …\n$ guest              &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ half_circle_frame  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ half_oval_frame    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ hills              &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ lake               &lt;int&gt; 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, …\n$ lakes              &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ lighthouse         &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ mill               &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ moon               &lt;int&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ mountain           &lt;int&gt; 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, …\n$ mountains          &lt;int&gt; 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, …\n$ night              &lt;int&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ ocean              &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, …\n$ oval_frame         &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ palm_trees         &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ path               &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ person             &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ portrait           &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ rectangle_3d_frame &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ rectangular_frame  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ river              &lt;int&gt; 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ rocks              &lt;int&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ seashell_frame     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ snow               &lt;int&gt; 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, …\n$ snowy_mountain     &lt;int&gt; 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, …\n$ split_frame        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ steve_ross         &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ structure          &lt;int&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ sun                &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, …\n$ tomb_frame         &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ tree               &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ trees              &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ triple_frame       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ waterfall          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ waves              &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …\n$ windmill           &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ window_frame       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ winter             &lt;int&gt; 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ wood_framed        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …"
  },
  {
    "objectID": "ae/ae12.html#re-cap-from-prepare",
    "href": "ae/ae12.html#re-cap-from-prepare",
    "title": "Probability I",
    "section": "Re-cap from prepare",
    "text": "Re-cap from prepare\n\nWarm-up\n\nThere are 403 episodes of Bob Ross. Assume exactly 1 painting is painted in each episode. Pretend that before each episode, Bob Ross flips a coin to decide whether or not to paint a sunrise. If it lands heads, a sunrise is in the painting and if it’s tails then he does not paint a sunrise. Let \\(X\\) be the number of episodes a sunrise is featured in the painting. What is the sample space of this random experiment?\nWhat is an event?\nLet A be the event \\(X&gt;10\\) and let B be the event \\(X&lt;3\\). Are A and B disjoint?\n\n\n\nProbability\nA probability is the long-run frequency of an event. In other words, the proportion of times we would see an event occur if we could repeat an experiment an infinite number of times. Probabilities take values between 0 and 1 inclusive.\n\n\n\n\n\n\nNote\n\n\n\nIf A and B are two disjoint events, then the probability of A or B occurring is equal to the probability of A plus the probability of B. More concisely, Pr(A or B) = Pr(A) + Pr(B).\n\n\n\nMore definitions\nLet A and B be two events.\n\nMarginal probability: The probability an event occurs regardless of values of the other event\n\nP(A)\nExample: What’s the probability that, in a randomly selected episode of Bob Ross, the painting features clouds?\n\nJoint probability: The probability two or more events simultaneously occur\n\nExample: What’s the probability that, in a randomly selected episode of Bob Ross, the painting features clouds and mountains?\nP(A and B)\n\nConditional probability: The probability an event occurs given the other has occurred\n\nP(A|B) or P(B|A)\nExample: What is the probability that a Bob Ross painting features clouds if it was randomly selected from season 1?\nP(A|B) = P(A and B) / P(B)\n\nIndependent events: Knowing one event has occurred does not lead to any change in the probability we assign to another event.\n\nP(A|B) = P(A) or P(B|A) = P(B)\nExample: The probability a painting features lakes is independent of whether or not it features rivers. More concisely, P(lakes | rivers) = P(lakes)\n\n\n\n\nExercise 1\nApproximate the probability of each example above using the bob_ross data set. For the last example (independence) validate or refute the claim using the data set.\n\n# code here"
  },
  {
    "objectID": "ae/ae12.html#data-generative-processes",
    "href": "ae/ae12.html#data-generative-processes",
    "title": "Probability I",
    "section": "Data generative processes",
    "text": "Data generative processes\nAs statisticians, we often want to model the process that generates data. For example, although Bob Ross probably does not flip a coin to decide what to paint, it might be a useful model for describing the data.\nTo formalize this concept, we will embrace two new concepts: random variables and distributions.\n\nRandom variables\nYou may not have realized it, but we’ve already seen random variables. A random variable is a function that maps an observed outcome to a number.\nFor example, when Bob Ross paints a tree and we label it “1” or does not paint a tree and we label it “0”, we are defining a random variable!\n\n\n\n\n\n\nNote\n\n\n\nRandom variables that only take the values 0 and 1 have a special name. They are called indicator random variables because they are thought of as indicating whether or not an event occurs.\n\n\nRandom variables have distributions…"
  },
  {
    "objectID": "ae/ae12.html#distributions",
    "href": "ae/ae12.html#distributions",
    "title": "Probability I",
    "section": "Distributions",
    "text": "Distributions\n\nBinomial distribution\nThe binomial distribution models the number of success in a series of independent and identical binary trials and is defined by two parameters:\n\n\\(k\\), the total number of trials,\n\\(p\\), the probability of a success in an individual trial.\n\nThe sample space of a binomial random variable is \\(\\{0, 1, \\ldots, k \\}\\). In words, there could be up to \\(k\\) success in an binomial experiment.\n\nExample\nYou toss a fair coin 10 times. Let A be the event there is at least one head. What is the probability of A?\nHere \\(k\\) is _ and \\(p\\) is _.\nrbinom() arguments:\n\nsize is the number of trials, aka the number of coin flips in 1 experiment\nprob is the probability of a success\nn defines how many times to repeat the entire experiment\n\n\nset.seed(100) # sets random seed to ensure we get same result\n\nN = 1000 # total number of experiments of 10 coin flips\n\ncoin_flips = data.frame(num_heads = rbinom(n = N, size = 10, prob = 0.5))\n\ncoin_flips %&gt;%\n  filter(num_heads &gt;=1) %&gt;%\n  nrow() / N\n\n[1] 0.998\n\n\n\ncoin_flips %&gt;%\n  ggplot(aes(x = num_heads)) +\n  geom_histogram(bins = 35) +\n  theme_bw() +\n  labs(x = \"Number of heads\",\n       y  = \"Count\",\n       title = \"Distribution of the total # heads in 10 fair coin flips\") +\n  scale_x_continuous(breaks = 0:10)\n\n\n\n\n\n\nExercise 2\nSuppose Bob Ross paintings feature a mountain with probability \\(0.7\\). (You might imagine that before Bob Ross paints, he flips an unfair coin that has a 70% chance of landing heads. If the coin lands on heads, he paints a mountain if it lands on tails he does not.) Given that there are 403 episodes of Bob Ross, what is the probability that at least 150 paintings feature a mountain?\nTo help you setup your simulation, set \\(N = 2000\\). What is \\(k\\)? What is \\(p\\)?\nTo ensure we get the same answer, use the seed provided below.\n\nset.seed(100)\n# code here\n\n\nAs a follow-up question, what is the probability that at least 100 paintings but not more than 200 feature a mountain?\n\n\n\n\nMathematics of the binomial distribution\n\\[\nX \\sim \\text{Binomial}(k, p)\n\\]\n“X has a binomial distribution with parameters k and p”\nWhat this means to us is:\n\nwe can simulate the distribution of X using the rbinom code from above\nwe can compute the probability X equals any specific value explicitly with the equation below\n\n\\[\nP(X = m) = {k \\choose m} (p)^{m}(1-p)^{k-m}\n\\] If we want to know, for example, the exact probability Bob Ross paints at least 150 paintings with a mountain, we would need to add up \\(P(X = 150) + P(X = 151) + P(X = 152) + \\ldots + P(X = 402) + P(X = 403)\\).\n\nExample\nWhat’s the probability that exactly 1 coin lands heads when we flip a fair coin 10 times?\n\n# P(X = m) \n\n# k choose m\n# k = 10, m = 1\n# fair coin means p = 0.5\n\nchoose(10, 1) * (0.5)^1 * (1 - 0.5)^(10 - 1)\n\n[1] 0.009765625\n\n\n\n\nExercise 3\nWhat’s the probability at least 1 coin lands heads? Use the equation above to compute. Hint: \\(P(X &gt;= 1) = 1 - P(X = 0)\\).\n\n# code here\n\n\n\nExercise 4 (before next class)\nPlay with the “random variables” lesson here. Describe what you observe."
  },
  {
    "objectID": "ae/ae1.html",
    "href": "ae/ae1.html",
    "title": "Welcome to R",
    "section": "",
    "text": "By the end of today you will…\n\nbegin to know your way around RStudio\nbe able to define package, data frame, variable, function, argument\nuse the function glimpse()"
  },
  {
    "objectID": "ae/ae1.html#today",
    "href": "ae/ae1.html#today",
    "title": "Welcome to R",
    "section": "",
    "text": "By the end of today you will…\n\nbegin to know your way around RStudio\nbe able to define package, data frame, variable, function, argument\nuse the function glimpse()"
  },
  {
    "objectID": "ae/ae1.html#getting-started",
    "href": "ae/ae1.html#getting-started",
    "title": "Welcome to R",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console (bottom left of screen)\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae1.qmd\",\ndestfile = \"ae1.qmd\")"
  },
  {
    "objectID": "ae/ae1.html#r-as-a-calculator",
    "href": "ae/ae1.html#r-as-a-calculator",
    "title": "Welcome to R",
    "section": "R as a calculator",
    "text": "R as a calculator\n\nUse R as a calculator by typing the following into the console:\n\n\n5 * 5 + 10\n\nx = 3\nx + x^2\n\nx = 1:10\nx * 7\nIn the last couple examples we save some value as the object “x”.\nWe can “print” x to the screen by typing the name of the object (“x”) in the console or in a code chunk."
  },
  {
    "objectID": "ae/ae1.html#tour-of-rstudio",
    "href": "ae/ae1.html#tour-of-rstudio",
    "title": "Welcome to R",
    "section": "Tour of RStudio",
    "text": "Tour of RStudio\n\nenvironment\nR functions\nloading and viewing a data frame"
  },
  {
    "objectID": "ae/ae1.html#load-a-package",
    "href": "ae/ae1.html#load-a-package",
    "title": "Welcome to R",
    "section": "Load a package",
    "text": "Load a package\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "ae/ae1.html#load-data",
    "href": "ae/ae1.html#load-data",
    "title": "Welcome to R",
    "section": "Load data",
    "text": "Load data\n\nroster = read_csv(\"https://sta101.github.io/static/appex/data/sample-roster.csv\")\nsurvey = read_csv(\"https://sta101.github.io/static/appex/data/sample-survey.csv\")\n\nQuestion: What objects store the data in the code chunk above? Can you print them to the screen?\nCreate a new code chunk with CMD+OPTION+I (mac) or CTRL+ALT+I (windows/linux)\nSo far we’ve already seen two functions. library and read_csv. Functions in R are attached to parentheses and take an input, aka an argument, and often (but not always) return an output. To learn more about a function, you can check the documentation with ?, e.g. ?library."
  },
  {
    "objectID": "ae/ae1.html#demos",
    "href": "ae/ae1.html#demos",
    "title": "Welcome to R",
    "section": "Demos",
    "text": "Demos\nLet’s glimpse the data frame.\n\nglimpse(survey)\n\nRows: 12\nColumns: 5\n$ name                 &lt;chr&gt; \"A\", \"Appa\", \"Bumi\", \"Soka\", \"Katara\", \"Suki\", \"Z…\n$ email                &lt;chr&gt; \"the-last-Rbender@duke.edu\", \"yip-yip-appa@duke.e…\n$ bender               &lt;chr&gt; \"Airbender\", \"Airbender\", \"Earthbender\", \"None\", …\n$ previous_programming &lt;chr&gt; \"No\", \"No\", \"No\", \"Somewhat\", \"Yes\", \"Yes\", \"Yes\"…\n$ cat_dog              &lt;chr&gt; \"dog\", \"cat\", \"cat\", \"dog\", \"dog\", \"cat\", \"cat\", …\n\n\nTo look at all of it, we can use view()\n\nview(survey)\n\nView the roster data in the console\nTerminology: “columns” of a dataframe are called variables whereas “rows” are observations.\nQuestion: How many variables are in the data frame survey? How many observations? What about the data frame roster?\nWhy must I input a specific email format?\n\nroster %&gt;% \n  left_join(survey, by = \"email\")"
  },
  {
    "objectID": "ae/ae26.html",
    "href": "ae/ae26.html",
    "title": "Correlation and Covariance",
    "section": "",
    "text": "Final project due date updated (see announcement)\nCourse evaluations. \\(&gt;80\\%\\) response \\(\\rightarrow\\) +1pt final project.\nIf \\(&gt;80\\%\\) TA evals, another 0.5 points."
  },
  {
    "objectID": "ae/ae26.html#bulletin",
    "href": "ae/ae26.html#bulletin",
    "title": "Correlation and Covariance",
    "section": "",
    "text": "Final project due date updated (see announcement)\nCourse evaluations. \\(&gt;80\\%\\) response \\(\\rightarrow\\) +1pt final project.\nIf \\(&gt;80\\%\\) TA evals, another 0.5 points."
  },
  {
    "objectID": "ae/ae26.html#getting-started",
    "href": "ae/ae26.html#getting-started",
    "title": "Correlation and Covariance",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae26.qmd\",\ndestfile = \"ae26.qmd\")"
  },
  {
    "objectID": "ae/ae26.html#load-packages",
    "href": "ae/ae26.html#load-packages",
    "title": "Correlation and Covariance",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ellipse)"
  },
  {
    "objectID": "ae/ae26.html#covariance-and-correlation",
    "href": "ae/ae26.html#covariance-and-correlation",
    "title": "Correlation and Covariance",
    "section": "Covariance and correlation",
    "text": "Covariance and correlation\nEmpirical covariance between two variables\n\\[\nCov(X,Y) = \\frac{1}{n-1} \\sum_{i = 1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})\n\\] Later in this application exercise we write \\(\\sigma_{xy}^2\\) as the covariance between x and y.\nEmpirical correlation between two variables,\n\\[\np_{XY} = \\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y}\n\\] where \\(\\sigma_X\\) and \\(\\sigma_Y\\) are the standard deviation of X and Y respectively.\n\nExample\n\nlibrary(tidyverse)\nx = c(1, 2, 3, 4, 5)\ny = c(0.5, 3, 2.2, 5, 5.5)\n\ndf = data.frame(x, y)\nfit1 = linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(y ~ x, data = df)\nr2 = glance(fit1)$r.squared\n\ncat(\"Correlation between x and y: \", cor(x,y),\n    \"\\nCovariance between x and y: \", cov(x,y),\n    \"\\nStandard deviation of x: \", sd(x),\n    \"\\nStandard deviation of y: \", sd(y),\n    \"\\nR squared: \", r2\n    )\n\nCorrelation between x and y:  0.9243906 \nCovariance between x and y:  3 \nStandard deviation of x:  1.581139 \nStandard deviation of y:  2.052559 \nR squared:  0.854498\n\n\n\n\nGuess that correlation!"
  },
  {
    "objectID": "ae/ae26.html#data",
    "href": "ae/ae26.html#data",
    "title": "Correlation and Covariance",
    "section": "Data",
    "text": "Data\nHawks is a subset of a data set by the same name in the Stat2Data package. Today we will focus on the following measurements of 891 hawks:\n\nSpecies: CH = cooper’s, RT = red-tailed, SS = sharp-shinned\nWeight: body weight in grams\nWing: length in mm of primary wing feather from tip to wrist it attaches to\nCulmen: length in mm of the upper bill from the tip to where it bumps into the fleshy part of the bird\nHallux: length in mm of the killing talon\n\n\nHawks = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/Hawks.csv\")\n\n\nglimpse(Hawks)\n\nRows: 891\nColumns: 5\n$ Species &lt;chr&gt; \"RT\", \"RT\", \"CH\", \"SS\", \"RT\", \"RT\", \"RT\", \"RT\", \"RT\", \"RT\", \"R…\n$ Weight  &lt;dbl&gt; 920, 990, 470, 170, 1090, 960, 855, 1210, 1120, 1010, 1010, 11…\n$ Wing    &lt;dbl&gt; 385, 381, 265, 205, 412, 370, 375, 412, 405, 393, 371, 390, 41…\n$ Culmen  &lt;dbl&gt; 25.7, 26.7, 18.7, 12.5, 28.5, 25.3, 27.2, 29.3, 26.0, 26.3, 25…\n$ Hallux  &lt;dbl&gt; 30.1, 31.3, 23.5, 14.3, 32.2, 30.1, 30.0, 31.3, 30.2, 30.8, 29…"
  },
  {
    "objectID": "ae/ae26.html#examples",
    "href": "ae/ae26.html#examples",
    "title": "Correlation and Covariance",
    "section": "Examples",
    "text": "Examples\n\nTwo measurements\nLet’s look at weight and wing length.\n\nHawks %&gt;%\n  ggplot(aes(x = Weight, y = Wing)) +\n  geom_point() +\n  theme_bw()\n\n\n\n\n\n# Standardize data\nHawks2 = Hawks %&gt;%\n  mutate(sWeight = (Weight - mean(Weight)) / sd(Weight),\n         sWing = (Wing - mean(Wing)) / sd(Wing))\n\nHawks2 %&gt;%\n  ggplot(aes(x = sWeight, y = sWing)) +\n  geom_point() +\n  theme_bw()\n\n\n\n\nHow can we describe the variability of the data?\n\n# Covariance matrix\ncovMatrix1 = Hawks %&gt;%\n  select(Weight, Wing) %&gt;%\n  cov()\n\ncovMatrix1\n\n          Weight      Wing\nWeight 214310.57 41247.975\nWing    41247.97  9085.273\n\ncovMatrix2 = Hawks2 %&gt;%\n  select(sWeight, sWing) %&gt;%\n  cov()\n\ncovMatrix2\n\n          sWeight     sWing\nsWeight 1.0000000 0.9347852\nsWing   0.9347852 1.0000000\n\n\nCovariance matrix \\(\\Sigma\\) collects variances and covariances together,\n\\[\n\\Sigma = \\begin{pmatrix}\\sigma_x^2 & \\sigma_{xy}^2\\\\\\ \\sigma_{xy}^2 & \\sigma_y^2\\end{pmatrix}\n\\] How can we visualize the covariance matrix above?\n\noffline example of matrix multiplication\n\nThe “matrix inverse” helps. The inverse of \\(\\Sigma\\) is denoted \\(\\Sigma^{-1}\\). The property of the inverse is:\n\\[\n\\Sigma^{-1} \\Sigma = \\begin{pmatrix}1 & 0\\\\\\ 0 & 1 \\end{pmatrix}\n\\]\n\\[\nz^T \\Sigma^{-1} z = c^2\n\\]\nwhere \\(z = (x, y)\\) and \\(\\Sigma^{-1} = \\begin{pmatrix}s_x^2 & s_{xy}^2\\\\\\ s_{xy}^2 & s_y^2\\end{pmatrix}\\). Where have we seen this before? Hint: ?pnorm or see multivariate normal\n\\[\n\\begin{pmatrix} x & y\\end{pmatrix}\n\\begin{pmatrix}s_x^2 & s_{xy}^2\\\\\\ s_{xy}^2 & s_y^2\\end{pmatrix}\n\\begin{pmatrix} x\\\\\\ y \\end{pmatrix}\n= c^2\n\\]\n\\[\n(x s_x^2 + y s_{xy}^2 \\ \\ \\ \\ x s_{xy}^2 + y s_y^2) \\begin{pmatrix} x\\\\\\ y \\end{pmatrix} = c^2\n\\]\n\\[\nx^2 s_x^2 + 2x y \\cdot s_{xy}^2 + y^2 s_y^2 = c^2\n\\] This is the equation of an ellipse.\n\n# Grab the points (x,y) that satisfy the equation above\nellipsePoints = data.frame(ellipse(covMatrix2))\n\nHawks2 %&gt;%\n  ggplot(aes(x = sWeight, y = sWing, color = Species)) +\n  geom_point() +\n  theme_bw() +\n  geom_point(aes(x = sWeight, y = sWing), data = ellipsePoints, color = 'steelblue')\n\n\n\n\nSet \\(c^2 = 6\\):\nTo make sure the function ellipse above is doing what we expect:\n\nFirst we get \\(\\Sigma^{-1}\\):\n\n\nsolve(covMatrix2)\n\n          sWeight     sWing\nsWeight  7.925401 -7.408548\nsWing   -7.408548  7.925401\n\n\nNext, we manually solve the quadratic equation using the quadratic formula:\n\ngetCoordinate = function(y, s1, s2, s12) {\n  A = s1\n  B = 2*y*s12\n  C = (y*s2) - 6\n  p1 = -1*B\n  p2 = sqrt(B^2 - (4*A*C))\n  p3 = 2*A\n  \n  coord1 = (p1 + p2) / p3\n  coord2 = (p1 - p2) / p3\n  return(c(coord1, coord2))\n}\n\ngetCoordinate(1, 7.925401, 7.925401, -7.408548)\n\n[1] 1.7290667 0.1405038\n\n\nThe axes of the ellipse provide the most informative directions to measure the data. In \\(n\\)-dimensions, where we have a \\(n\\)-dimensional ellipsoid, it can be useful to look at \\(p&lt;n\\) axes. The largest \\(p\\) axes are called the “principal components”.\n\nExample of “principle component analysis” in action: genes mirror geography within Europe and an associated news article discussing the work."
  },
  {
    "objectID": "ae/ae2.html",
    "href": "ae/ae2.html",
    "title": "Intro to R and EDA",
    "section": "",
    "text": "first lab tomorrow, due following Thursday at 11:59pm on Gradescope\nbe sure to complete prepare material (on the schedule) before each class"
  },
  {
    "objectID": "ae/ae2.html#bulletin",
    "href": "ae/ae2.html#bulletin",
    "title": "Intro to R and EDA",
    "section": "",
    "text": "first lab tomorrow, due following Thursday at 11:59pm on Gradescope\nbe sure to complete prepare material (on the schedule) before each class"
  },
  {
    "objectID": "ae/ae2.html#today",
    "href": "ae/ae2.html#today",
    "title": "Intro to R and EDA",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\npractice using glimpse(), names(), nrow(), ncol(), count()\ndefine and compute various statistics\nbegin to gain familiarity with ggplot"
  },
  {
    "objectID": "ae/ae2.html#getting-started",
    "href": "ae/ae2.html#getting-started",
    "title": "Intro to R and EDA",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console (bottom left of screen)\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae2.qmd\",\ndestfile = \"ae2.qmd\")\n\nA note on navigating RStudio\n\nSource code vs visual editor\ncode chunks and narrative\nfile system"
  },
  {
    "objectID": "ae/ae2.html#load-packages",
    "href": "ae/ae2.html#load-packages",
    "title": "Intro to R and EDA",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\n\n\nA package within a package…\n\nThe tidyverse package contains the dplyr and ggplot packages we will use today. ggplot contains functions for plotting data while dplyr contains tools to wrangle, manipulate and summarize data frames."
  },
  {
    "objectID": "ae/ae2.html#load-data",
    "href": "ae/ae2.html#load-data",
    "title": "Intro to R and EDA",
    "section": "Load data",
    "text": "Load data\n\nflint = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/flint.csv\")\n\n\nData dictionary\n\nid: sample ID number (identifies the home)\nzip: ZIP code in Flint of the sample’s location\nward: ward in Flint of the sample’s location\ndraw: which time point the water was sampled from\nlead: lead content in parts per billion"
  },
  {
    "objectID": "ae/ae2.html#goal",
    "href": "ae/ae2.html#goal",
    "title": "Intro to R and EDA",
    "section": "Goal",
    "text": "Goal\nWe want to learn about the population using a sample.\nIn the case we want to learn about the lead content in all of Flint, MI homes but only have available water readings from a sample of homes (our data set).\n\nExercise 1:\nLook at the data, how many observations are there? How many variables?\n[answer here]"
  },
  {
    "objectID": "ae/ae2.html#count",
    "href": "ae/ae2.html#count",
    "title": "Intro to R and EDA",
    "section": "Count",
    "text": "Count\nLet’s count() to find the number of different time points water was sampled.\n\ncount(flint, draw)\n\n# A tibble: 3 × 2\n  draw       n\n  &lt;chr&gt;  &lt;int&gt;\n1 first    271\n2 second   271\n3 third    271\n\n\nHow many unique homes are in the data set?\n\nflint %&gt;%\n  count(id)\n\n# A tibble: 269 × 2\n      id     n\n   &lt;dbl&gt; &lt;int&gt;\n 1     1     3\n 2     2     3\n 3     4     3\n 4     5     3\n 5     6     3\n 6     7     3\n 7     8     3\n 8     9     3\n 9    12     3\n10    13     3\n# ℹ 259 more rows\n\n\n\nA note on pipes %&gt;%"
  },
  {
    "objectID": "ae/ae2.html#exercise-2",
    "href": "ae/ae2.html#exercise-2",
    "title": "Intro to R and EDA",
    "section": "Exercise 2",
    "text": "Exercise 2\nFill in the code to see how many samples were taken from each zip code. Uncomment the lines (i.e. remove the # before running the code)\n\n# flint %&gt;% \n # count(______)\n\nWhich ZIP code had the most samples drawn?"
  },
  {
    "objectID": "ae/ae2.html#statistics",
    "href": "ae/ae2.html#statistics",
    "title": "Intro to R and EDA",
    "section": "Statistics",
    "text": "Statistics\nWhat is a statistic? It’s any mathematical function of the data. Sometimes, a statistic is referred to as “sample statistic” since you compute it from a sample (the data) and not the entire population.\n\nmeasure of central tendency:\n\nmean\nmedian\nmode\n\n\n\nmeasures of spread:\n\nvariance\nstandard deviation\nrange\nquartiles\ninter-quartile range (IQR)\n\n\n\norder statistics:\n\nquantiles\nminimum (0 percentile)\nmedian (50th percentile)\nmaximum (100 percentile)\n\n… and any other arbitrary function of the data you can come up with!\n\n\nExercise 3:\nCome up with your own statistic and write it in the narrative here.\nTo access a column of the data, we’ll use data$column.\nLet’s compute each of these statistics for lead ppb in R.\n\n# code here"
  },
  {
    "objectID": "ae/ae2.html#plotting",
    "href": "ae/ae2.html#plotting",
    "title": "Intro to R and EDA",
    "section": "Plotting",
    "text": "Plotting\nLet’s take a look at the distribution of lead content in homes in Flint, MI.\n\nflint %&gt;% # data\n  ggplot(aes(x = lead)) + # columns we want to look at\n  geom_histogram() # geometry of the visualization\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nWe can make this plot look nicer by adjusting the number of bins and/or the x-axis.\n\nflint %&gt;% # data\n  ggplot(aes(x = lead)) + # columns we want to look at\n  geom_histogram(bins = 50) + # geometry of the visualization\n  xlim(0, 100) # limit the x-axis to a certain range\n\n\n\n\nLet’s visualize some of our summary statistics on the plot.\n\nExercise 4:\nUn-comment the code below and fill in the blank with the mean.\n\nflint %&gt;% \n  ggplot(aes(x = lead)) + \n  geom_histogram(bins = 50) + \n  xlim(0,100) #+\n  #geom_vline(xintercept = __, color = 'red')\n\nAdd another geom_vline with the median. Use a separate color."
  },
  {
    "objectID": "ae/ae2.html#box-plots",
    "href": "ae/ae2.html#box-plots",
    "title": "Intro to R and EDA",
    "section": "Box plots",
    "text": "Box plots\nLet’s make some plots, where we will focus on zip codes 48503, 48504, 48505, 48506, and 48507. We will restrict our attention to samples with lead values less than 1,000 ppb.\n\nflint_focus &lt;- flint %&gt;% \n  filter(zip %in% 48503:48507, lead &lt; 1000)\n\nBelow are side-by-side box plots for the three flushing times in each of the five zip codes we considered. Add x and y labels; add a title by inserting title = \"title_name\" inside the labs() function.\n\nggplot(data = flint_focus, aes(x = factor(zip), y = lead)) +\n  geom_boxplot(aes(fill = factor(draw))) +\n  labs(x = \"--------\", y = \"--------\", fill = \"Flushing time\") +\n  scale_fill_discrete(breaks = c(\"first\", \"second\", \"third\"),\n                      labels = c(\"0 (sec)\", \"45 (sec)\", \"120 (sec)\")) +\n  coord_flip() +\n  theme_bw()\n\n\n\n\nAdd labels for x, y, a title, and subtitle to the code below to update the corresponding plot.\n\nggplot(data = flint_focus, aes(x = factor(zip), y = lead)) +\n  geom_boxplot(aes(fill = factor(draw))) + \n  labs(x = \"--------\", y = \"--------\", fill = \"Flushing time\",\n       subtitle = \"--------\") +\n  scale_fill_discrete(breaks = c(\"first\", \"second\", \"third\"),\n                      labels = c(\"0 (sec)\", \"45 (sec)\", \"120 (sec)\")) +\n  coord_flip(ylim = c(0, 50)) +\n  theme_bw()\n\n\n\n\nWhat is the difference between the two plots? What are the advantages and disadvantages to each plot?"
  },
  {
    "objectID": "ae/ae2.html#references",
    "href": "ae/ae2.html#references",
    "title": "Intro to R and EDA",
    "section": "References",
    "text": "References\n\nLangkjaer-Bain, R. (2017). The murky tale of Flint’s deceptive water data. Significance, 14: 16-21.\nKelsey J. Pieper, Rebekah Martin, Min Tang, LeeAnne Walters, Jeffrey Parks, Siddhartha Roy, Christina Devine, and Marc A. Edwards Environmental Science & Technology 2018 52 (15), 8124-8132 DOI: 10.1021/acs.est.8b00791"
  },
  {
    "objectID": "ae/ae16.html",
    "href": "ae/ae16.html",
    "title": "Central limit theorem",
    "section": "",
    "text": "Lab 6 due Thursday\nProject proposal due Friday"
  },
  {
    "objectID": "ae/ae16.html#bulletin",
    "href": "ae/ae16.html#bulletin",
    "title": "Central limit theorem",
    "section": "",
    "text": "Lab 6 due Thursday\nProject proposal due Friday"
  },
  {
    "objectID": "ae/ae16.html#today",
    "href": "ae/ae16.html#today",
    "title": "Central limit theorem",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nUse Central Limit Theorem to define distribution of sample means\nCalculate probabilities from the normal distribution\nUse Central Limit Theorem (CLT) to conduct inference on a population mean"
  },
  {
    "objectID": "ae/ae16.html#getting-started",
    "href": "ae/ae16.html#getting-started",
    "title": "Central limit theorem",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae16.qmd\",\ndestfile = \"ae16.qmd\")"
  },
  {
    "objectID": "ae/ae16.html#load-packages",
    "href": "ae/ae16.html#load-packages",
    "title": "Central limit theorem",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "ae/ae16.html#load-data",
    "href": "ae/ae16.html#load-data",
    "title": "Central limit theorem",
    "section": "Load data",
    "text": "Load data\n\nmanhattan = read_csv(\n  \"https://sta101-fa22.netlify.app/static/appex/data/manhattan.csv\"\n  )"
  },
  {
    "objectID": "ae/ae16.html#notes",
    "href": "ae/ae16.html#notes",
    "title": "Central limit theorem",
    "section": "Notes",
    "text": "Notes\nHow do we know when to expect a normal distribution to show up?\nLast time we saw an example where the distribution of sample means looked approximately normal but the distribution of sample medians was not.\nExample:\n\nset.seed(1)\nboot_dist = manhattan %&gt;%\n  specify(response = rent) %&gt;% \n  generate(reps = 1000, type = \"bootstrap\")\n\nboot_dist %&gt;%\n  calculate(stat = \"mean\") %&gt;%\n  visualize() +\n  labs(x = \"Sample mean\", \n       title = \"Simulated distribution of the sample mean\")\n\n\n\nboot_dist %&gt;%\n  calculate(stat = \"median\") %&gt;%\n  visualize() + \n  labs(x = \"Sample median\",\n       title = \"Simulated distribution of the sample median\")\n\n\n\n\nAre there times when the sample mean will not look normal?\n\nExercise 1\nThe proportion of observed successes for a binary variable is a sample mean.\nScenario: You flip a biased coin numFlips times and compute the sample mean (the proportion of flips that land heads). You repeat this experiment 1000 times and obtain a distribution of sample means.\n\nHow does the shape of the distribution change as you increase the number of coin flips per sample?\n\n\nset.seed(714)\nnumFlips = 1\nnumHeads = rbinom(n = 1000, size = numFlips, prob = 0.9)\ndf = data.frame(numHeads) # new data frame called df\ndf %&gt;%\nmutate(propHeads = numHeads / numFlips) %&gt;%\nggplot(aes(x = propHeads)) +\ngeom_histogram(binwidth = .01)"
  },
  {
    "objectID": "ae/ae16.html#what-is-the-central-limit-theorem",
    "href": "ae/ae16.html#what-is-the-central-limit-theorem",
    "title": "Central limit theorem",
    "section": "What is the central limit theorem?",
    "text": "What is the central limit theorem?\nThe central limit theorem is a statement about the distribution of the sample mean, \\(\\bar{x}\\).\nThe central limit theorem guarantees that, when certain criteria are satisfied, the sample mean (\\(\\bar{x}\\)) is normally distributed.\nSpecifically, if\n\nObservations in the sample are independent. Two rules of thumb to check this:\n\ncompletely random sampling\nif sampling without replacement, sample should be less than 10% of the population size\n\n\nand\n\nThe sample is large enough. The required size varies in different contexts, but some good rules of thumb are:\n\nif the population itself is normal, sample size does not matter.\nif numerical require, &gt;30 observations\nif binary outcome, at least 10 successes and 10 failures.\n\n\nthen\n\\[\n\\bar{x} \\sim N(\\mu, \\sigma / \\sqrt{n})\n\\]\ni.e. \\(\\bar{x}\\) is normally distributed (unimodal and symmetric with bell shape) with mean \\(\\mu\\) and standard deviation \\(\\sigma / \\sqrt{n}\\). The standard deviation of the sampling distribution is called the standard error.\n\n\n\n\n\n\nNote\n\n\n\nThe standard deviation of the sample mean depends on the number of samples, \\(n\\)."
  },
  {
    "objectID": "ae/ae16.html#practice-using-clt-normal-distribution",
    "href": "ae/ae16.html#practice-using-clt-normal-distribution",
    "title": "Central limit theorem",
    "section": "Practice using CLT & Normal distribution",
    "text": "Practice using CLT & Normal distribution\nSuppose the bone density for 65-year-old women is normally distributed with mean \\(809 mg/cm^3\\) and standard deviation of \\(140 mg/cm^3\\).\nLet \\(x\\) be the bone density of 65-year-old women. We can write this distribution of \\(x\\) in mathematical notation as\n\\[x \\sim N(809, 140)\\]"
  },
  {
    "objectID": "ae/ae16.html#visualize-the-population-distribution",
    "href": "ae/ae16.html#visualize-the-population-distribution",
    "title": "Central limit theorem",
    "section": "Visualize the population distribution",
    "text": "Visualize the population distribution\n\nggplot(data = data.frame(x = c(809 - 140*3, 809 + 140*3)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 809, sd = 140),\n                color = \"black\") +\n  stat_function(fun = dnorm, args = list(mean = 809, sd = 140/sqrt(10)),\n                color = \"red\",lty = 2) + theme_bw() +\n  labs(title = \"Black solid line = population dist., Red dotted line = sampling dist.\")\n\n\n\n\n\nExercise 2\nBefore typing any code, based on what you know about the normal distribution, what do you expect the median bone density to be?\nWhat bone densities correspond to \\(Q_1\\) (25th percentile), \\(Q_2\\) (50th percentile), and \\(Q_3\\) (the 75th percentile) of this distribution? Use the qnorm() function to calculate these values.\n\n\nExercise 3\nThe densities of three woods are below:\n\nPlywood: 540 mg/cubic centimeter\nPine: 600 mg/cubic centimeter\nMahogany: 710 mg/cubic centimeter\nWhat is the probability that a randomly selected 65-year-old woman has bones less dense than Pine?\nWould you be surprised if a randomly selected 65-year-old woman had bone density less than Mahogany? What if she had bone density less than Plywood? Use the respective probabilities to support your response.\n\n\n\nExercise 4\nSuppose you want to analyze the mean bone density for a group of 10 randomly selected 65-year-old women.\n\nAre the conditions for the Central Limit Theorem met?\n\nIndependence?\nSample size/distribution?\n\nWhat is the shape, center, and spread of the distribution of \\(\\bar{x}\\), the mean bone density for a group of 10 randomly selected 65-year-old women?\nWrite the distribution of \\(\\bar{x}\\) using mathematical notation.\n\n\n\nExercise 5\n\nWhat is the probability that the mean bone density for the group of 10 randomly-selected 65-year-old women is less dense than Pine?\nWould you be surprised if a group of 10 randomly-selected 65-year old women had a mean bone density less than Mahogany? What the group had a mean bone density less than Plywood? Use the respective probabilities to support your response.\n\n\n\nExercise 6\nExplain how your answers differ in Exercises 3 and 5."
  },
  {
    "objectID": "ae/ae16.html#on-your-own",
    "href": "ae/ae16.html#on-your-own",
    "title": "Central limit theorem",
    "section": "On your own",
    "text": "On your own\nSuppose the distribution of the number of minutes users engage with apps on an iPad has a mean of 8.2 minutes and standard deviation of 1 minute. Let \\(x\\) be the number of minutes users engage with apps on an iPad, \\(\\mu\\) be the population mean and \\(\\sigma\\) the population standard deviation. Then,\n\\[x \\sim N(8.2, 1)\\]\nSuppose you take a sample of 60 randomly selected app users and calculate the mean number of minutes they engage with apps on an iPad, \\(\\bar{x}\\). The conditions (independence & sample size/distribution) to apply the Central Limit Theorem are met. Then by the Central Limit Theorem\n\\[\\bar{x} \\sim N(8.2, 1/\\sqrt{60})\\]\n\nWhat is the probability a randomly selected user engages with iPad apps for more than 8.3 minutes? Use pnorm for calculations.\n\n#add code\n\nWhat is the probability the mean minutes of app engagement for a group of 60 randomly selected iPad users is more than 8.3 minutes? Use pnorm for calculations.\n\n#add code\n\nWhat is the probability the mean minutes of app engagement for a group of 60 randomly selected iPad users is between 8.3 and 8.4 minutes? Use pnorm for calculations.\n\n\n    #add code"
  },
  {
    "objectID": "ae/ae10.html",
    "href": "ae/ae10.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Regression project released"
  },
  {
    "objectID": "ae/ae10.html#bulletin",
    "href": "ae/ae10.html#bulletin",
    "title": "Logistic Regression",
    "section": "",
    "text": "Regression project released"
  },
  {
    "objectID": "ae/ae10.html#today",
    "href": "ae/ae10.html#today",
    "title": "Logistic Regression",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nunderstand logistic regression as a linear model of binary outcomes\nbe able to fit logistic regression in R"
  },
  {
    "objectID": "ae/ae10.html#getting-started",
    "href": "ae/ae10.html#getting-started",
    "title": "Logistic Regression",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae10.qmd\",\ndestfile = \"ae10.qmd\")"
  },
  {
    "objectID": "ae/ae10.html#load-packages-and-data",
    "href": "ae/ae10.html#load-packages-and-data",
    "title": "Logistic Regression",
    "section": "Load packages and data",
    "text": "Load packages and data\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(boot) # contains inv.logit() function\n\nTo illustrate logistic regression, we will build a spam filter from email data. Today’s data consists of 4601 emails that are classified as spam or non-spam. The data was collected at Hewlett-Packard labs and contains 58 variables. The first 48 variables are specific keywords and each observation is the percentage of appearance (frequency) of that word in the message. Click here to read more.\n\ntype \\(= 1\\) is spam\ntype \\(= 0\\) is non-spam\n\n\nspam = read_csv(\"https://sta101.github.io/static/appex/data/spam.csv\")\nglimpse(spam)\n\nRows: 4,601\nColumns: 58\n$ make              &lt;dbl&gt; 0.00, 0.21, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.15…\n$ address           &lt;dbl&gt; 0.64, 0.28, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ all               &lt;dbl&gt; 0.64, 0.50, 0.71, 0.00, 0.00, 0.00, 0.00, 0.00, 0.46…\n$ num3d             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ our               &lt;dbl&gt; 0.32, 0.14, 1.23, 0.63, 0.63, 1.85, 1.92, 1.88, 0.61…\n$ over              &lt;dbl&gt; 0.00, 0.28, 0.19, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ remove            &lt;dbl&gt; 0.00, 0.21, 0.19, 0.31, 0.31, 0.00, 0.00, 0.00, 0.30…\n$ internet          &lt;dbl&gt; 0.00, 0.07, 0.12, 0.63, 0.63, 1.85, 0.00, 1.88, 0.00…\n$ order             &lt;dbl&gt; 0.00, 0.00, 0.64, 0.31, 0.31, 0.00, 0.00, 0.00, 0.92…\n$ mail              &lt;dbl&gt; 0.00, 0.94, 0.25, 0.63, 0.63, 0.00, 0.64, 0.00, 0.76…\n$ receive           &lt;dbl&gt; 0.00, 0.21, 0.38, 0.31, 0.31, 0.00, 0.96, 0.00, 0.76…\n$ will              &lt;dbl&gt; 0.64, 0.79, 0.45, 0.31, 0.31, 0.00, 1.28, 0.00, 0.92…\n$ people            &lt;dbl&gt; 0.00, 0.65, 0.12, 0.31, 0.31, 0.00, 0.00, 0.00, 0.00…\n$ report            &lt;dbl&gt; 0.00, 0.21, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ addresses         &lt;dbl&gt; 0.00, 0.14, 1.75, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ free              &lt;dbl&gt; 0.32, 0.14, 0.06, 0.31, 0.31, 0.00, 0.96, 0.00, 0.00…\n$ business          &lt;dbl&gt; 0.00, 0.07, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ email             &lt;dbl&gt; 1.29, 0.28, 1.03, 0.00, 0.00, 0.00, 0.32, 0.00, 0.15…\n$ you               &lt;dbl&gt; 1.93, 3.47, 1.36, 3.18, 3.18, 0.00, 3.85, 0.00, 1.23…\n$ credit            &lt;dbl&gt; 0.00, 0.00, 0.32, 0.00, 0.00, 0.00, 0.00, 0.00, 3.53…\n$ your              &lt;dbl&gt; 0.96, 1.59, 0.51, 0.31, 0.31, 0.00, 0.64, 0.00, 2.00…\n$ font              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ num000            &lt;dbl&gt; 0.00, 0.43, 1.16, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ money             &lt;dbl&gt; 0.00, 0.43, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.15…\n$ hp                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ hpl               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ george            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ num650            &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ lab               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ labs              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ telnet            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ num857            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ data              &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.15…\n$ num415            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ num85             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ technology        &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ num1999           &lt;dbl&gt; 0.00, 0.07, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ parts             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pm                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ direct            &lt;dbl&gt; 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ cs                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ meeting           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ original          &lt;dbl&gt; 0.00, 0.00, 0.12, 0.00, 0.00, 0.00, 0.00, 0.00, 0.30…\n$ project           &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ re                &lt;dbl&gt; 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ edu               &lt;dbl&gt; 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ table             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ conference        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ charSemicolon     &lt;dbl&gt; 0.000, 0.000, 0.010, 0.000, 0.000, 0.000, 0.000, 0.0…\n$ charRoundbracket  &lt;dbl&gt; 0.000, 0.132, 0.143, 0.137, 0.135, 0.223, 0.054, 0.2…\n$ charSquarebracket &lt;dbl&gt; 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.0…\n$ charExclamation   &lt;dbl&gt; 0.778, 0.372, 0.276, 0.137, 0.135, 0.000, 0.164, 0.0…\n$ charDollar        &lt;dbl&gt; 0.000, 0.180, 0.184, 0.000, 0.000, 0.000, 0.054, 0.0…\n$ charHash          &lt;dbl&gt; 0.000, 0.048, 0.010, 0.000, 0.000, 0.000, 0.000, 0.0…\n$ capitalAve        &lt;dbl&gt; 3.756, 5.114, 9.821, 3.537, 3.537, 3.000, 1.671, 2.4…\n$ capitalLong       &lt;dbl&gt; 61, 101, 485, 40, 40, 15, 4, 11, 445, 43, 6, 11, 61,…\n$ capitalTotal      &lt;dbl&gt; 278, 1028, 2259, 191, 191, 54, 112, 49, 1257, 749, 2…\n$ type              &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\n\nThe basic logic of our model is that the frequency of certain words can help us determine whether or not an email is spam.\nFor example, these emails came from George’s inbox. If the word “george” is not present in the message and the dollar symbol (charDollar) is, you might expect the email to be spam.\nUsing this data, we want to build a model that predicts whether a new email is spam or not. How do we build a model that can do this?\n\nExercise 1\nStart by examining 1 predictor.\n\nVisualize a linear model where the outcome is type (spam or not) and george is the predictor.\nDiscuss your visualization with your neighbor. Is this a good model? Why or why not?\n\n\n# code here"
  },
  {
    "objectID": "ae/ae10.html#example",
    "href": "ae/ae10.html#example",
    "title": "Logistic Regression",
    "section": "Example",
    "text": "Example\nLet’s build a model centered around just two predictor variables.\nThe first will be the word you and the second will be capitalTotal (the total number of capital letters in the message).\n\nExercise 2\nCreate a visualization with you on the x-axis and capitalTotal on the y-axis. Color data points by whether or not they are spam.\n\n# code here\n\nLet’s fit the model!\n\nfit_1 = logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  fit(as.factor(type) ~ you + capitalTotal, data = spam, family = \"binomial\")\n  \nfit_1 %&gt;%\n  tidy()\n\n# A tibble: 3 × 5\n  term         estimate std.error statistic   p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  -1.50     0.0554       -27.1 2.97e-162\n2 you           0.361    0.0198        18.3 1.84e- 74\n3 capitalTotal  0.00173  0.000104      16.6 5.66e- 62\n\n\n\n\nExercise 3\n\nWhat is different in the code above from previous linear models we fit?\n\n\n\nExercise 4\n\nWhat is the probability the email is spam if the frequency of you is 5% in the email and there are 2500 capital letters. Use the model equation above.\nWhat is the log-odds? (Recall from the prep that log-odds \\(= \\log \\frac{p}{1-p}\\)). Use the code below to check your work.\n\n\nnewdata = data.frame(you = 5, capitalTotal = 2500)\n\n# code here\n\n# check work\ncheckLogOdds = predict(fit_1$fit, newdata)\ncheckLogOdds\n\n       1 \n4.633134 \n\ncheckP = inv.logit(checkLogOdds)\ncheckP\n\n        1 \n0.9903694"
  },
  {
    "objectID": "ae/ae10.html#visualize-logistic-regression",
    "href": "ae/ae10.html#visualize-logistic-regression",
    "title": "Logistic Regression",
    "section": "Visualize logistic regression",
    "text": "Visualize logistic regression\n\nbeta = fit_1$fit$coefficients\nhyperplane = function(x){\n    decisionBoundary = 0.5\n    c = logit(decisionBoundary)\n    const = c - beta[1]\n    return((-beta[2]*x + const) / beta[3])\n}\n\nspam %&gt;%\n  ggplot(aes(x = you, y = capitalTotal, color = as.factor(type))) + \n  geom_point(alpha = 0.3) +\n  geom_function(fun = hyperplane) +\n  scale_colour_manual(values = c(\"orange\", \"steelblue\")) +\n  theme_minimal()\n\nWarning: Multiple drawing groups in `geom_function()`\nℹ Did you use the correct group, colour, or fill aesthetics?\n\n\n\n\n\n\nJust because there’s greater than 50% probability an email is spam doesn’t mean we have to label it as such. We can adjust our threshold or critical probability, a.k.a. decision boundary to be more or less sensitive to spam emails.\n\nIn other words we get to select a number \\(p^*\\) such that\nif \\(p &gt; p^*\\), then label the email as spam.\n\nExercise 5\n\nWhat would you set your decision boundary to and why?\nChange decisionBoundary in the code above to 0.01 and 0.999999. Do the results surprise you? Why or why not?\nlower boundary means that we label more emails as spam, high boundary means fewer emails as spam. We can adjust the boundary depending on how much we value receiving important emails vs how much we dislike spam.\n0 means all emails are spam, 1 means no emails are spam. Note you cannot set decision boundary to 0 or 1 because of logit function (would evaluate to inf or negative inf)"
  },
  {
    "objectID": "ae/ae10.html#classify-a-new-email",
    "href": "ae/ae10.html#classify-a-new-email",
    "title": "Logistic Regression",
    "section": "Classify a new email",
    "text": "Classify a new email\n\nemail = readLines(\"https://sta101.github.io/static/appex/data/test-email.txt\")\nemail\n\n[1] \"You Have Been Selected To Win A Free Trip To Disney World! \"\n[2] \"\"                                                           \n[3] \"YOU HAVE 30 SECONDS TO CLICK HERE TO CLAIM YOUR REWARD!\"    \n[4] \"\"                                                           \n[5] \"WHAT ARE YOU WAITING FOR? ACT NOW!\"                         \n[6] \"\"                                                           \n[7] \"SINCERELY,\"                                                 \n[8] \"\"                                                           \n[9] \"WALT DISNEY\"                                                \n\ntotalWord = sum(str_count(email, \" \"))\ntotalYou = sum(str_count(tolower(email), \"you\"))\ncapitalTotal = sum(str_count(email, \"[A-Z]\"))\n\nyouFreq = 100 * totalYou / totalWord\nnewemail = data.frame(you = youFreq, capitalTotal = capitalTotal)\n\nlogOdds = predict(fit_1$fit, newemail)\nlogOdds\n\n       1 \n3.648776 \n\ninv.logit(logOdds)\n\n        1 \n0.9746371 \n\n\n\nExercise 6\n\nDoes the code above count the correct number of “you”? Why or why not?\nDo you believe the predicted odds of the email being spam? Why or why not?\nWhat is the probability the test email is spam?"
  },
  {
    "objectID": "ae/ae10.html#assessing-predictive-ability",
    "href": "ae/ae10.html#assessing-predictive-ability",
    "title": "Logistic Regression",
    "section": "Assessing predictive ability",
    "text": "Assessing predictive ability\nWe will divide the data into a training set and testing set.\n\nset.seed(6)\nsampleIndices = sample.int(n = nrow(spam), size = 2000, replace = F)\ntrain = spam[sampleIndices, ]\ntest  = spam[-sampleIndices, ] %&gt;%\n  slice_sample(n = 2000)\n\n\nExercise 7\nNext, let’s train your model on the training set. Build a predictive model using any combination of predictors from spam. Save your fitted model as myModel\n\n# code here\n\n#example (delete this):\nmyModel = logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  fit(as.factor(type) ~ you + address, data = train, family = \"binomial\")\n\nand test it on the testing set,\n\nprediction = test %&gt;%\n  mutate(myModelPrediction = predict(myModel, test)$.pred_class) \n\nprediction\n\n# A tibble: 2,000 × 59\n    make address   all num3d   our  over remove internet order  mail receive\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1  0       0     0.7      0  0     0.14   0        0     0.28  0       0   \n 2  0       0     0        0  0     0      0        0     0     0       0   \n 3  0       0     2.5      0  0     0      0        0     0     0       0   \n 4  0       2.08  0        0  3.12  0      1.04     0     0     0       0   \n 5  0       0     1.04     0  1.04  0      0        1.39  0.34  0       0   \n 6  0       0.35  0.7      0  0.35  0      0        0     0     0       0   \n 7  0.39    0     0        0  1.17  0      0        0     0     0.39    0   \n 8  0       0.25  0.75     0  1     0.25   0        0     0     0       0.25\n 9  0       0     0        0  0.87  0      0        0     0     0       1.31\n10  0       0     0        0  1.11  0      0        0.55  0     3.91    0   \n# ℹ 1,990 more rows\n# ℹ 48 more variables: will &lt;dbl&gt;, people &lt;dbl&gt;, report &lt;dbl&gt;, addresses &lt;dbl&gt;,\n#   free &lt;dbl&gt;, business &lt;dbl&gt;, email &lt;dbl&gt;, you &lt;dbl&gt;, credit &lt;dbl&gt;,\n#   your &lt;dbl&gt;, font &lt;dbl&gt;, num000 &lt;dbl&gt;, money &lt;dbl&gt;, hp &lt;dbl&gt;, hpl &lt;dbl&gt;,\n#   george &lt;dbl&gt;, num650 &lt;dbl&gt;, lab &lt;dbl&gt;, labs &lt;dbl&gt;, telnet &lt;dbl&gt;,\n#   num857 &lt;dbl&gt;, data &lt;dbl&gt;, num415 &lt;dbl&gt;, num85 &lt;dbl&gt;, technology &lt;dbl&gt;,\n#   num1999 &lt;dbl&gt;, parts &lt;dbl&gt;, pm &lt;dbl&gt;, direct &lt;dbl&gt;, cs &lt;dbl&gt;, …\n\n\n\n\nExercise 8\nWhat is the proportion of false positives (i.e. classified as spam but was not)? False negatives?\n\n# code here"
  },
  {
    "objectID": "ae/ae19.html",
    "href": "ae/ae19.html",
    "title": "Tidy hypotheses",
    "section": "",
    "text": "Lab 7 due tonight"
  },
  {
    "objectID": "ae/ae19.html#bulletin",
    "href": "ae/ae19.html#bulletin",
    "title": "Tidy hypotheses",
    "section": "",
    "text": "Lab 7 due tonight"
  },
  {
    "objectID": "ae/ae19.html#today",
    "href": "ae/ae19.html#today",
    "title": "Tidy hypotheses",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nhypothesis test the tidy way\npractice more than testing proportions"
  },
  {
    "objectID": "ae/ae19.html#getting-started",
    "href": "ae/ae19.html#getting-started",
    "title": "Tidy hypotheses",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae19.qmd\",\ndestfile = \"ae19.qmd\")"
  },
  {
    "objectID": "ae/ae19.html#load-packages",
    "href": "ae/ae19.html#load-packages",
    "title": "Tidy hypotheses",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "ae/ae19.html#notes",
    "href": "ae/ae19.html#notes",
    "title": "Tidy hypotheses",
    "section": "Notes",
    "text": "Notes\n\nNot just a coin flip\n\nHere’s an example of testing a proportion outside the context of coins.\n\n\npush_pull = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/push_pull.csv\")\n\n\npush_pull %&gt;%\n  slice(1:3, 24:26)\n\n# A tibble: 6 × 7\n  participant_id   age push1 push2 pull1 pull2 training\n           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   \n1              1    41    41    45    16    17 density \n2              2    32    35    44     9    11 density \n3              3    44    33    38    10    11 density \n4             24    36    31    60     9    15 gtg     \n5             25    50    35    42     9    12 gtg     \n6             26    34    23    39     9    13 gtg     \n\n\nThe push_pull dataset comes from a “mini study” by mountain tactical institute.\n26 individuals completed 1 of 2 exercise regiments for 3.5 weeks to increase their pushups and pullups. Codebook below:\n\nparticipant_id: unique identifier for each participant\nage: age of participant\npush1/2: push-ups at beginning and end of program respectively\npull1/2: pull-ups at beginning and end of program respectively\ntraining: which training protocol the individual participated in\n\n\npush_pull = push_pull %&gt;%\n  mutate(\n    pct_push_inc = (push2 / push1 ) - 1,\n    pct_pull_inc = (pull2 / pull1) - 1)\n\nHypothesis: “Most people who train consistently will see at least a 15% increase in push-ups over a 3.5 week training period.”\nBreaking it down:\n\n“Most” i.e. “greater than 50%” indicates we should examine a proportion.\n\n\nExercise 1\nWhat’s the null?\n\n“will see at least a 15% increase”. Each person either increases by 15% over 3.5 weeks or does not. This is our binary outcome.\ncreate a new column called over_15pct that tells you whether or not an individual achieved at least a 15% increase in push-ups\n\n\n# code here\n\n\nWhat would be a default theory (null hypothesis)?\n\n\n\nExercise 2\nWrite the null and alternative in mathematical notation.\n\n\nExercise 3\nWhat is the observed statistic? Compute and write it in mathematical notation.\n\n\nExercise 4\nNext, simulate under the null and compute the p-value. State your conclusion with \\(\\alpha = 0.05\\). As a bonus, visualize the null distribution and shade in the p-value.\n\n\n\nMore than a proportion\n\nWhat if we want to make a claim about a different population parameter than a proportion? Maybe a mean, or median? We can’t necessarily flip a coin. The answer, is once again, bootstrap sampling.\n\nHypothesis: “The mean age of push-up/pull-up training participants is greater than 30”.\nLet’s investigate this hypothesis with a significance level \\(\\alpha = 0.01\\).\n\nExercise 5\nWrite down the null and alternative hypotheses in words and mathematical notation\n\n\nExercise 6\nWhat is the observed statistic? Write it in mathematical notation.\nBootstrapping does the following…\n\n# find observed statistic\nobs_mean_age = push_pull %&gt;%\n  drop_na(age) %&gt;%\n  summarize(meanAge = mean(age)) %&gt;%\n  pull()\n# subtract observed_mean - desired_mean from age\nage_and_null = push_pull %&gt;%\n  select(age) %&gt;%\n  drop_na(age) %&gt;%\n  mutate(nullAge = age - (obs_mean_age - 30))\n# show data frame\nage_and_null\n\n# A tibble: 25 × 2\n     age nullAge\n   &lt;dbl&gt;   &lt;dbl&gt;\n 1    41    35.8\n 2    32    26.8\n 3    44    38.8\n 4    37    31.8\n 5    37    31.8\n 6    21    15.8\n 7    33    27.8\n 8    38    32.8\n 9    49    43.8\n10    33    27.8\n# ℹ 15 more rows\n\n# show the means of each column\nage_and_null %&gt;%\n  summarize(meanAge = mean(age),\n  mean_nullAge = mean(nullAge))\n\n# A tibble: 1 × 2\n  meanAge mean_nullAge\n    &lt;dbl&gt;        &lt;dbl&gt;\n1    35.2           30\n\n\nIf we take bootstrap samples from this new nullAge column, we are sampling from data with the same variability as our original data, but a different mean. This is a nice way to explore the null!\n\nset.seed(3)\n\n# simulate null\nnull_dist = push_pull %&gt;%\n  specify(response = age) %&gt;%\n  hypothesize(null = \"point\", mu = 30) %&gt;%\n  generate(reps = 10000, type = \"bootstrap\") %&gt;%\n  calculate(stat = \"mean\")\n\n# get observed statistic\nobs_stat = obs_mean_age\n\np_value = null_dist %&gt;%\n  get_p_value(obs_stat, direction = \"right\")\n\np_value\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1  0.0001\n\n\n\nThe p-value 1e-04 is less than \\(\\alpha = 0.01\\). I reject the null hypothesis. In context, there is evidence to suggest that average push/pull trainee age is older than 30 years old.\n\n\n\nExercise 7\nSay we are interested in the performance of trainees at this particular facility and the sample is representative of the population.\nHypothesis: The median number of pull-ups trainees can perform is less than 20 even after training for 3.5 weeks.\nWrite down the null and alternative hypothesis in mathematical notation.\n\n\nExercise 8\nWrite down the observed statistic. Simulate under the null and compute the p-value. Finally, visualize and interpret the p-value in context."
  },
  {
    "objectID": "ae/ae19.html#summary",
    "href": "ae/ae19.html#summary",
    "title": "Tidy hypotheses",
    "section": "Summary",
    "text": "Summary\n\nHypothesis testing procedure\n\nSpecify the null and alternative hypothesis. Choose or know \\(\\alpha\\).\nCollect/examine the data. Compute the observed statistic.\nSimulate under the null and compute the p-value using the observed statistic and the alternative hypothesis.\nCompare the p-value to your significance level \\(\\alpha\\) and reject or fail to reject the null. Interpret your result in context."
  },
  {
    "objectID": "ae/ae19.html#which-training-method-is-better",
    "href": "ae/ae19.html#which-training-method-is-better",
    "title": "Tidy hypotheses",
    "section": "Which training method is better?",
    "text": "Which training method is better?\nTwo exercise regimes:\n\n“density” training\n“grease-the-groove” (gtg)\n\nWe want to know, is the average pull-up percent increase of a gtg trainee significantly different than a density trainee?\nFundamentally, does the categorical variable training affect the average percentage increase in pull-ups?\nState the null hypothesis:\n\\[\n\\mu_d = \\mu_{gtg}\n\\]\n\\[\nH_0: \\mu_d - \\mu_{gtg} = 0\n\\]\nWhat we want to do to simulate data under this null:\n\nrandom_training = sample(push_pull$training, replace = FALSE)\n\npush_pull %&gt;%\n  select(pct_pull_inc) %&gt;%\n  mutate(random_training = random_training)\n\n\nExercise 9:\n\nComplete the hypothesis specification above by stating the alternative. Check the observed statistic reported below.\n\n\n# code here\n\nSimulating under the null and computing the p-value:\n\nsim_num = 10000\nset.seed(1)\n# simulate null\nnull_dist = push_pull %&gt;%\n  specify(response = pct_pull_inc, explanatory = training) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = sim_num, type = \"permute\") %&gt;%\n  calculate(stat = \"diff in means\", order = c(\"density\", \"gtg\"))\n# observed statistic\nobs_stat = .196 - .489\n# visualize / get p\nvisualize(null_dist) +\n  shade_p_value(obs_stat, direction = \"both\")\n\n\n\n\n\n\nExercise 10\nCompute the p-value and state your conclusion with \\(\\alpha = 0.05\\)"
  },
  {
    "objectID": "ae/ae19.html#summary-of-generate-options",
    "href": "ae/ae19.html#summary-of-generate-options",
    "title": "Tidy hypotheses",
    "section": "Summary of generate() options",
    "text": "Summary of generate() options\n\n1. draw\n\ndescription: flip a coin with probability p, or roll a die with probabilities associated with each side. See here for reference to the multinomial setting.\ntypical case: test the proportion of a binary outcome\nnull: proportion \\(p\\) is some fixed number\nExample (hypothesis test for a single proportion):\n\n\n\n2. bootstrap\n\ndescription: re-sample your data with replacement\ntypical case (in hypothesis testing): does the mean equal a specific value? Does the median equal a specific value?\nnull: what would the data have looked like if nothing but the point estimate changed?\n\n\n\n3. permute\n\ndescription: permutes variables\ntypical case: is there a difference in the outcome between groups?\nassociated null: group membership does not matter i.e. group A and group B have the same outcome\nExample (test for independence):"
  },
  {
    "objectID": "ae/ae23.html",
    "href": "ae/ae23.html",
    "title": "Project Tips",
    "section": "",
    "text": "Reminders\n\nDraft final project report due Friday December 2. Peer-review in this lab.\nLab 09 due Thursday December 1st."
  },
  {
    "objectID": "ae/ae23.html#bulletin",
    "href": "ae/ae23.html#bulletin",
    "title": "Project Tips",
    "section": "",
    "text": "Reminders\n\nDraft final project report due Friday December 2. Peer-review in this lab.\nLab 09 due Thursday December 1st."
  },
  {
    "objectID": "ae/ae23.html#today",
    "href": "ae/ae23.html#today",
    "title": "Project Tips",
    "section": "Today",
    "text": "Today\nBy the end of today you will practice a few quarto/markdown tricks to polish your report and simplify your presentation. Specifically we will discuss:\n\ncode chunk settings\ncitations\nkable() tables\nquarto presentations"
  },
  {
    "objectID": "ae/ae23.html#getting-started",
    "href": "ae/ae23.html#getting-started",
    "title": "Project Tips",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae23.qmd\",\ndestfile = \"ae23.qmd\")\nYou can also download the references.bib file using the code below.\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/references.bib\",\ndestfile = \"references.bib\")"
  },
  {
    "objectID": "ae/ae23.html#code-chunk-settings",
    "href": "ae/ae23.html#code-chunk-settings",
    "title": "Project Tips",
    "section": "Code chunk settings",
    "text": "Code chunk settings\nSome options available for customizing output (see quarto documenation for more detail).\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\neval\nEvaluate the code chunk (if false, just echos the code into the output)\n\n\necho\nInclude the source code in output\n\n\nwarning\nInclude warnings in the output\n\n\nmessage\nWhether to preserve messages emitted by message() (similar to the option warning)\n\n\ninclude\nCatch all for preventing any output (code or results) from being included (e.g. include: false suppresses all output from the code block)\n\n\n\nThese options can be applied globally (the whole document) or locally (a specific code chunk). Global settings are controlled in the YAML (see the top of the document) while local code chunk options can be applied with #| (see example below).\n\nExercise 1\nIn the code chunk below:\n\nset warning to false\nset echo to false\n\nand re-render.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(knitr)\n\nIn addition to code chunks, figures have settings as well.\nWe can set captions and an alt attributes using #| fig-cap: and #| fig-alt: respectively. alt captions specify “alternate text” for an image. Alternative text appears if an image cannot be displayed and is also read by screen-readers.\nAdditional figure options include\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\nfig-width\nfigure width in inches\n\n\nfig-height\nfigure height in inches\n\n\nfig.align\ne.g. fig.align: center centers figure alignment\n\n\nfig.asp\nchanges figure height based on aspect ratio with width\n\n\nout.width\nsets figure width relative to text (1000 = 100% text width), e.g. out.width: 1000\n\n\n\nIn all cases above, we can again set options locally or globally. Note: local options override global options.\n\n\nExercise 2\nAdd a figure caption to the figure below. Next, change the output width to be 50% of the text. Finally, align the figure with the center of the page.\n\nstarwars %&gt;%\n  ggplot(aes(x = height)) +\n  geom_density() +\n  labs(x = \"Height (cm)\", y = \"Density\") +\n  theme_bw()\n\nWarning: Removed 6 rows containing non-finite values (`stat_density()`).\n\n\n\n\n\n\n\nProject specific notes\nFor the project, you will set the option echo: FALSE and warning: FALSE to hide all code and warnings in your final report.\nSuggestion: make your figures consistently themed, e.g. use similar figure size/aspect ratio and color scheme throughout your report. Change the default gray background, see themes.\n\n\nExercise 3\nChange the global code chunk settings so the document is formatted as your final project will be. Render and take a look at the updated PDF."
  },
  {
    "objectID": "ae/ae23.html#citations",
    "href": "ae/ae23.html#citations",
    "title": "Project Tips",
    "section": "Citations",
    "text": "Citations\nYour report will include citations, e.g. the data source, previous research, and other sources as needed. At a minimum, you should have a citation for the data source.\nAll of your bibliography entries will be stored in a .bib file. The entries of the bibliography are stored using BibTex, i.e., a format to store citations in LaTeX. Let’s take a look at references.bib.\nIn addition to the .bib file:\n\nInclude bibliography: references.bib in the YAML.\nAt the end of the report, include ## References. This will list all of the references at the end of the document.\n\n\nCitation examples\n\nIn Wickham, Chang, and Wickham (2016), the authors focus present the grammar of graphics package ggplot2 for R.\nWithin the grammar of graphics, ggplot() is the first layer of any plot (Wickham, Chang, and Wickham 2016).\n\n\nExercise 4\n\nAdd a citation for tidytuesday to this document. Hint: check out the tidytuesday GitHub page."
  },
  {
    "objectID": "ae/ae23.html#links",
    "href": "ae/ae23.html#links",
    "title": "Project Tips",
    "section": "Links",
    "text": "Links\nAdd URLs to your document using the following syntax:\nDISPLAYED TEXT"
  },
  {
    "objectID": "ae/ae23.html#neat-kable-table",
    "href": "ae/ae23.html#neat-kable-table",
    "title": "Project Tips",
    "section": "Neat kable table",
    "text": "Neat kable table\n\nCalculate the mean, median, and standard deviation of mass. Display the results.\n\n\nExercise 5\n\n# code here\n\n\nLet’s neatly display the results using the kable function from the knitr package. We will\n\nDisplay results to 2 decimal places\nCustomize column names\nAdd a caption\n\n\n\n## add code"
  },
  {
    "objectID": "ae/ae23.html#presentations-demo",
    "href": "ae/ae23.html#presentations-demo",
    "title": "Project Tips",
    "section": "Presentations (demo)",
    "text": "Presentations (demo)"
  },
  {
    "objectID": "ae/ae7.html",
    "href": "ae/ae7.html",
    "title": "Linear regression II",
    "section": "",
    "text": "Lab 3 due Thursday\nExam 1 released Thursday and due Monday\n\ncheck out practice and sakai solutions\nno TA office hours Friday/Monday\nask questions early\n\nLooking towards next week, please fill out this optional form to request group members (from your lab) to work on the projects with."
  },
  {
    "objectID": "ae/ae7.html#bulletin",
    "href": "ae/ae7.html#bulletin",
    "title": "Linear regression II",
    "section": "",
    "text": "Lab 3 due Thursday\nExam 1 released Thursday and due Monday\n\ncheck out practice and sakai solutions\nno TA office hours Friday/Monday\nask questions early\n\nLooking towards next week, please fill out this optional form to request group members (from your lab) to work on the projects with."
  },
  {
    "objectID": "ae/ae7.html#recap-warmup",
    "href": "ae/ae7.html#recap-warmup",
    "title": "Linear regression II",
    "section": "Recap (warmup)",
    "text": "Recap (warmup)\nFrom last time…\n\nWhat is \\(\\hat{y}\\)? How is it different than \\(y\\)?\nWhat is \\(\\hat{\\beta}\\)? How is it different than \\(\\beta\\)?\nWhat is a residual? How is it different than error?"
  },
  {
    "objectID": "ae/ae7.html#today",
    "href": "ae/ae7.html#today",
    "title": "Linear regression II",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\ncompute \\(R^2\\) and use it to select between models\nunderstand the geometric picture of multiple linear regression\nbe able to build, fit and interpret linear models with \\(&gt;1\\) predictor"
  },
  {
    "objectID": "ae/ae7.html#getting-started",
    "href": "ae/ae7.html#getting-started",
    "title": "Linear regression II",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console (bottom left of screen)\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae7.qmd\",\ndestfile = \"ae7.qmd\")"
  },
  {
    "objectID": "ae/ae7.html#load-packages-and-data",
    "href": "ae/ae7.html#load-packages-and-data",
    "title": "Linear regression II",
    "section": "Load packages and data",
    "text": "Load packages and data\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(scatterplot3d)\n\nToday’s data is a collection of tech stock prices from January 1st 2020 to December 31st 2021. I pulled this data off Yahoo finance using their API via the tidyquant package July 2022.\n\nstocks = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/stocks2.csv\")"
  },
  {
    "objectID": "ae/ae7.html#notes",
    "href": "ae/ae7.html#notes",
    "title": "Linear regression II",
    "section": "Notes",
    "text": "Notes"
  },
  {
    "objectID": "ae/ae7.html#r2-and-checking-model-fit",
    "href": "ae/ae7.html#r2-and-checking-model-fit",
    "title": "Linear regression II",
    "section": "\\(R^2\\) and checking model fit",
    "text": "\\(R^2\\) and checking model fit\n\nConceptual introduction\n\\(R^2\\), aka “the coefficient of determination” or “correlation squared” is a way to see how well a given model fits the data. Formally,\n\\[\nR^2 = 1 - \\frac{\\sum_i r_i^2}{\\sum_i (y_i - \\bar{y})^2}\n\\]\nwhere \\(\\bar{y}\\) is the mean of all y values.\nIn words,\n\\[\nR^2 = 1 - \\frac{\\text{sum of squared residuals}}{\\text{sum of outcome squared distance from the mean}}\n\\]\nLet’s focus on the word version to build intuition.\n\nThe sum of squared residuals is a measure of how wrong our model is (how much our model doesn’t explain)\nThe denominator is proportional to the average square distance from the mean, i.e. the variance, i.e. the amount of variability in the data.\nTogether, the fraction represents the proportion of variability that is not explained by the model.\n\nIf the sum of squared residuals is 0, then the model explains all variability and \\(R^2 = 1 - 0 = 1\\).\nSimilarly if the sum of squared residuals is the same as all the variability in the data, then model does not explain any variability and \\(R^2 = 1 - 1 = 0\\).\nFinal take-away: \\(R^2\\) is a measure of the proportion of variability the model explains. An \\(R^2\\) of 0 is a poor fit and \\(R^2\\) of 1 is a perfect fit.\n\n\nHow to find \\(R^2\\)\nTo find \\(R^2\\) simply call the function glance() on your modelFit, e.g.\nmodelFit = linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(outcome ~ predictor, data = data_set)\n  \nglance(modelFit)"
  },
  {
    "objectID": "ae/ae7.html#two-predictor-main-effects-model-and-notation",
    "href": "ae/ae7.html#two-predictor-main-effects-model-and-notation",
    "title": "Linear regression II",
    "section": "Two predictor main effects model and notation",
    "text": "Two predictor main effects model and notation\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\n\\]\n\n\\(y\\): the outcome variable. Also called the “response” or “dependent variable”. In prediction problems, this is what we are interested in predicting.\n\\(x_i\\): the \\(i^{th}\\) predictor. Also commonly referred to as “regressor”, “independent variable”, “covariate”, “feature”, “the data”.\n\\(\\beta_i\\): “constants” or coefficients i.e. fixed numbers. These are population parameters. \\(\\beta_0\\) has another special name, “the intercept”.\n\\(\\epsilon\\): the error. This quantity represents observational error, i.e. the difference between our observation and the true population-level expected value: \\(\\beta_0 + \\beta_1 x\\).\n\nEffectively this model says our data \\(y\\) is linearly related to the \\(x_1\\) and \\(x_2\\) but is not perfectly observed due to some error.\n\nA simple example\nLet’s examine the first quarter of 2020 high prices of Microsoft, IBM and Apple stocks to illustrate some ideas.\n\n\n\n\n\n\nIf we have three measurements (variables) then each observation is a point in three-dimensional space. In this example, we can choose one of our measurements to be the outcome variable (e.g. Apple stock price) and use our other two measurements (MSFT and IBM price) as predictors.\nIn general, the total number of measurements, i.e. variables (columns) in our linear model represents the spatial dimension of our model.\nOur fitted linear model no longer looks like a line, but instead looks like a plane.\n\n\n\n\n\n\n\nThis plane shows our prediction of AAPL price (\\(y\\)) given both MSFT price (\\(x_1\\)) and IBM price (\\(x_2\\))\nDemo: building intuition for higher dimensional linear models\n\n\nExercise 1\nIn \\(n\\)-dimensional space, a linear equation creates a \\(\\text{insert number here}\\)-dimensional object."
  },
  {
    "objectID": "ae/ae7.html#fitting-a-multiple-regression-model-in-r",
    "href": "ae/ae7.html#fitting-a-multiple-regression-model-in-r",
    "title": "Linear regression II",
    "section": "Fitting a multiple regression model in R",
    "text": "Fitting a multiple regression model in R\nFind the equation of the plane above with this one simple trick!\nmyModelFit = linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(outcome ~ predictor1 + predictor2 + predictor3 + ..., data = data-set-here)\nwe can simply ‘add’ in new predictors! This code template will fit the model according to the ordinary least squares (OLS) objective function, i.e. we are finding the equation of the hyperplane that minimizes the sum of squared residuals.\nYou can subsequently print the coefficients (\\(\\beta\\)s) to the screen by simply typing the model name, e.g. myModelFit or calling the tidy() function on your fitted model, e.g. tidy(myModelFit).\n\nExercise 2\nIn the code chunk below, fit the multiple regression model described above where\n\\(y\\): AAPL high price, \\(x_1\\): MSFT high price, \\(x_2\\): IBM high price.\nThen write the equation of your fitted model below.\n\nNote: you should change the name of “myModelFit” to be something more meaningful, e.g. apple_high_fit\n\n\n# code here \n\nThe equation of the plane above:\n\\[\n\\text{your equation here}\n\\]\n\n\nExercise 3\nInterpret the coefficients in your equation above.\n[your interpretation here]"
  },
  {
    "objectID": "ae/ae7.html#a-better-model",
    "href": "ae/ae7.html#a-better-model",
    "title": "Linear regression II",
    "section": "A better model",
    "text": "A better model\n\nLog return\nApplying a model to values outside of the original data is called extrapolation. Extrapolation can be very unreliable.\nThat being noted, it would be nice if our model was only able to predict realistic outcomes. If we consider extrapolating our forecast, we will see that our linear model can easily predict unrealistic values. For example, with a negative slope, we can imagine that a very high Microsoft price drives our Apple prediction down to a negative value.\nHowever, stock prices cannot be negative. A more useful modeling framework used by investors is to predict the “log return” of a stock. Over the course of day, the log return is defined:\n\\[\n\\log(\\text{close price}) - \\log(\\text{open price}) = \\log \\left( \\frac{\\text{close price}}{\\text{open price}} \\right)\n\\]\n\nExercise 4\nStarting with your stocks data frame, create new columns AAPL.LogReturn, MSFT.LogReturn, IBM.LogReturn that shows the daily log return of each stock. Continue this for the remaining stocks in the data frame. Save your new data frame as stock_returns.\n\n# code here\n\n\n\nExercise 5\nFit the following model:\n\\[\ny = \\beta_0 + x_1 \\beta_1 + x_2 \\beta_2 + \\epsilon\n\\] where\n\n\\(y\\): AAPL daily log return\n\\(x_1\\): MSFT daily log return\n\\(x_2\\): IBM daily log return\n\nand report \\(R^2\\).\n\n# code here \n\n\n\n\nPredicting the future\nSo far we’ve only used the present to predict the present. i.e. we’ve used January 1st IBM prices to predict January 1st AAPL prices. While the resulting models are quite good, they are not particularly useful.\nIt would be much more useful if we could predict the return of AAPL tomorrow so that we could make an informed decision about buying or selling it.\nTo begin such an endeavor, let’s build a model that uses yesterday’s log-return of IBM and MSFT to predict today’s log return of AAPL.\n\nExercise 6\nWhat should our data frame look like?\n[ your answer here ]\nLet’s make that data frame! Adapt the example below to create new columns for yesterday’s IBM and MSFT returns.\n\nstock_returns2 = stock_returns %&gt;%\n  mutate(AAPL.LogReturnYesterday = lag(AAPL.LogReturn, 1)) %&gt;%\n  filter(!is.na(AAPL.LogReturnYesterday))\n\nstock_returns2\n\n\n\nExercise 7\nFit the following model:\n\\[\ny = \\beta_0 + x_1 \\beta_1 + x_2 \\beta_2 + \\epsilon\n\\]\nwhere\n\\(y\\): AAPL daily log return \\(x_1\\): MSFT log return yesterday \\(x_2\\): IBM log return yesterday\nand report \\(R^2\\). What do you notice?\n\n# code here"
  },
  {
    "objectID": "project/project-1.html",
    "href": "project/project-1.html",
    "title": "Project 1",
    "section": "",
    "text": "More info to be posted."
  },
  {
    "objectID": "project/project-tips-resources.html",
    "href": "project/project-tips-resources.html",
    "title": "Project tips + resources",
    "section": "",
    "text": "To be posted."
  },
  {
    "objectID": "labs/lab-7.html",
    "href": "labs/lab-7.html",
    "title": "Lab 7",
    "section": "",
    "text": "To be posted…"
  },
  {
    "objectID": "labs/lab-6.html",
    "href": "labs/lab-6.html",
    "title": "Lab 6",
    "section": "",
    "text": "To be posted…"
  },
  {
    "objectID": "labs/lab-3.html",
    "href": "labs/lab-3.html",
    "title": "Lab 3",
    "section": "",
    "text": "To be posted…"
  },
  {
    "objectID": "labs/lab-1.html",
    "href": "labs/lab-1.html",
    "title": "Lab 1",
    "section": "",
    "text": "To be posted…"
  },
  {
    "objectID": "computing/computing-access.html",
    "href": "computing/computing-access.html",
    "title": "Computing access",
    "section": "",
    "text": "To access RStudio in the browser, go to the Posit Cloud space for this course.\nAt the beginning of the semester you should have received a link to join this space. If you haven’t yet joined, you can find the link in Canvas Announcements or ask for it on the course Slack."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 101: Data Analysis and Statistical Inference",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses and the timeline of topics and assignments might be updated throughout the semester.\n\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n  \n  \n    \n    \n      WEEK\n      DATE\n      TOPIC\n      PREPARE\n      MATERIALS\n      DUE\n    \n  \n  \n    1\nMon, Aug 28\nWelcome to STA 101!\n\n\n🖥️ slides \n☁️ ae-01\n\n\n    \nWed, Aug 30\nHello data\n\n📖 ims: Chp 1 \n💻 tutorial: 01-data-01 |&gt; submit\n\n\n🔘 Survey: Getting to know you\n\n    \nFri, Sep 1\nLab 1: Hello R!\n\n\n\n\n    2\nMon, Sep 4\nLabor Day - No lecture\n\n\n\n\n    \nWed, Sep 6\nStudy design\n\n📖 ims: Chp 2 \n💻 tutorial: 01-data-02 |&gt; submit \n💻 tutorial: 01-data-03 |&gt; submit\n\n\n\n    \nThu, Sep 7\n\n\n\nLab 1 at 5 pm\n\n    \nFri, Sep 8\nLab 2: Hello data!\n\n📖 ims: Chp 3 \n💻 tutorial: 01-data-04 |&gt; submit\n\n\n\n    3\nMon, Sep 11\nExploring categorical data\n\n\n\n\n    \nWed, Sep 13\nExploring numerical data\n\n\n\n\n    \nThu, Sep 14\n\n\n\nLab 2 at 5 pm\n\n    \nFri, Sep 15\nLab 3: Exploratory data analysis\n\n\n\n\n    4\nMon, Sep 18\nRegression with a single predictor I \nGuest Lecture: Dr. Alex Fisher\n\n\n\n\n    \nWed, Sep 20\nRegression with a single predictor II \nVirtual class: Zoom link TBA\n\n\n\n\n    \nThu, Sep 21\n\n\n\nLab 3 at 5 pm\n\n    \nFri, Sep 22\nLab 4: Regression\n\n\n\n\n    5\nMon, Sep 25\nRegression with multiple predictors I\n\n\n\n\n    \nWed, Sep 27\nRegression with multiple predictors II\n\n\n\n\n    \nThu, Sep 28\n\n\n\nLab 4 at 5 pm\n\n    \nFri, Sep 29\nExam 1 Review\n\n\n\n\n    6\nMon, Oct 2\nLogistic regression\n\n\n\n\n    \nWed, Oct 4\nExam 1 - In class\n\n\n\n\n    \nFri, Oct 6\nNo lab: Work on Exam 1 - Take home\n\n\n\nExam 1 - Take home at 5 pm\n\n    7\nMon, Oct 9\nHypothesis testing with randomization I\n\n\n\n\n    \nWed, Oct 11\nHypothesis testing with randomization II\n\n\n\n\n    \nFri, Oct 13\nProject 1 workday\n\n\n\nProject 1 at 5 pm\n\n    8\nMon, Oct 16\nFall Break - No lecture\n\n\n\n\n    \nWed, Oct 18\nConfidence intervals with bootstrapping\n\n\n\n\n    \nFri, Oct 20\nLab 5: Inference with simulation\n\n\n\n\n    9\nMon, Oct 23\nInference with mathematical models\n\n\n\n\n    \nWed, Oct 25\nDecision errors \nGuest Lecture: Dr. Yue Jiang\n\n\n\n\n    \nThu, Oct 26\n\n\n\nLab 5 at 5 pm\n\n    \nFri, Oct 27\nLab 6: Foundations of inference\n\n\n\n\n    10\nMon, Oct 30\nInference for a single proportion\n\n\n\n\n    \nWed, Nov 1\nInference for comparing two proportions\n\n\n\n\n    \nThu, Nov 2\n\n\n\nLab 6 at 5 pm\n\n    \nFri, Nov 3\nLab 7: Inference for proportions\n\n\n\n\n    11\nMon, Nov 6\nInference for a single mean\n\n\n\n\n    \nWed, Nov 8\nInference for comparing two means\n\n\n\n\n    \nThu, Nov 9\n\n\n\nLab 7 at 5 pm\n\n    \nFri, Nov 10\nExam 2 Review\n\n\n\n\n    12\nMon, Nov 13\nInference for two-way tables\n\n\n\n\n    \nWed, Nov 15\nExam 2 - In class\n\n\n\n\n    \nFri, Nov 17\nNo lab: Work on Exam 2 - Take home\n\n\n\nExam 2 - Take home at 5 pm\n\n    13\nMon, Nov 20\nInference for comparing many means\n\n\n\n\n    \nWed, Nov 22\nThanksgiving Break - No lecture\n\n\n\n\n    \nFri, Nov 24\nThanksgiving Break - No lecture\n\n\n\n\n    14\nMon, Nov 27\nInference for regression with a single predictor\n\n\n\n\n    \nWed, Nov 29\nInference for regression with multiple predictors\n\n\n\n\n    \nFri, Dec 1\nLab 8: Inference\n\n\n\n\n    15\nMon, Dec 4\nLooking forward: TBA\n\n\n\n\n    \nWed, Dec 6\nLooking forward: TBA\n\n\n\n\n    \nThu, Dec 7\n\n\n\nLab 8 at 5 pm\n\n    \nFri, Dec 8\nProject 2 peer review\n\n\n\n\n    16\nThu, Dec 14\nProject 2 presentations (2 - 5 pm)\n\n\n\nProject 2 at 2 pm"
  },
  {
    "objectID": "course-faq.html",
    "href": "course-faq.html",
    "title": "FAQ",
    "section": "",
    "text": "The short answer is, I’d rather you didn’t, to save yourself some headache. But, the long answer is, sure! But you will need to install a specific versions of R and RStudio for everything to work as expected. You will also need to install the R packages we’re using as well as have Git installed on your computer. These are not extremely challenging things to get right, but they are not trivial either, particularly on certain operating systems. Myself and the TAs are always happy to provide help with any computational questions when you’re working in the containers we have provided for you. If you’re working on your local setup, we can’t guarantee being able to resolve your issues, though we’re happy to try.\nIf you want to take this path, here is what you need to do:\n\nDownload and install R 4.3.1: https://cran.r-project.org\nDownload and install the preview build of RStudio: https://posit.co/download/rstudio-desktop\nInstall any necessary packages with install.packages(\"___\")\n\nAnd I’d like to reiterate again that successful installation of these software is not a learning goal of this course. So if any of this seems tedious or intimidating in any way, just use the computing environment we have set up for you. More on that here."
  },
  {
    "objectID": "course-faq.html#can-i-use-a-local-install-of-posit-cloud",
    "href": "course-faq.html#can-i-use-a-local-install-of-posit-cloud",
    "title": "FAQ",
    "section": "",
    "text": "The short answer is, I’d rather you didn’t, to save yourself some headache. But, the long answer is, sure! But you will need to install a specific versions of R and RStudio for everything to work as expected. You will also need to install the R packages we’re using as well as have Git installed on your computer. These are not extremely challenging things to get right, but they are not trivial either, particularly on certain operating systems. Myself and the TAs are always happy to provide help with any computational questions when you’re working in the containers we have provided for you. If you’re working on your local setup, we can’t guarantee being able to resolve your issues, though we’re happy to try.\nIf you want to take this path, here is what you need to do:\n\nDownload and install R 4.3.1: https://cran.r-project.org\nDownload and install the preview build of RStudio: https://posit.co/download/rstudio-desktop\nInstall any necessary packages with install.packages(\"___\")\n\nAnd I’d like to reiterate again that successful installation of these software is not a learning goal of this course. So if any of this seems tedious or intimidating in any way, just use the computing environment we have set up for you. More on that here."
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "This course introduces students to the discipline of statistics as a science of understanding and analyzing data. Themes include data collection, exploratory analysis, inference, and modeling. Focus on principles underlying quantitative research in social sciences, humanities, and public policy. Research projects teach the process of scientific discovery and synthesis and critical evaluation of research and statistical arguments. Readings give perspective on why in 1950, S. Wilks said, “Statistical thinking will one day be as necessary a qualification for efficient citizenship as the ability to read and write.”\nIn this course, students learn how to effectively make use of data in the face of uncertainty: how to collect data, how to analyze data, and how to use data to make inferences and conclusions about real world phenomena. Critiquing data-based claims and evaluating data-based decisions is at the core of this course. Throughout the course students acquire a conceptual understanding and mastery of statistical and quantitative reasoning tools in order to be able to make such critiques and evaluations.\nIn addition, students are presented with novel data sets and application examples on a daily basis, and they use these data to model outcomes and make inferences about unknown population characteristics. Students learn that the first step of any analysis is identifying the assumptions and conditions necessary to apply the statistical technique(s) required to answer the research question at hand. Students not only learn the mechanics of the quantitative analysis, but also how to interpret conclusions based on quantitative evidence in context of the data and the research questions as well as identifying limitations due to data collection and study design.\nFor the lab component of this course students prepare weekly lab reports presenting statistical analysis of real data. In addition, students complete two independent data analysis projects where they answer significant research questions via the analysis of real data using statistical inference and modeling tools."
  },
  {
    "objectID": "course-syllabus.html#course-description",
    "href": "course-syllabus.html#course-description",
    "title": "Syllabus",
    "section": "",
    "text": "This course introduces students to the discipline of statistics as a science of understanding and analyzing data. Themes include data collection, exploratory analysis, inference, and modeling. Focus on principles underlying quantitative research in social sciences, humanities, and public policy. Research projects teach the process of scientific discovery and synthesis and critical evaluation of research and statistical arguments. Readings give perspective on why in 1950, S. Wilks said, “Statistical thinking will one day be as necessary a qualification for efficient citizenship as the ability to read and write.”\nIn this course, students learn how to effectively make use of data in the face of uncertainty: how to collect data, how to analyze data, and how to use data to make inferences and conclusions about real world phenomena. Critiquing data-based claims and evaluating data-based decisions is at the core of this course. Throughout the course students acquire a conceptual understanding and mastery of statistical and quantitative reasoning tools in order to be able to make such critiques and evaluations.\nIn addition, students are presented with novel data sets and application examples on a daily basis, and they use these data to model outcomes and make inferences about unknown population characteristics. Students learn that the first step of any analysis is identifying the assumptions and conditions necessary to apply the statistical technique(s) required to answer the research question at hand. Students not only learn the mechanics of the quantitative analysis, but also how to interpret conclusions based on quantitative evidence in context of the data and the research questions as well as identifying limitations due to data collection and study design.\nFor the lab component of this course students prepare weekly lab reports presenting statistical analysis of real data. In addition, students complete two independent data analysis projects where they answer significant research questions via the analysis of real data using statistical inference and modeling tools."
  },
  {
    "objectID": "course-syllabus.html#learning-objectives",
    "href": "course-syllabus.html#learning-objectives",
    "title": "Syllabus",
    "section": "Learning objectives",
    "text": "Learning objectives\nThe course learning objectives are as follows:\n\nRecognize the importance of data collection, identify limitations in data collection methods, and determine how they affect the scope of inference.\nUse statistical software to summarize data numerically and visually, and to perform data analysis.\nHave a conceptual understanding of the unified nature of statistical inference.\nApply estimation and testing methods to analyze single variables or the relationship between two variables in order to understand natural phenomena and make data-based decisions.\nModel numerical response variables using a single or multiple explanatory variables.\nInterpret results correctly, effectively, and in context without relying on statistical jargon.\nCritique data-based claims and evaluate data-based decisions.\nComplete research projects demonstrating mastery of statistical data analysis from exploratory analysis to inference to modeling."
  },
  {
    "objectID": "course-syllabus.html#prerequisites",
    "href": "course-syllabus.html#prerequisites",
    "title": "Syllabus",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis course has no pre-requisites."
  },
  {
    "objectID": "course-syllabus.html#workload",
    "href": "course-syllabus.html#workload",
    "title": "Syllabus",
    "section": "Workload",
    "text": "Workload\nYou are expected to put in ~6 hours of work / week outside of class. Some of you will do well with less time than this, and some of you will need more."
  },
  {
    "objectID": "course-syllabus.html#tips-for-success",
    "href": "course-syllabus.html#tips-for-success",
    "title": "Syllabus",
    "section": "Tips for success",
    "text": "Tips for success\n\nComplete the reading before a new unit begins, and then review again after the unit is over.\nBe an active participant during lectures and labs.\nAsk questions - during class or office hours, or by email. Ask me, your TAs, and your classmates.\nDo the problem sets - start early and make sure you attempt and understand all questions.\nStart your project early and and allow adequate time to complete it.\nGive yourself plenty of time time to prepare a good cheat sheet for exams. This requires going through the material and taking the time to review the concepts that you’re not comfortable with.\nDo not procrastinate - don’t let a unit go by with unanswered questions as it will just make the following unit’s material even more difficult to follow."
  },
  {
    "objectID": "course-syllabus.html#textbooks",
    "href": "course-syllabus.html#textbooks",
    "title": "Syllabus",
    "section": "Textbooks",
    "text": "Textbooks\nReadings for the course will come from the following textbooks. They are freely available online and you do not need to purchase a physical copy of either book to succeed in this class.\n\n[ims]: Mine Çetinkaya-Rundel and Jo Hardin. Introduction to Modern Statistics. (in progress) 2nd edition. OpenIntro, 2023.\n[r4ds]: Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund. R for Data Science. 2nd edition. O’Reilly, 2022."
  },
  {
    "objectID": "course-syllabus.html#course-community",
    "href": "course-syllabus.html#course-community",
    "title": "Syllabus",
    "section": "Course community",
    "text": "Course community\n\nDuke Community Standard\nAll students must adhere to the Duke Community Standard (DCS): Duke University is a community dedicated to scholarship, leadership, and service and to the principles of honesty, fairness, and accountability. Citizens of this community commit to reflect upon these principles in all academic and non-academic endeavors, and to protect and promote a culture of integrity.\nTo uphold the Duke Community Standard, students agree:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\n\n\n\n\nInclusive community\nIt is my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength, and benefit. It is my intent to present materials and activities that are respectful of diversity and in alignment with Duke’s Commitment to Diversity and Inclusion. Your suggestions are encouraged and appreciated. Please let me know ways to improve the effectiveness of the course for you personally, or for other students or student groups.\nFurthermore, I would like to create a learning environment for my students that supports a diversity of thoughts, perspectives and experiences, and honors your identities. To help accomplish this:\n\nIf you have a name that differs from those that appear in your official Duke records, please let me know! You’ll be able to note this in the Getting to know you survey.\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don’t hesitate to come and talk with me. If you prefer to speak with someone outside of the course, your academic dean is an excellent resource.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please let me or a member of the teaching team know.\n\n\n\nPronouns\nPronouns are meaningful tools to communicate identities and experiences, and using pronouns supports a campus environment where all community members can thrive.\nPlease update your gender pronouns in Duke Hub. You can learn more at the Center for Sexual and Gender Diversity’s website.\n\n\nAccessibility\nIf there is any portion of the course that is not accessible to you due to challenges with technology or the course format, please let me know so we can make appropriate accommodations.\nThe Student Disability Access Office (SDAO) is available to ensure that students are able to engage with their courses and related assignments. Students should be in touch with the Student Disability Access Office to request or update accommodations under these circumstances.\n\n\nCommunication\nAll lecture notes, assignment instructions, an up-to-date schedule, and other course materials may be found on the course website: sta101-f23.github.io.\nI will regularly send course announcements via email and Canvas, make sure to check one or the other of these regularly. If an announcement is sent Monday through Thursday, I will assume that you have read the announcement by the next day. If an announcement is sent on a Friday or over the weekend, I will assume that you have read it by Monday.\n\n\nWhere to get help\n\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours to ask questions about the course content and assignments. Many questions are most effectively answered as you discuss them with others, so office hours are a valuable resource. Please use them!\nOutside of class and office hours, any general questions about course content or assignments should be posted on the course Slack. There is a chance another student has already asked a similar question, so please check the other posts on Slack before adding a new question. If you know the answer to a question posted on Slack, I encourage you to respond!\n\nCheck out the Support page for more resources.\nI want to make sure that you learn everything you were hoping to learn from this class. If this requires flexibility, please don’t hesitate to ask.\n\nYou never owe me personal information about your health (mental or physical) but you’re always welcome to talk to me. If I can’t help, I likely know someone who can.\nI want you to learn lots of things from this class, but I primarily want you to stay healthy, balanced, and grounded during this crisis."
  },
  {
    "objectID": "course-syllabus.html#lectures-and-lab",
    "href": "course-syllabus.html#lectures-and-lab",
    "title": "Syllabus",
    "section": "Lectures and lab",
    "text": "Lectures and lab\nThe goal of both the lectures and the labs is for them to be as interactive as possible. My role as instructor is to introduce you new tools and techniques, but it is up to you to take them and make use of them. A lot of what you do in this course will involve writing code, and coding is a skill that is best learned by doing. Therefore, as much as possible, you will be working on a variety of tasks and activities throughout each lecture and lab. Attendance will not be taken during class but you are expected to attend all lecture and lab sessions and meaningfully contribute to in-class exercises and discussion.\nYou are expected to bring a laptop to each class so that you can take part in the in-class exercises. Please make sure your laptop is fully charged before you come to class as the number of outlets in the classroom will not be sufficient to accommodate everyone. See Duke LIFE loaner laptop program if you need a loaner laptop."
  },
  {
    "objectID": "course-syllabus.html#assessments-and-grading",
    "href": "course-syllabus.html#assessments-and-grading",
    "title": "Syllabus",
    "section": "Assessments and grading",
    "text": "Assessments and grading\nThe final course grade will be calculated as follows:\n\n\n\nCategory\nPercentage\n\n\n\n\nAttendance and participation\n5%\n\n\nInteractive tutorials\n5%\n\n\nLabs\n25%\n\n\nExam 1\n20%\n\n\nExam 2\n20%\n\n\nProject 1\n10%\n\n\nProject 2\n15%\n\n\n\nThe final letter grade will be determined based on the following thresholds:\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA\n&gt;= 93\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n&lt; 60\n\n\n\nThese are upper bounds for grade cutoffs, depending on the class performance the cutoffs may be lowered but they won’t be increased.\nAll work is expected to be submitted by the deadline and there are no make ups for any missed assessments. See Section 10.2 for policies on late work.\n\nAttendance and participation\nYou are expected to be present at class meeting and actively participate in the discussion. Your attendance and participation during class, as well as your activity on the course Slack will make up a non-insignificant portion of your grade in this class. While I might sometimes call on you during the class discussion, it is your responsibility to be an active participant without being called on.\nIf you attend at least 80% of the classes, you’ll get all available points for this component.\n\n\nInteractive tutorials\nYou will be assigned a number of interactive tutorials each week from the textbook. You will be asked to submit these on a weekly basis and graded on a check/no check basis.\nMake sure to add your name and your Net ID before generating the hash. Submit these in Canvas.\nIf you’ve completed at least 80% of the tutorials, you’ll get all available points for this component.\n\n\nLabs\nIn labs, you will apply the concepts discussed in lecture to various data analysis scenarios. Labs will focus on both computation and conceptualization. Lab assignments will be completed using Quarto and submitted as PDF for grading in Gradescope. While you may collaborate with others on lab assignments, your final solution should be your own.\nLowest lab score will be dropped.\n\n\nExams\nThere will be two exams. Each exam will be comprised of two components:\n\nIn class: 75 minute in-class exam. This exam is closed book, however you are allowed to use one sheet of notes (“cheat sheet”) to the midterm and the final. This sheet must be no larger than 8 1/2 x 11, and must be prepared by you. You may use both sides of the sheet. (70% of the grade)\nTake home: Following the in class exam, you’ll have 48 hours to complete the take home portion of your exam. The take home portion will follow from the in class exam and focus on the analysis of a dataset introduced in the take home exam. (30% of the grade)\n\nThrough these exams you have the opportunity to demonstrate what you’ve learned in the course thus far. Each exam will include small analyses and computational tasks related to the content in application exercises and labs. More details about the content and structure of the exams will be discussed during the semester.\nSee Section 12 for dates and times of the exams. Exam dates cannot be changed and no make-up exams will be given. If you can’t take the exams on these dates, you should drop this class.\n\n\nProjects\nThere will be a mid-semester prediction project and a final project. The prediction project will introduce you to conducting independent analyses and writing a formal report using a pre-specified data set. The final project allows you to explore a question and data set of your own. More details about the projects will be provided during the semester. Projects will be completed in teams.\nYou will be assigned to a different team for each of your two projects. You are encouraged to sit with your teammates in lecture and you will also work with them in the lab sessions. All team members are expected to contribute equally to the completion of each project and you will be asked to evaluate your team members after each assignment is due. Failure to adequately contribute to an assignment will result in a penalty to your mark relative to the team’s overall mark.\nSee Section 12 for dates and times of project deadlines. Project deadlines cannot be changed. If you can’t be in class for the final project presentation, you should drop this class."
  },
  {
    "objectID": "course-syllabus.html#course-policies",
    "href": "course-syllabus.html#course-policies",
    "title": "Syllabus",
    "section": "Course policies",
    "text": "Course policies\n\nAcademic honesty\nTL;DR: Don’t cheat!\nPlease abide by the following as you work on assignments in this course:\n\nCollaboration: Only work that is clearly assigned as team work should be completed collaboratively.\n\nThe labs must also be completed individually and you are welcomed to discuss the assignment with classmates at a high level (e.g., discuss what’s the best way for approaching a problem, what functions are useful for accomplishing a particular task, etc.). However you may not directly share answers to homework questions (including any code) with anyone other than myself and the teaching assistants.\nFor the projects, collaboration within teams is not only allowed, but expected. Communication between teams at a high level is also allowed however you may not share code or components of the project across teams.\nOn individual assignments you may not directly share code with another student in this class, and on team assignments you may not directly share code with another team in this class.\n\nOnline resources: I am well aware that a huge volume of code is available on the web to solve any number of problems. Unless I explicitly tell you not to use something, the course’s policy is that you may make use of any online resources (e.g., StackOverflow) but you must explicitly cite where you obtained any code you directly use (or use as inspiration). Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism.\nUse of generative artificial intelligence (AI): You should treat generative AI, such as ChatGPT, the same as other online resources. There are two guiding principles that govern how you can use AI in this course:1 (1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate—rather than hinder—learning. (2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.\n\n✅ AI tools for code: You may make use of the technology for coding examples on assignments; if you do so, you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. You may use these guidelines for citing AI-generated content.\n❌ AI tools for narrative: Unless instructed otherwise, you may not use generative AI to write narrative on assignments. In general, you may use generative AI as a resource as you complete assignments but not to answer the exercises for you. You are ultimately responsible for the work you turn in; it should reflect your understanding of the course content.\n\n\nRegardless of course delivery format, it is the responsibility of all students to understand and follow all Duke policies, including academic integrity (e.g., completing one’s own work, following proper citation of sources, adhering to guidance around group work projects, and more). Ignoring these requirements is a violation of the Duke Community Standard. Any questions and/or concerns regarding academic integrity can be directed to the Office of Student Conduct and Community Standards at conduct@duke.edu.\nAny violations in academic honesty standards as outlined in the Duke Community Standard and those specific to this course will\n\nautomatically result in a 0 for the assignment,\ncan further impact your overall course grade, and\nwill be reported to the Office of Student Conduct for further action.\n\n\n\nLate work & extensions\nThe due dates for assignments are there to help you keep up with the course material and to ensure the teaching team can provide feedback within a timely manner. We understand that things come up periodically that could make it difficult to submit an assignment by the deadline.\nPolicy on late work depends on the particular course component:\n\nLabs:\n\nLate, but within 24 hours of deadline: -20% of available points.\nAny later: No credit, and we will not provide written feedback.\nNote that lowest lab score will be dropped, even if that score is a 0.\n\nExams:\n\nIn class portions of the exams can obviously not be turned in late.\nLate exams are not accepted.\n\nProjects: The following three components contribute to your project score.\n\nPresentation: Late presentations are not accepted and there are no make ups for missed presentations.\nWrite up: GitHub repositories will be closed to contributions at the deadline. If you need to submit your work late, Slack/email me to reopen your repository.\n\nLate, but within 24 hours of deadline: -20% of available points.\nAny later: No credit, and we will not provide written feedback.\n\nPeer evaluation: Late peer evaluations are not accepted. If you do not turn in your peer evaluation, you get 0 points for your own peer score as well, regardless of how your teammates have evaluated you.\n\n\n\n\nWaiver for extenuating circumstances\nIf there are circumstances that prevent you from completing a lab or homework assignment by the stated due date, you may email the Head TA (Shuo Wang, shuo.wang717@duke.edu) before the deadline to waive the late penalty. In your email, you only need to request the waiver; you do not need to provide explanation. This waiver may only be used for once in the semester, so only use it for a truly extenuating circumstance.\nIf there are circumstances that are having a longer-term impact on your academic performance, please let your academic dean know, as they can be a resource. Please let me know if you need help contacting your academic dean.\n\n\nRegrade requests\nEvery effort will be made to mark your work accurately. We are on your side, and want you to receive every point you have worked to earn. However, sometimes grading mistakes happen. If you believe that an error has been made, return the paper to the instructor within four days, stating your claim in writing.\nThe following claims will be considered for re-grading:\n\npoints are not totaled correctly;\nthe grader did not see a correct answer that is on your paper;\nyour answer is the same as the correct answer, but in a different form (e.g., you wrote a correct answer as 1/3 and the grader was looking for 0.333);\nyour answer to a free response question is essentially correct but stated slightly differently than the grader’s expectation.\n\nThe following claims will not be considered for re-grading:\n\narguments about the number of points lost;\narguments about question wording.\n\nConsidering re-grades consumes time and resources that TAs and the instructor would rather spend helping you understand material. Please bring only claims of type 1-4 to our attention.\nNote that during the regrade process your score could go up or go down or not change.\n\n\n\n\n\n\nWarning\n\n\n\nNo grades will be changed after the project presentations.\n\n\n\n\nAttendance policy\nResponsibility for class attendance rests with individual students. Since regular and punctual class attendance is expected, students must accept the consequences of failure to attend. More details on Trinity attendance policies are available here.\nHowever, there may be many reasons why you cannot be in class on a given day, particularly with possible extra personal and academic stress and health concerns this semester. All course lectures will be recorded and available to enrolled students after class. If you miss a lecture, make sure to watch the recording and review the material before the next class session. Lab time is dedicated to working on your assignments and collaborating with your teammates on your project. If you miss a lab session, make sure to communicate with your team about how you can make up your contribution. Given the technologies we use in the course, this is straightforward to do asynchronously. If you know you’re going to miss a lab session and you’re feeling well enough to do so, notify your teammates ahead of time. Overall these policies are put in place to ensure communication between team members, respect for each others’ time, and also to give you a safety net in the case of illness or other reasons that keep you away from attending class.\nNote that attendance and participation is part of your grade as well.\n\n\nAttendance policy related to COVID symptoms, exposure, or infection\nStudent health, safety, and well-being are the university’s top priorities. To help ensure your well-being and the well-being of those around you, please do not come to class if you have tested positive for COVID-19 or have possible symptoms and have not yet been tested. If any of these situations apply to you, you must follow university guidance related to the ongoing COVID-19 pandemic and current health and safety protocols. If you are experiencing any COVID-19 symptoms, contact student health (dshcheckin@duke.edu, 919- 681-9355). Learn more about current university policy related to COVID-19 at https://coronavirus.duke.edu. To keep the university community as safe and healthy as possible, you will be expected to follow these guidelines. Please reach out to me and your academic dean as soon as possible if you need to quarantine or isolate so that we can discuss arrangements for your continued participation in class.\n\n\nInclement weather policy\nIn the event of inclement weather or other connectivity-related events that prohibit class attendance, I will notify you how we will make up missed course content and work. This might entail holding the class on Zoom synchronously, we may rely on Duke’s designated make-up days, or you may be asked to watch a recording of the class.\n\n\nPolicy on video recording course content\nAll lectures will be recorded and available on Panopto, so students should not need to create their own recordings of lectures. If you feel that you need record the lectures yourself, you must get permission from me ahead of time and these recordings should be used for personal study only, no for distribution. The full policy on recording of lectures falls under the Duke University Policy on Intellectual Property Rights, available at https://policies.provost.duke.edu/docs/faculty-handbook-appendix-m-intellectual-property. Unauthorized distribution is a cause for disciplinary action by the Judicial Board."
  },
  {
    "objectID": "course-syllabus.html#accommodations",
    "href": "course-syllabus.html#accommodations",
    "title": "Syllabus",
    "section": "Accommodations",
    "text": "Accommodations\n\nAcademic accommodations\nIf you are a student with a disability and need accommodations for this class, it is your responsibility to register with the Student Disability Access Office (SDAO) and provide them with documentation of your disability. SDAO will work with you to determine what accommodations are appropriate for your situation. Please note that accommodations are not retroactive and disability accommodations cannot be provided until a Faculty Accommodation Letter has been given to me. Please contact SDAO for more information: sdao@duke.edu or access.duke.edu.\n\n\nReligious accommodations\nStudents are permitted by university policy to be absent from class to observe a religious holiday. Accordingly, Trinity College of Arts & Sciences and the Pratt School of Engineering have established procedures to be followed by students for notifying their instructors of an absence necessitated by the observance of a religious holiday. Please submit requests for religious accommodations at the beginning of the semester so that we can work to make suitable arrangements well ahead of time. You can find the policy and relevant notification form here: https://trinity.duke.edu/undergraduate/academic-policies/religious-holidays.\nNote: If you’ve read this far in the syllabus, email me a picture of your pet if you have one or your favorite meme!"
  },
  {
    "objectID": "course-syllabus.html#sec-important-dates",
    "href": "course-syllabus.html#sec-important-dates",
    "title": "Syllabus",
    "section": "Important dates",
    "text": "Important dates\n\nMonday, August 28: Classes begin\nMonday, September 4: Labor Day - No lecture\nFriday, September 8: Drop/add ends\nWednesday, October 4: Exam 1 - In class\nFriday, October 6: Exam 1 - Take home due\nFriday, October 13: Project 1 due + Mid-semester grades reported\nMonday, October 16: Fall Break - No lecture\nFriday, November 10: Last day to withdraw with W\nWednesday, November 15: Exam 2 - In class\nFriday, November 17: Exam 2 - Take home due\nWednesday, November 22: Thanksgiving Break - No lecture\nFriday, November 24: Thanksgiving Break - No lab\nFriday, December 8: Classes end\nSaturday, December 9 - Tuesday, December 12: Reading period\nThursday, December 14, 2-5pm: Project 2 presentations\n\nFor more important dates, see the full Duke Academic Calendar."
  },
  {
    "objectID": "course-syllabus.html#footnotes",
    "href": "course-syllabus.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese guiding principles are based on Course Policies related to ChatGPT and other AI Tools developed by Joel Gladd, Ph.D.↩︎"
  },
  {
    "objectID": "course-links.html",
    "href": "course-links.html",
    "title": "Useful links",
    "section": "",
    "text": "Posit Cloud\n🔗 on Posit Cloud\n\n\nLecture recordings\n🔗 on Panopto / Duke Capture [TO DO: Add link]\n\n\nGradebook\n🔗 on Canvas [TO DO: Add link]\n\n\nTexbooks\n🔗 Introduction to Modern Statistics, 2nd Edition\n🔗 R for Data Science, 2nd Edition\n\n\nPackage documentation\n🔗 tidyverse: tidyverse.org\n🔗 tidymodels: tidymodels.org"
  },
  {
    "objectID": "computing/computing-troubleshooting.html",
    "href": "computing/computing-troubleshooting.html",
    "title": "Computing troubleshooting",
    "section": "",
    "text": "If you’re having difficulty launching an RStudio session from Posit Cloud, go to status.posit.co and check under Posit Cloud.\n\nIf it shows “Operational”, this means the system is expected to be working. Check your internet connection, if need be, restart your computer to ensure a fresh new connection. If your issue persists, post on the course Slack with details on what you’ve tried and the errors you see (including verbatim errors and/or screenshots).\nIf it shows “Partial Outage” or “Outage” (or anything other than “Operational”), this means there is a known incident with the containers. Check back later to see if it’s been resolved. If there’s a deadline coming up soon, post on the course Slack to let us know that there’s an issue. We can look into how quickly it might get resolved and decide on what to do about the deadline accordingly. Note: We don’t anticipate this to happen regularly, the systems are Operational a huge majority of the time!s"
  },
  {
    "objectID": "computing/computing-cheatsheets.html",
    "href": "computing/computing-cheatsheets.html",
    "title": "R cheatsheets",
    "section": "",
    "text": "The following cheatsheets come from https://posit.co/resources/cheatsheets. We haven’t covered every function and functionality listed on them, but you might still find them useful as references."
  },
  {
    "objectID": "labs/lab-2.html",
    "href": "labs/lab-2.html",
    "title": "Lab 2",
    "section": "",
    "text": "To be posted…"
  },
  {
    "objectID": "labs/lab-4.html",
    "href": "labs/lab-4.html",
    "title": "Lab 4",
    "section": "",
    "text": "To be posted…"
  },
  {
    "objectID": "labs/lab-8.html",
    "href": "labs/lab-8.html",
    "title": "Lab 8",
    "section": "",
    "text": "To be posted…"
  },
  {
    "objectID": "labs/lab-5.html",
    "href": "labs/lab-5.html",
    "title": "Lab 5",
    "section": "",
    "text": "To be posted…"
  },
  {
    "objectID": "project/project-2.html",
    "href": "project/project-2.html",
    "title": "Project 2",
    "section": "",
    "text": "More info to be posted."
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Teaching team",
    "section": "",
    "text": "Dr. Mine Çetinkaya-Rundel (she/her) is Professor of the Practice and Director of Undergraduate Studies at the Department of Statistical Science at Duke University and an affiliated faculty in the Computational Media, Arts, and Cultures. Mine’s work focuses on innovation in statistics and data science pedagogy, with an emphasis on computing, reproducible research, student-centered learning, and open-source education as well as pedagogical approaches for enhancing retention of women and under-represented minorities in STEM.\n\n\n\nOffice hours\nLocation\n\n\n\n\nTue 3:30 - 5:30 pm\nOld Chem 213"
  },
  {
    "objectID": "course-team.html#instructor",
    "href": "course-team.html#instructor",
    "title": "Teaching team",
    "section": "",
    "text": "Dr. Mine Çetinkaya-Rundel (she/her) is Professor of the Practice and Director of Undergraduate Studies at the Department of Statistical Science at Duke University and an affiliated faculty in the Computational Media, Arts, and Cultures. Mine’s work focuses on innovation in statistics and data science pedagogy, with an emphasis on computing, reproducible research, student-centered learning, and open-source education as well as pedagogical approaches for enhancing retention of women and under-represented minorities in STEM.\n\n\n\nOffice hours\nLocation\n\n\n\n\nTue 3:30 - 5:30 pm\nOld Chem 213"
  },
  {
    "objectID": "course-team.html#teaching-assistants",
    "href": "course-team.html#teaching-assistants",
    "title": "Teaching team",
    "section": "Teaching assistants",
    "text": "Teaching assistants\n\n\n\n\nName\nRole\nLab section\nOffice hours\n\n\n\n\n\nShuo Wang\nHead + Lab TA\nFri 8:30 - 9:45 am\nMon 3 - 5 pm\nOld Chemistry 203B\n\n\n\nSylvia Vincent\nLab TA\nFri 10:05 - 11:20 am\nWed 3 - 5 pm\nLocation TBA\n\n\n\nJohn Gillen\nLab TA\nFri 11:45 am - 1:00 pm\nWed 10:30 am - 12:30 pm\nZoom\n\n\n\nChris Oswald\nLab TA\nFri 1:25 - 2:40 pm\nMon 9 - 10 am\nTue 9 - 10 am\nJones Open Lab (Bostock Library, 1st Floor)\n\n\n\nMinh Anh To\nTA\n\nMon 10 am - 12 pm\nOld Chemistry 203B\n\n\n\nHao Wang\nTA\n\nTue 10 am - 12 pm\nOld Chemistry 203B\n\n\n\nNoah Obuya\nTA\n\nSun 12 - 2 pm\nZoom\n\n\n\nMeghna Katyal\nTA\n\nMon 7:30 - 8:30 pm\nZoom\n\n\n\nAvery Hodges\nTA\n\nMon 5 - 7 pm\nZoom"
  },
  {
    "objectID": "ae/ae20.html",
    "href": "ae/ae20.html",
    "title": "Hypothesis tests and confidence intervals",
    "section": "",
    "text": "Exam 2 Thursday\n\nno lab Friday, no TA office hours Friday/Monday"
  },
  {
    "objectID": "ae/ae20.html#bulletin",
    "href": "ae/ae20.html#bulletin",
    "title": "Hypothesis tests and confidence intervals",
    "section": "",
    "text": "Exam 2 Thursday\n\nno lab Friday, no TA office hours Friday/Monday"
  },
  {
    "objectID": "ae/ae20.html#today",
    "href": "ae/ae20.html#today",
    "title": "Hypothesis tests and confidence intervals",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\ndefine type I and type II error\ncompare hypothesis tests with confidence intervals\npractice conducting hypothesis tests"
  },
  {
    "objectID": "ae/ae20.html#getting-started",
    "href": "ae/ae20.html#getting-started",
    "title": "Hypothesis tests and confidence intervals",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae20.qmd\",\ndestfile = \"ae20.qmd\")"
  },
  {
    "objectID": "ae/ae20.html#load-packages",
    "href": "ae/ae20.html#load-packages",
    "title": "Hypothesis tests and confidence intervals",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)"
  },
  {
    "objectID": "ae/ae20.html#practice",
    "href": "ae/ae20.html#practice",
    "title": "Hypothesis tests and confidence intervals",
    "section": "Practice",
    "text": "Practice\n\nLoad data\nThe stent30 data set comes from the openintro package and is from a study conducted in 2011 on the effects of arterial stents as a therapy for stroke patients. See the original publication:\nChimowitz MI, Lynn MJ, Derdeyn CP, et al. 2011. Stenting versus Aggressive Med- ical Therapy for Intracranial Arterial Stenosis. New England Journal of Medicine 365:993- 1003. doi: 10.1056/NEJMoa1105335.\nor check ?stent30 for more information.\n\ndata(stent30)\n\n\nglimpse(stent30)\n\nRows: 451\nColumns: 2\n$ group   &lt;fct&gt; treatment, treatment, treatment, treatment, treatment, treatme…\n$ outcome &lt;fct&gt; stroke, stroke, stroke, stroke, stroke, stroke, stroke, stroke…\n\n\n\nExercise 1\nDo stents affect stroke outcome in patients?\n\nWrite the null and alternative hypothesis. Report the observed statistic.\nSimulate under the null and visualize the null distribution.\nCompute and report the p-value, compare to \\(\\alpha = 0.05\\) and make a conclusion with appropriate context"
  },
  {
    "objectID": "ae/ae20.html#confidence-intervals-and-hypothesis-tests",
    "href": "ae/ae20.html#confidence-intervals-and-hypothesis-tests",
    "title": "Hypothesis tests and confidence intervals",
    "section": "Confidence intervals and hypothesis tests",
    "text": "Confidence intervals and hypothesis tests\nHere we revisit the data from the first three seasons of NC Courage games (2017-2019).\n\ncourage = read_csv(\"https://sta101-fa22.netlify.app/static/labs/data/courage.csv\")\n\n\nglimpse(courage)\n\nRows: 78\nColumns: 10\n$ game_id     &lt;chr&gt; \"washington-spirit-vs-north-carolina-courage-2017-04-15\", …\n$ game_date   &lt;chr&gt; \"4/15/2017\", \"4/22/2017\", \"4/29/2017\", \"5/7/2017\", \"5/14/2…\n$ game_number &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ home_team   &lt;chr&gt; \"WAS\", \"NC\", \"NC\", \"BOS\", \"ORL\", \"NC\", \"NC\", \"CHI\", \"NC\", …\n$ away_team   &lt;chr&gt; \"NC\", \"POR\", \"ORL\", \"NC\", \"NC\", \"CHI\", \"NJ\", \"NC\", \"KC\", \"…\n$ opponent    &lt;chr&gt; \"WAS\", \"POR\", \"ORL\", \"BOS\", \"ORL\", \"CHI\", \"NJ\", \"CHI\", \"KC…\n$ home_pts    &lt;dbl&gt; 0, 1, 3, 0, 3, 1, 2, 3, 2, 3, 0, 0, 2, 1, 1, 0, 1, 2, 2, 2…\n$ away_pts    &lt;dbl&gt; 1, 0, 1, 1, 1, 3, 0, 2, 0, 1, 1, 1, 0, 0, 0, 1, 2, 0, 3, 1…\n$ result      &lt;chr&gt; \"win\", \"win\", \"win\", \"win\", \"loss\", \"loss\", \"win\", \"loss\",…\n$ season      &lt;dbl&gt; 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2017…\n\n\nDo National Women’s Soccer League (NWSL) teams have a home-field advantage? We’ll answer this question in a few separate ways.\nHypothesis testing framework: does NC Courage score a significantly different number of points (on average) away than at home?\n\nExercise 2\n\nCreate a new column location that tells you whether the courage are “home” or “away”\nCreate a new column pts that always reports the Courage points scored in a game.\nSave your result as a new data frame titled courage2.\n\n\n# code here\n\n\n\nExercise 3\nTo answer the question does NC Courage score a significantly different number of points (on average) away than at home?\n\nWrite the null and alternative hypothesis. Report the observed statistic.\nSimulate under the null and visualize the null distribution.\nCompute and report the p-value, compare to \\(\\alpha = 0.05\\) and make a conclusion with appropriate context\n\n\n# code here\n\n\n\nExercise 4\n\nReport the mean difference between away and home games and report a 95% bootstrap confidence interval. Use set.seed(3) and reps=5000 Interpret your interval in context.\n\n\n# code here\n\n\n\nExercise 5\nIs there a better way we could investigate whether or not the Courage have a home-field advantage? Why?"
  },
  {
    "objectID": "ae/ae20.html#notes",
    "href": "ae/ae20.html#notes",
    "title": "Hypothesis tests and confidence intervals",
    "section": "Notes",
    "text": "Notes\n\nType 1 and Type 2 Errors\n\n\n\nTruth\nReject the null\nFail to reject the null\n\n\n\n\n\\(H_0\\) is true\nType 1 error\n✔️\n\n\n\\(H_A\\) is true\n✔️\nType 2 error\n\n\n\nThe significance level, \\(\\alpha\\), is the probability of a type 1 error. In some contexts, a type 1 error may be referred to as a “false positive” and a type 2 error as a “false negative”.\nIntuitively, by considering extremes, one can see a trade-off exists between type 1 and type 2 error.\n\nIf \\(\\alpha = 0\\), then the p-value stands no chance of being smaller than \\(\\alpha\\) and we always fail to reject the null. This makes type 1 errors impossible.\n\nSimilarly, if \\(\\alpha = 1\\), then all p-values will be smaller than \\(\\alpha\\) and type 2 errors will become impossible, because we will always reject the null.\n\\(\\beta\\) is used to denote the probability of a type 2 error.\nThe power of a test is \\(1 - \\beta\\), which is the probability that your test rejects the null hypothesis when the null hypothesis is false."
  },
  {
    "objectID": "ae/ae20.html#why-its-important-to-be-careful-with-interpretation",
    "href": "ae/ae20.html#why-its-important-to-be-careful-with-interpretation",
    "title": "Hypothesis tests and confidence intervals",
    "section": "Why it’s important to be careful with interpretation",
    "text": "Why it’s important to be careful with interpretation\n\n(And why hypothesis tests don’t tell the whole story)\nThe data for this example comes from Confounding and Simpson’s paradox1 by Julious and Mullee.\nThe data examines 901 individuals with diabetes and includes the following variables\n\ninsulin_dep: whether or not the patient has insulin dependent or non-insulin dependent diabetes\nage: whether or not the individual is less than 40 years old\nsurvival: whether or not the individual survived the length of the study\n\n\ndiabetes = read_csv(\"https://sta101.github.io/static/appex/data/diabetes.csv\")\n\nFlex Aisher thinks people with insulin dependent diabetes actually survive longer than those without insulin dependence. Flex wants to formally test his hypothesis.\nLet \\(p_{d}\\) be the probability of insulin dependent survival and \\(p_{i}\\) be the probability of insulin independent survival.\n\\[\nH_0: p_{d} - p_{i} = 0\n\\]\n\\[\nH_A: p_{d} - p_{i} &gt; 0\n\\]\nAt first glance the data seem to back up his claim…\n\n\nExercise 6\nCompute the probability of survival and death for diabetic individuals with and without insulin dependence.\n\n#  code here\n\n\n\nExercise 7\nIs Flex’s claim significant at the \\(\\alpha = 0.05\\) level? Perform a hypothesis test and report your results.\n\n# code here\n\n\n\nExercise 8\nIs the aggregate data misleading? Use the code chunk below to investigate further.\n\n# code here"
  },
  {
    "objectID": "ae/ae20.html#footnotes",
    "href": "ae/ae20.html#footnotes",
    "title": "Hypothesis tests and confidence intervals",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nJulious, S A, and M A Mullee. “Confounding and Simpson’s paradox.” BMJ (Clinical research ed.) vol. 309,6967 (1994): 1480-1. doi:10.1136/bmj.309.6967.1480↩︎"
  },
  {
    "objectID": "ae/ae8.html",
    "href": "ae/ae8.html",
    "title": "Linear regression III",
    "section": "",
    "text": "Lab 3 due tonight\nLooking towards next week, please fill out this optional form to request group members (from your lab) to work on the projects with.\nExam 1 released tonight and due Monday\n\nno TA office hours Friday/Monday\nask questions early"
  },
  {
    "objectID": "ae/ae8.html#bulletin",
    "href": "ae/ae8.html#bulletin",
    "title": "Linear regression III",
    "section": "",
    "text": "Lab 3 due tonight\nLooking towards next week, please fill out this optional form to request group members (from your lab) to work on the projects with.\nExam 1 released tonight and due Monday\n\nno TA office hours Friday/Monday\nask questions early"
  },
  {
    "objectID": "ae/ae8.html#warm-up",
    "href": "ae/ae8.html#warm-up",
    "title": "Linear regression III",
    "section": "Warm-up",
    "text": "Warm-up\nCheck you understanding! Answer the following…\n\nTo “fit” a linear model means…[fill in the blank]\nIs \\(y = \\beta_0 + \\beta_1 \\log(x_1)+ \\beta_2 x_2^2 + \\epsilon\\) a linear model? Why or why not?"
  },
  {
    "objectID": "ae/ae8.html#today",
    "href": "ae/ae8.html#today",
    "title": "Linear regression III",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nfit and interpret models with categorical predictors\nfit and interpret models with interactive effects"
  },
  {
    "objectID": "ae/ae8.html#getting-started",
    "href": "ae/ae8.html#getting-started",
    "title": "Linear regression III",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console (bottom left of screen)\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae8.qmd\",\ndestfile = \"ae8.qmd\")"
  },
  {
    "objectID": "ae/ae8.html#load-packages-and-data",
    "href": "ae/ae8.html#load-packages-and-data",
    "title": "Linear regression III",
    "section": "Load packages and data",
    "text": "Load packages and data\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(palmerpenguins)\n\nToday we will return to our Palmer penguins data set\n\ndata(penguins)\n\nUse ?penguins or click here for more info."
  },
  {
    "objectID": "ae/ae8.html#notes",
    "href": "ae/ae8.html#notes",
    "title": "Linear regression III",
    "section": "Notes",
    "text": "Notes"
  },
  {
    "objectID": "ae/ae8.html#main-effects",
    "href": "ae/ae8.html#main-effects",
    "title": "Linear regression III",
    "section": "Main effects",
    "text": "Main effects\nUp until now, we’ve seen models that look like this:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\epsilon\n\\]\nHere’s an example:\n\\(y\\): body mass (g)\n\\(x_1\\): bill length (mm)\n\\(x_2\\): 1 if island Dream, 0 otherwise\n\\(x_3\\): 1 if island Torgersen, 0 otherwise\n\\[\ny = \\beta_0 + {\\beta_1} x_1 + {\\beta_2} x_2 + {\\beta_3} x_3 + \\epsilon\n\\]\nNotice that\n\nWe have a categorical predictor island that takes three values: Dream, Torgersen, and Biscoe.\nDespite taking three values, there are only two island variables in the model. One for Dream and one for Torgersen. Biscoe island is considered the default. This always occurs when we have a categorical variable – one category is considered the default.\nBill length only impacts body mass via the term \\(\\beta_1 x_1\\).\n\\(x_2\\) and \\(x_3\\) can be thought of as turning on or off a constant.\n\nLet’s visualize the main effects model below.\n\n\n\n\n\nWe can fit the “main effects” model above with our standard procedure:\n\nmain_fit = linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(body_mass_g ~ bill_length_mm + island, data = penguins)\n\n  main_fit %&gt;%\n  tidy()\n\n# A tibble: 4 × 5\n  term            estimate std.error statistic  p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)       1226.     243.        5.04 7.58e- 7\n2 bill_length_mm      77.1      5.31     14.5  1.66e-37\n3 islandDream       -919.      58.6     -15.7  5.15e-42\n4 islandTorgersen   -523.      85.5      -6.12 2.64e- 9\n\n\nIf we want to know how bill length relates to body mass for penguins on island Biscoe, we plug in \\(0\\) for \\(x_2\\) and \\(x_3\\) and write the resulting model. If we repeat as appropriate for each island, the result is 3 separate fitted models:\nBiscoe:\n\\[\n\\hat{y} = 1225.8 + 77.1 x_1\n\\]\nDream:\n\\[\n\\hat{y} = 1225.8 + 77.1 x_1 - 919.1\n\\]\nTorgersen:\n\\[\n\\hat{y} = 1225.8 + 77.1 x_1 -523.3\n\\]\nNotice that in each case, the slope associated with bill length (\\(x_1\\)) is the same.\n\nInteraction effects\nInteraction effect models contain products of predictors, e.g.\n\\[\ny = {\\beta_0} + {\\beta_1} x_1 + {\\beta_2} x_2 + {\\beta_3} x_3 +  {\\beta_4} x_1 x_2 + {\\beta_5} x_1 x_3 + \\epsilon\n\\]\nHere we have an interaction between bill length and island (\\(\\beta_4 x_1 x_2\\) and \\(\\beta_5 x_1 x_3\\)).\nTake-away idea: \\(x_1\\) is related to \\(y\\) but the relationship changes depending on \\(x_2\\) and \\(x_3\\).\nThe simplest scenario is one of “group membership”. In other words, knowing the group your measurement belongs to affects the relationship between \\(x_1\\) and \\(y\\).\nHere, we see bill length (\\(x_1\\)) show up multiple times in our linear model paired with islands. In other words, the relationship between bill length and body mass depends on the island a penguin is from.\nWe fit this interaction model using the code below:\n\ninteraction_fit = linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(body_mass_g ~ bill_length_mm * island, data = penguins)\n\n  interaction_fit %&gt;%\n  tidy()\n\n# A tibble: 6 × 5\n  term                           estimate std.error statistic  p.value\n  &lt;chr&gt;                             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)                     -1726.     292.       -5.91 8.43e- 9\n2 bill_length_mm                    142.       6.42     22.2  9.14e-68\n3 islandDream                      4479.     395.       11.3  2.03e-25\n4 islandTorgersen                  2871.     778.        3.69 2.60e- 4\n5 bill_length_mm:islandDream       -121.       8.77    -13.8  1.93e-34\n6 bill_length_mm:islandTorgersen    -76.6     19.5      -3.92 1.07e- 4\n\n\n\n\n\n\n\nInterpreting interactions can be difficult, especially without writing things down. To make it easier, we will compare the implied linear models:\nPlug in 0 for islandDream (\\(x_2\\)) and 0 for islandTorgersen (\\(x_3\\)) to get the linear model for islandBiscoe penguins\nPlug in 1 for islandDream (\\(x_2\\)) and 0 for islandTorgersen (\\(x_3\\)) to get the linear model for islandDream penguins\nPlug in 0 for islandDream (\\(x_2\\)) and 1 for islandTorgersen (\\(x_3\\)) to get the linear model for islandTorgersen penguins\n\nBiscoe fitted model:\n\n\\[\n\\hat{y} = -1726.0+ 142.3 x_1\n\\]\n\nDream fitted model:\n\n\\[\n\\hat{y} = -1726.0 + 142.3 x_1 + 4478.7 -120.6 x_1\n\\]\nCombine terms:\n\\[\n\\hat{y} = 2752.7 + 21.7 x_1\n\\]\n\nExercise 1\nWrite out the fitted model for Torgersen island below.\n\nTorgersen model: \\[\n\\hat{y} = [\\text{write here}]\n\\]\n\n\n\n\nInterpreting\nNow we can interpret the interaction model by comparing bill length slopes between islands.\n\nFor a unit increase in bill length of a penguin from the island Dream, how much do we expect the body mass to increase?\n\n\nExercise 2\n\nYou measured the bill length of a penguin from island Biscoe and a penguin from island Torgersen a year ago. You re-measure them today and find the bill length of each one grew by exactly 2 mm. How much mass do you expect each penguin to have gained?\n\n\n\nExercise 3\nAre the intercepts meaningful?\n\n\nExercise 4\nIs the relationship between Body mass (g) and Bill depth (mm) positive or negative? Create a convincing argument from the data.\n\n\nExercise 5\nBuild a linear model of body mass using bill depth and one other predictor of your choosing (hint: see previous exercise!)\n\nWrite out a linear model with both predictors and fit the model.\nFit the linear model\nDo you prefer this model to the interaction effects model from a previous exercise? Why?"
  },
  {
    "objectID": "ae/ae14.html",
    "href": "ae/ae14.html",
    "title": "Likelihoods",
    "section": "",
    "text": "Lab 05 due Thursday\nFinal project released"
  },
  {
    "objectID": "ae/ae14.html#bulletin",
    "href": "ae/ae14.html#bulletin",
    "title": "Likelihoods",
    "section": "",
    "text": "Lab 05 due Thursday\nFinal project released"
  },
  {
    "objectID": "ae/ae14.html#today",
    "href": "ae/ae14.html#today",
    "title": "Likelihoods",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nbe able to define a likelihood function\nunderstand the intuition behind likelihood-based inference"
  },
  {
    "objectID": "ae/ae14.html#getting-started",
    "href": "ae/ae14.html#getting-started",
    "title": "Likelihoods",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae14.qmd\",\ndestfile = \"ae14.qmd\")"
  },
  {
    "objectID": "ae/ae14.html#load-packages",
    "href": "ae/ae14.html#load-packages",
    "title": "Likelihoods",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\ncoin_flips = read_csv(\n  \"https://sta101-fa22.netlify.app/static/appex/data/coin_flips_LE.csv\"\n  )"
  },
  {
    "objectID": "ae/ae14.html#the-likelihood-function",
    "href": "ae/ae14.html#the-likelihood-function",
    "title": "Likelihoods",
    "section": "The likelihood function",
    "text": "The likelihood function\n\\[\nL(\\theta | x) = f(x | \\theta)\n\\]\nThe likelihood of parameter(s) \\(\\theta\\) given the data \\(x\\) is equivalent to the density of sample \\(x\\) given the parameter(s) \\(\\theta\\).\n\nIf \\(x\\) is a discrete random variable, the likelihood function is the probability of the data given \\(\\theta\\).\n\n\nExample 1\n\\[\nX \\sim \\text{Binomial}(k, p)\n\\]\nwhere \\(k\\) is the number of trials and \\(p\\) is the probability of a success. The parameters of the distribution are \\(k\\) and \\(p\\).\n\n\n\n\n\n\nNote\n\n\n\nRecall: you can find the formula for the binomial density function in the documentation. See ?dbinom\n\n\n\nExercise 1\nImagine we flip a coin 10 times and observe 7 heads and 3 tails. All together we have:\n\n\\(k\\): __\n\\(x\\): __\n\nThere is some true \\(p\\) for this coin. What is your best guess (or estimate), \\(\\hat{p}\\)?\n\nnum_success = coin_flips %&gt;%\n  summarize(total = sum(result)) %&gt;%\n  pull(total)\n\nlikelihood = function(p) {\n  return(dbinom(x = num_success, size = 10, prob = p))\n}\n\nggplot() +\nxlim(0, 1) +\n  geom_function(fun = likelihood) +\n  theme_bw() + \n  labs(x = \"p\", y = \"Likelihood\",\n       title = \"Likelihood of 7/10 coin flips landing heads as a function of p\") \n\n\n\n\nIn the example above, \\(p\\) is the parameter we are interested in, so we would write \\(L(p | x)\\). If we maximize \\(L(p | x)\\), we obtain the maximum likelihood estimate \\(\\hat{p}\\). The maximum likelihood estimate is the value of the parameter that maximizes the likelihood function."
  },
  {
    "objectID": "ae/ae14.html#example-2",
    "href": "ae/ae14.html#example-2",
    "title": "Likelihoods",
    "section": "Example 2",
    "text": "Example 2\nLikelihood-based inference works with continuous random variables as well.\n\\[\nX \\sim N(\\mu, \\sigma)\n\\] where \\(\\mu\\) is the mean (location) and \\(\\sigma\\) is the standard deviation (scale). \\(\\mu\\) and \\(\\sigma\\) are the parameters of the distribution.\n\nExercise 2\nImagine resting heart rates in this class are normally distributed with unknown mean \\(\\mu\\) and a standard deviation of 5 beats per minute. You randomly sample 1 person in the class and find their heart rate is 72 beats per minute. What is your best guess for the mean, \\(\\hat{\\mu}\\)?\n[answer here]\nYou randomly sample two additional people and find their resting heart rate is 65 and 75 beats per minute. What is your new best guess \\(\\hat{\\mu}\\) given these three data points?\n[answer here]\nUse geom_vline() to add your guesses to the plots below.\n\nlikelihoodNormal1 = function(mu) {\n  return(dnorm(x = 72 , mean = mu, sd = 5))\n}\n\nlikelihoodNormal2 = function(mu) {\n  return(dnorm(x = 65 , mean = mu, sd = 5))\n}\n\nlikelihoodNormal3 = function(mu) {\n  return(dnorm(x = 75 , mean = mu, sd = 5))\n}\n\n\n\nggplot() +\nxlim(50, 90) +\n  geom_function(fun = likelihoodNormal1) +\n  theme_bw() +\n  labs(x = \"Mu\",\n       y = \"Likelihood\",\n       title = \"Likelihood of mu given 1 data point\")\n\n\n\n\nIt’s a similar story for each of the other two data points. If we want to find the likelihood of some parameter under three independent observations, we can multiply our likelihoods together.\n\ncombinedLikelihood = function(mu) {\n  return(likelihoodNormal1(mu) * \n           likelihoodNormal2(mu) * \n           likelihoodNormal3(mu))\n}\n\nggplot() +\nxlim(50, 90) +\n  geom_function(fun = combinedLikelihood) +\n  theme_bw() +\n  labs(x = \"Mu\",\n       y = \"Likelihood\",\n       title = \"Likelihood of mu given 3 data points\")\n\n\n\n\nWhat do you notice about the shape of the function as the number of data points increases?\n\n\n\n\n\n\nNote\n\n\n\nAs the number of data points increases, the product of several likelihood functions gets very small very quickly. For this reason we often work with the log-likelihood. It is a fact that if \\(x &lt; y\\) then \\(\\log(x) &lt; \\log(y)\\). This property is known as monotonicity. Because of this, the maximum of the log-likelihood will be the same as the maximum of the likelihood function.\n\n\n\n\nExample 3\nIn all of the examples above, the findings were obvious upon inspection. How do you compute the likelihood in, e.g. a regression setting? Remember that \\(\\text{AIC} = 2k - 2 \\log \\text{likelihood}\\).\nA simple model:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\epsilon\n\\]\nA common assumption:\n\\[\n\\epsilon \\sim N(0, \\sigma)\n\\]\nFrom this assumption:\n\\[\ny \\sim N(\\beta_0 + \\beta_1x_1, \\sigma)\n\\]\n\nExercise 3\nAssume \\(\\sigma = 2\\) is known and \\(\\beta_0 = 1\\). You observe the data point, \\((x,y) = (1, 6.5)\\). What is your best estimate \\(\\hat{\\beta_1}\\)?\n\nregLikelihood = function(beta1) {\n  return(dnorm(x = 6.5, mean = (1 + (beta1*1)), sd = 2))\n}\n\nggplot() +\nxlim(-1, 12) +\n  geom_function(fun = regLikelihood) +\n  theme_bw() +\n  labs(x = \"beta1\",\n       y = \"Likelihood\",\n       title = \"Likelihood of beta1 given 1 data points\")\n\n\n\n\nIn the code above, why do we write x = 6.5 when \\(6.5\\) is the observed value of \\(y\\)?\n[answer here]\nYou observe two more data points, \\((2, 10)\\) and \\((5,26)\\). What is your best guess of \\(\\hat{\\beta_1}\\) based on the three points you observed?\n\nregLikelihood2 = function(beta1) {\n  return(dnorm(x = 10, mean = (1 + (beta1*2)), sd = 2))\n}\n\nregLikelihood3 = function(beta1) {\n  return(dnorm(x = 26, mean = (1 + (beta1*5)), sd = 2))\n}\n\ncombinedRegLikelihood = function(beta1){ \n  return(log(regLikelihood(beta1)) + log(regLikelihood2(beta1)) +\n           log(regLikelihood3(beta1)))\n  }\n\n\nggplot() +\nxlim(-2, 12) +\n  geom_function(fun = combinedRegLikelihood) +\n  theme_bw() +\n  labs(x = \"beta1\",\n       y = \"log-likelihood\",\n       title = \"Likelihood of beta1 given 3 data points\")"
  },
  {
    "objectID": "ae/ae21.html",
    "href": "ae/ae21.html",
    "title": "Ethics in Statistics and Data Science",
    "section": "",
    "text": "Lab 09\nThis Friday is last lab before peer-review in two weeks"
  },
  {
    "objectID": "ae/ae21.html#bulletin",
    "href": "ae/ae21.html#bulletin",
    "title": "Ethics in Statistics and Data Science",
    "section": "",
    "text": "Lab 09\nThis Friday is last lab before peer-review in two weeks"
  },
  {
    "objectID": "ae/ae21.html#today",
    "href": "ae/ae21.html#today",
    "title": "Ethics in Statistics and Data Science",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\ncritically examine graphics, models and results\ndiscuss data privacy and redundancy\nanalyze a real data example of Simpson’s paradox"
  },
  {
    "objectID": "ae/ae21.html#getting-started",
    "href": "ae/ae21.html#getting-started",
    "title": "Ethics in Statistics and Data Science",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae21.qmd\",\ndestfile = \"ae21.qmd\")"
  },
  {
    "objectID": "ae/ae21.html#load-packages",
    "href": "ae/ae21.html#load-packages",
    "title": "Ethics in Statistics and Data Science",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\nGuidelines for Discussion\n\nListen respectfully. Listen actively and with an ear to understanding others’ views.\nCriticize ideas, not individuals.\nCommit to learning, not debating. Comment in order to share information, not to persuade.\nAvoid blame, speculation, and inflammatory language.\nAvoid assumptions about any member of the class or generalizations about social groups."
  },
  {
    "objectID": "ae/ae21.html#data-representation",
    "href": "ae/ae21.html#data-representation",
    "title": "Ethics in Statistics and Data Science",
    "section": "Data Representation",
    "text": "Data Representation\n\nMisleading Data Visualizations1\nBrexit\n\n\n\nBrexit\n\n\n\nWhat is the graph trying to show?\nWhy is this graph misleading?\nHow can you improve this graph?\n\nSpurious Correlations2\n\n\n\nA Spurious Correlation\n\n\n\nWhat is the graph trying to show?\nWhy is this graph misleading?"
  },
  {
    "objectID": "ae/ae21.html#statistical-modeling",
    "href": "ae/ae21.html#statistical-modeling",
    "title": "Ethics in Statistics and Data Science",
    "section": "Statistical modeling",
    "text": "Statistical modeling\nRead, with a critical eye, page 2 and table 1 from Physician–patient racial concordance and disparities inbirthing mortality for newborns and chat with your neighbor."
  },
  {
    "objectID": "ae/ae21.html#data-privacy",
    "href": "ae/ae21.html#data-privacy",
    "title": "Ethics in Statistics and Data Science",
    "section": "Data privacy",
    "text": "Data privacy\n\nWeb scraping3\nA data analyst received permission to post a data set that was scraped from a social media site. The full data set included name, screen name, email address, geographic location, IP (Internet protocol) address, demographic profiles, and preferences for relationships. The analyst removes name and email address from the data set in effort to deidentify it.\n\nWhy might it be problematic to post this data set publicly?\nHow can you store the full dataset in a safe and ethical way?\nYou want to make the data available so your analysis is transparent and reproducible. How can you modify the full data set to make the data available in an ethical way?\n\n\n\nRedundancy\n\nslides\n\n\n\nAdditional readings\n\nWhy pokemon go’s plan to 3d scan the world is dangerous\nHow companies learn your secrets\n\n\n\nDiscussion questions\n\n“Simpson’s paradox”, where conclusions drawn from analyzing subgroups differ from conclusions drawn when the groups are combined. Can you demonstrate Simpson’s Paradox with the data below? 4\nFor further reading see Bickel, Peter J., Eugene A. Hammel, and J. William O’Connell. “Sex Bias in Graduate Admissions: Data from Berkeley: Measuring bias is harder than is usually assumed, and the evidence is sometimes contrary to expectation.” Science 187.4175 (1975): 398-404.\n\n\nberk = read_csv(\"https://sta101.github.io/static/appex/data/BerkeleyAdmissionsData.csv\")\n\nRows: 7 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Dept\ndbl (4): MaleYes, MaleNo, FemaleYes, FemaleNo\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nberk\n\n# A tibble: 7 × 5\n  Dept  MaleYes MaleNo FemaleYes FemaleNo\n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 A         512    313        89       19\n2 B         313    207        17        8\n3 C         120    205       202      391\n4 D         138    279       131      244\n5 E          53    138        94      299\n6 F          22    351        24      317\n7 All      1158   1493       557     1278"
  },
  {
    "objectID": "ae/ae21.html#footnotes",
    "href": "ae/ae21.html#footnotes",
    "title": "Ethics in Statistics and Data Science",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSource: https://humansofdata.atlan.com/2019/02/dos-donts-data-visualization↩︎\nSource: https://www.tylervigen.com/spurious-correlations Content warning: some examples include death or suicide.↩︎\nModified from Modern Data Science with R, 2nd Edition↩︎\nSource: https://www.randomservices.org/random/data/Berkeley.html↩︎"
  },
  {
    "objectID": "ae/ae24.html",
    "href": "ae/ae24.html",
    "title": "Forensic genetic analysis",
    "section": "",
    "text": "Wednesday office hours moved to Thursday after class\nLab 09 due Thursday. Draft peer-report due Friday (in lab review)\ncourse evaluations open. \\(&gt;80\\%\\) response \\(\\rightarrow\\) +1pt final project"
  },
  {
    "objectID": "ae/ae24.html#bulletin",
    "href": "ae/ae24.html#bulletin",
    "title": "Forensic genetic analysis",
    "section": "",
    "text": "Wednesday office hours moved to Thursday after class\nLab 09 due Thursday. Draft peer-report due Friday (in lab review)\ncourse evaluations open. \\(&gt;80\\%\\) response \\(\\rightarrow\\) +1pt final project"
  },
  {
    "objectID": "ae/ae24.html#getting-started",
    "href": "ae/ae24.html#getting-started",
    "title": "Forensic genetic analysis",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae24.qmd\",\ndestfile = \"ae24.qmd\")"
  },
  {
    "objectID": "ae/ae24.html#load-packages",
    "href": "ae/ae24.html#load-packages",
    "title": "Forensic genetic analysis",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "ae/ae24.html#background",
    "href": "ae/ae24.html#background",
    "title": "Forensic genetic analysis",
    "section": "Background",
    "text": "Background\nDNA evidence is sometimes used in court. In this AE, you will learn about the case of Dr. Schmidt from Lafayette, Louisiana, who was accused of infecting his former lover with HIV through a contaminated blood sample of one of his patients. Read more about this court case here and here."
  },
  {
    "objectID": "ae/ae24.html#today",
    "href": "ae/ae24.html#today",
    "title": "Forensic genetic analysis",
    "section": "Today",
    "text": "Today\nBy the end of today you will\n\nbe able to critical think about the use of DNA evidence and statistics in court\nanalyze non-numerical data rigorously"
  },
  {
    "objectID": "ae/ae24.html#explore-the-data",
    "href": "ae/ae24.html#explore-the-data",
    "title": "Forensic genetic analysis",
    "section": "Explore the data",
    "text": "Explore the data\n\ndf_HIV = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/HIV.csv\")\n\ndf_HIV contains several observations of HIV genomes. Within this data set are two samples with special ids: sample1 and sample2.\nFor the purpose of this exercise, you might imagine sample1 is associated with the HIV sampled from the plaintiff while sample2 belongs to that of the defendant’s patient.\nFundamentally, we are interested in whether or not sample1 and sample2 are closely related.\n\nExercise 1\n\nhow many observations are present in the data set?\nwhat are the observational units?\nhow many bases does the first DNA sequence contain? Hint: use the R function nchar."
  },
  {
    "objectID": "ae/ae24.html#extracting-a-sub-sequence-of-dna",
    "href": "ae/ae24.html#extracting-a-sub-sequence-of-dna",
    "title": "Forensic genetic analysis",
    "section": "Extracting a sub-sequence of DNA",
    "text": "Extracting a sub-sequence of DNA\nFor computational speed, we will have to work with shorter sub-sequences of DNA.\nThe function str_sub from the package stringr in the Tidyverse can be used to extract a sub-string from a character vector.\n\nExercise 2\n\nhow many arguments does the function str_sub take? What does each argument do?\nuse str_sub to extract (i) the first four letters of the words statistics, (ii) the sub-string between the third and the seventh letters, and (iii) the last four letters\n\n\nterm = \"statistics\"\n\nLet’s use str_sub to extract the first 500 bases from the DNA sequences using the template below.\n\nHIV = df_HIV %&gt;% \n  mutate(dna_short = ___) %&gt;%\n  select(-dna)\n\n\nuse nchar to verify that each sub-sequence dna_short contains 500 bases. Hint: nchar is vectorized, meaning if given a vector input, it will return a vector output."
  },
  {
    "objectID": "ae/ae24.html#computing-the-pairwise-distances-between-the-dna-sequence",
    "href": "ae/ae24.html#computing-the-pairwise-distances-between-the-dna-sequence",
    "title": "Forensic genetic analysis",
    "section": "Computing the pairwise distances between the DNA sequence",
    "text": "Computing the pairwise distances between the DNA sequence\nNow that the data have been prepared, we will establish how similar/different each DNA sequence is to the others. To accomplish this, given two DNA sequences we will count the number of bases for which they differ. The rationale for this step is the following. If two DNA differ on many bases, it means that they have evolved separately for a while and had the time to undergo numerous mutations. On the other hand, if they only differ on a few bases, it means that the two sequences have only recently began to evolve separately.\nThe following code creates a data frame where each row corresponds to a pair of DNA sequences.\n\nd_pairs &lt;- combn(HIV$person_id, 2) %&gt;%\n  t() %&gt;% # go from wide matrix (2 rows) to long matrix (2 columns)\n  as_tibble() %&gt;%\n  rename(id1 = V1, id2 = V2) %&gt;%\n  left_join(HIV, by = c(\"id1\" = \"person_id\")) %&gt;% # add dna for person 1\n  rename(dna1 = dna_short) %&gt;%\n  left_join(HIV, by = c(\"id2\" = \"person_id\")) %&gt;% # add dna for person 1\n  rename(dna2 = dna_short)\n\nQuestion: What are the dimensions of d_pairs?\n\nHamming distance\nTo measure the distance between two sequences, we first consider the Hamming distance\n\\[\nd(i,j) = \\sum_{k=1}^n 1\\{\\text{dna}_{ik} \\neq \\text{dna}_{jk}\\}\n\\]\nwhich counts the number of elements that are different between two sequences. Here \\(d(i,j)\\) denotes the Hamming distance between sequences \\(i\\) and \\(j\\), and \\(1\\{\\text{dna}_{ik} \\neq \\text{dna}_{jk}\\}\\) is equal to \\(1\\) is the \\(k\\)-th element of sequence \\(i\\) is different from that of sequence \\(j\\).\nThe following code computes the Hamming distance between each pair of sequences. We first construct function compute_hamming which computes the Hamming distance between two DNA sequences. We then apply this function to each row of the d_pairs data frame.\n\ncompute_hamming &lt;- function(dna1, dna2) {\n  \n  dna1_split &lt;- str_split(dna1, pattern = \"\", simplify = TRUE)\n  dna2_split &lt;- str_split(dna2, pattern = \"\", simplify = TRUE)\n  \n  hamming_distance &lt;- sum(dna1_split != dna2_split)\n  return(hamming_distance)\n}\n\nd_hamming &lt;- d_pairs %&gt;%\n  mutate(\n    distance_ham = list(dna1, dna2) %&gt;% pmap_dbl(compute_hamming)\n  )\n\n\nExercise 3\n\nMake a histogram of the Hamming distances and describe the distribution.\n\n\nFind the 10 pairs of DNA sequences that are the closest to the plaintiff’s sequence in terms of Hamming distance.\n\n\nWhich sequence is most closely related to the plaintiff’s sequence?\nHow many differences are there between the DNA sequence of the plaintiff and that of the defendant’s patient?\nCan you identify a shortcoming of the Hamming distance? Does a large Hamming distance necessarily imply that the two DNA sequences are very different?\n\n\n\n\nAn alternative measure of distance for DNA sequences\nLet us now consider an alternative measure of distance for DNA sequences.\n\nd_biology &lt;- d_pairs %&gt;%\n  mutate(\n    distance_bio = list(dna1, dna2) %&gt;% pmap_dbl(adist)\n    )\n\n\nExercise 4\n\nAgain, make a histogram of the new distance and find the 10 DNA sequences closest to the plaintiff’s.\n\n\nWhich of the two measures do you find more adequate for these data?\n\n\n\nExercise 5\n\nImagine that you are a juror in this court case, would you find this piece of evidence convincing? Would you like to have more information? If so, what additional information would you need? Now, imagine that you are a judge; do you think that this piece of evidence should be admitted to court? Why?"
  },
  {
    "objectID": "ae/ae15.html",
    "href": "ae/ae15.html",
    "title": "The bootstrap",
    "section": "",
    "text": "Lab 05 due tonight"
  },
  {
    "objectID": "ae/ae15.html#bulletin",
    "href": "ae/ae15.html#bulletin",
    "title": "The bootstrap",
    "section": "",
    "text": "Lab 05 due tonight"
  },
  {
    "objectID": "ae/ae15.html#today",
    "href": "ae/ae15.html#today",
    "title": "The bootstrap",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nbe able to draw a bootstrap sample and calculate a bootstrap statistic\nuse infer to obtain a bootstrap distribution\ncalculate a confidence interval from the bootstrap distribution\ninterpret a confidence interval in context of the data"
  },
  {
    "objectID": "ae/ae15.html#getting-started",
    "href": "ae/ae15.html#getting-started",
    "title": "The bootstrap",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae15.qmd\",\ndestfile = \"ae15.qmd\")"
  },
  {
    "objectID": "ae/ae15.html#load-packages",
    "href": "ae/ae15.html#load-packages",
    "title": "The bootstrap",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "ae/ae15.html#load-data",
    "href": "ae/ae15.html#load-data",
    "title": "The bootstrap",
    "section": "Load data",
    "text": "Load data\n\nmanhattan = read_csv(\n  \"https://sta101-fa22.netlify.app/static/appex/data/manhattan.csv\"\n  )"
  },
  {
    "objectID": "ae/ae15.html#notes-for-reference",
    "href": "ae/ae15.html#notes-for-reference",
    "title": "The bootstrap",
    "section": "Notes (for reference)",
    "text": "Notes (for reference)\nBootstrapping is a re-sampling technique. The key idea is you have already collected a sample of size \\(N\\) from the population. To create a bootstrap sample, you sample with replacement from your original sample \\(N\\) times.\nLet’s say you measure the height of five Duke students in meters:\n\nheights = c(1.51, 1.62, 1.89, 2.01, 1.78)\n\nstudents = data.frame(heights)\n\nThere are many ways to create a bootstrap sample in R. We will focus on the tidy way below. which uses the infer package that loads with tidymodels.\n\nExample\n\nset.seed(2)\nstudents %&gt;%\n  specify(response = heights) %&gt;%\n  generate(reps = 1, type = \"bootstrap\")\n\nResponse: heights (numeric)\n# A tibble: 5 × 2\n# Groups:   replicate [1]\n  replicate heights\n      &lt;int&gt;   &lt;dbl&gt;\n1         1    1.78\n2         1    1.51\n3         1    1.78\n4         1    1.51\n5         1    2.01\n\n\n\n\n\n\n\n\nNote\n\n\n\nSampling is random. Notice the seed above ensures we get the same bootstrap sample.\n\n\nFrom here, we can compute a bootstrap statistic. E.g.\n\nset.seed(2)\nstudents %&gt;%\n  specify(response = heights) %&gt;%\n  generate(reps = 1, type = \"bootstrap\") %&gt;%\n  calculate(stat = \"median\")\n\nResponse: heights (numeric)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  1.78"
  },
  {
    "objectID": "ae/ae15.html#example-rent-in-manhattan",
    "href": "ae/ae15.html#example-rent-in-manhattan",
    "title": "The bootstrap",
    "section": "Example: rent in Manhattan",
    "text": "Example: rent in Manhattan\nOn a given day in 2018, twenty one-bedroom apartments were randomly selected on Craigslist Manhattan from apartments listed as “by owner”. The data are in the manhattan data frame. We will use this sample to conduct inference on the typical rent of 1 bedroom apartments in Manhattan.\n\nPart 1: Drawing a bootstrap sample\nLet’s start by using bootstrapping to estimate the mean rent of one-bedroom apartments in Manhattan.\n\nExercise 1\nWhat is a point estimate (i.e. single number summary) of the typical rent?\n\n\nExercise 2\nLet’s bootstrap!\n\nTo bootstrap we will sample with replacement by drawing a value from the box.\nHow many draws do we need for our bootstrap sample?\n\nFill in the values from the bootstrap sample conducted in class. Once the values are filled in, un-comment the code.\n\n# class_bootstrap = c()\n\n\n\nExercise 3\n\nAbout what value do you expect the bootstrap statistic to take?\nCalculate the statistic from the bootstrap sample.\n\n\n# add code\n\n\n\n\nPart 2: Bootstrap confidence interval\nWe will calculate a 95% confidence interval for the mean rent of one-bedroom apartments in Manhattan.\nWe start by setting a seed to ensure our analysis is reproducible.\n\nGenerating the bootstrap distribution\nWe can use R to take many bootstrap samples, compute a statistic and then view the bootstrap distribution of that statistic.\nUn-comment the lines and fill in the blanks to create the bootstrap distribution of sample means and save the results in the data frame boot_dist.\nUse 1000 reps for the in-class activity. (You will use about 10,000 reps for assignments outside of class.)\n\nset.seed(7182022)\n\nboot_dist = manhattan #%&gt;%\n  #specify(______) %&gt;%\n  #generate(______) %&gt;%\n  #calculate(______)\n\n\nHow many rows are in boot_dist?\nWhat does each row represent?\nWhat are the variables in boot_dist? What do they mean?\n\n\n\nVisualize the bootstrap distribution\nA sample statistic is a random variable, we can look at its distribution.\nVisualize the bootstrap distribution using a histogram. Describe the shape and center of the distribution.\n\n# add code\n\n\n\nCalculate the confidence interval\nUncomment the lines and fill in the blanks to construct the 95% bootstrap confidence interval for the mean rent of one-bedroom apartments in Manhattan.\n\n#___ %&gt;%\n#  summarize(lower = quantile(______),\n  #          upper = quantile(______))\n\n\n\nInterpret the interval\nWrite the interpretation for the interval calculated above.\n\nQuestion: Does a confidence interval have to be symmetric?\nWhat is one advantage to using a 90% confidence interval instead of a 95% confidence interval to estimate a parameter? - What is one advantage to using a 99% confidence interval instead of a 95% confidence interval to estimate a parameter?"
  },
  {
    "objectID": "ae/ae6.html",
    "href": "ae/ae6.html",
    "title": "Simple Regression",
    "section": "",
    "text": "lab 02 due tonight\nexam 01 next week"
  },
  {
    "objectID": "ae/ae6.html#bulletin",
    "href": "ae/ae6.html#bulletin",
    "title": "Simple Regression",
    "section": "",
    "text": "lab 02 due tonight\nexam 01 next week"
  },
  {
    "objectID": "ae/ae6.html#today",
    "href": "ae/ae6.html#today",
    "title": "Simple Regression",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nunderstand the grammar of linear modeling, including \\(y\\), \\(x\\), \\(\\beta\\), \\(\\epsilon\\), fitted estimates and residuals\nadd linear regression plots to your 2D graphs\nbe able to write a simple linear regression model mathematically and\nfit the model to data in R in a tidy way"
  },
  {
    "objectID": "ae/ae6.html#getting-started",
    "href": "ae/ae6.html#getting-started",
    "title": "Simple Regression",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console (bottom left of screen)\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae6.qmd\",\ndestfile = \"ae6.qmd\")"
  },
  {
    "objectID": "ae/ae6.html#load-packages-and-data",
    "href": "ae/ae6.html#load-packages-and-data",
    "title": "Simple Regression",
    "section": "Load packages and data",
    "text": "Load packages and data\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nToday’s data is Apple and Microsoft stock prices from January 1st 2020 to December 31st 2021. I pulled this data off the Yahoo finance using their API via the tidyquant package July 2022.\n\nstocks = read_csv(\"https://sta101.github.io/static/appex/data/stocks1.csv\")"
  },
  {
    "objectID": "ae/ae6.html#notes",
    "href": "ae/ae6.html#notes",
    "title": "Simple Regression",
    "section": "Notes",
    "text": "Notes\n\nThe simple regression model and notation\n\\[\ny = \\beta_0 + \\beta_1 x + \\epsilon\n\\]\n\n\\(y\\): the outcome variable. Also called the “response” or “dependent variable”. In prediction problems, this is what we are interested in predicting.\n\\(x\\): the predictor. Also commonly referred to as “regressor”, “independent variable”, “covariate”, “feature”, “the data”.\n\\(\\beta_0\\), \\(\\beta_1\\) are called “constants” or coefficients. They are fixed numbers. These are population parameters. \\(\\beta_0\\) has another special name, “the intercept”.\n\\(\\epsilon\\): the error. This quantity represents observational error, i.e. the difference between our observation and the true population-level expected value: \\(\\beta_0 + \\beta_1 x\\).\n\nEffectively this model says our data \\(y\\) is linearly related to \\(x\\) but is not perfectly observed due to some error.\n\n\nA simple example\nLet’s examine January 2020 open prices of Microsoft and Apple stocks to illustrate some ideas.\n\nstocks_subset = stocks %&gt;%\n  slice(1:21)\n\nstocks_subset %&gt;%\n  ggplot(aes(x = MSFT.Open, y = AAPL.Open)) +\n  geom_point() + \n  labs(x = \"MSFT Open\", y = \"AAPL Open\", title = \"Open prices of MSFT and AAPL January 2020\") +\n  theme_bw() \n\n\n\n  # more code here\n\n\nExercise 1\nAdd geom_abline() to the above plot and try different slopes and intercepts until you find a trendline you are satisfied with. The equation below describes your fitted model. Re-write the equation below, filling in \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) with your estimates.\n\\[\n\\hat{y} = \\hat{\\beta_0} +  \\hat{\\beta_1} x\n\\]\n\n\\(\\hat{y}\\) is the expected outcome.\n\\(\\hat{\\beta}\\) is the estimated or fitted coefficient\nthere is no error term here because we do not predict error\n\nThe equation of my line above:\n\\[\n\\text{[your equation here]}\n\\]\nThe central idea is that if we measure every \\(x\\) and every \\(y\\) in existence, (“the entire population”) there is some true “best” \\(\\beta_0\\) and \\(\\beta_1\\) that describe the relationship between \\(x\\) and \\(y\\). Since we only have a sample of the data, we estimate \\(\\beta_0\\) and \\(\\beta_1\\). We call our estimates \\(\\hat{\\beta_0}\\), \\(\\hat{\\beta_1}\\) “beta hat”. We never have all the data, thus we never can really know what the true \\(\\beta\\)s are."
  },
  {
    "objectID": "ae/ae6.html#ordinary-least-squares-ols-regression",
    "href": "ae/ae6.html#ordinary-least-squares-ols-regression",
    "title": "Simple Regression",
    "section": "Ordinary least squares (OLS) regression",
    "text": "Ordinary least squares (OLS) regression\n\nThe residuals\nFor any linear equation we write down, there will be some difference between the predicted outcome of our linear model (\\(\\hat{y}\\)) and what we observe (\\(y\\))… (But of course! Otherwise everything would fall on a perfect straight line!)\nThis difference between what we observe and what we predict \\(y - \\hat{y}\\) is known as a residual \\(r\\).\nMore concisely,\n\\[\nr = y - \\hat{y}\n\\]\nResiduals are dependent on the line we draw. Visually, here is a model of the data, \\(y = -5 + \\frac{1}{2}x\\) and 1 of the residuals is outlined in red.\n\n\n\n\n\nThere is, in fact, a residual associated with every single point in the plot.\n\npredictAAPL = function(x) {\n  return(-5 + (0.5*x))\n}\n\nxPoints = stocks$MSFT.Open[1:21]\nyPoints = stocks$AAPL.Open[1:21]\nyHat = predictAAPL(xPoints)\n\nstocks_subset %&gt;%\n  ggplot(aes(x = MSFT.Open, y = AAPL.Open)) +\n  geom_point() + \n  labs(x = \"MSFT Open\", y = \"AAPL Open\", title = \"Open prices of MSFT and AAPL January 2020\") +\n  theme_bw() +\n  geom_abline(slope = 0.5, intercept = -5) +\n  geom_segment(x = xPoints, xend = xPoints, y  = yPoints, yend = yHat, color = 'red')\n\n\n\n\nWe often wish to find a line that fits the data “really well”, but what does this mean? Well, we want small residuals! So we pick an objective function. That is, a function we wish to minimize or maximize.\n\n\nThe objective function\n\nExercise 2\nAt first, you might be tempted to minimize \\(\\sum_i r_i\\), but this is problematic. Why? Can you come up with a better solution (other than the one listed below)?\n[answer here]\nIn practice, we minimize the sum of squared residuals:\n\\[\n\\sum_i r_i^2\n\\]\nNote, this is the same as\n\\[\n\\sum_i (y_i - \\hat{y})^2\n\\]\n\n\nExercise 3\nCheck out an interactive visualization of “least squares regression” here. Click on I and drag the points around to get started. Describe what you see.\n[response here]\n\n\nExercise 4\n\nCheck for understanding\n\nHow far off is your model (from exercise 1) from the actual observed data on January 11 2020? The observed value is MSFT: $164.35 and AAPL: $78.4. Compute the single square residual using your fitted model from exercise 1.\n\n# code here"
  },
  {
    "objectID": "ae/ae6.html#plotting-the-ols-regression-line",
    "href": "ae/ae6.html#plotting-the-ols-regression-line",
    "title": "Simple Regression",
    "section": "Plotting the OLS regression line",
    "text": "Plotting the OLS regression line\nPlotting the OLS regression line, that is, the line that minimizes the sum of square residuals is very easy with ggplot. Simply add\ngeom_smooth(method = 'lm', se = F)\nto your plot.\nmethod = lm says to draw a line according to a “linear model” and se = F turns off standard error bars. You can try without these options for comparison.\nOptionally, you can change the color of the line, e.g.\ngeom_smooth(method = 'lm', se = F, color = 'red')\n\nExercise 5\nCopy your code from exercise 1 below. Add geom_smooth() as described above with color = 'steelblue' to see how close your line is.\n\n# code here"
  },
  {
    "objectID": "ae/ae6.html#finding-hatbeta",
    "href": "ae/ae6.html#finding-hatbeta",
    "title": "Simple Regression",
    "section": "Finding \\(\\hat{\\beta}\\)",
    "text": "Finding \\(\\hat{\\beta}\\)\nTo fit the model in R, i.e. to “find \\(\\hat{\\beta}\\)”, use the code below as a template:\nmodelFit = linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(y-variable-here ~ x-variable-here, data = data-frame-here)\n\nlinear_reg tells R we will perform linear regression\nset_engine tells R to use the standard linear modeling (lm) machinery\nfit defines the outcome \\(y\\), predictor \\(x\\) and the data set\n\nRunning the code above, but replacing the arguments of the fit command appropriately will create a new object called “modelFit” (defined on the first line) that stores all information about your fitted model.\nTo access the information, you can run, e.g.\ntidy(modelFit)\nLet’s try it out.\n\nExercise 6\nFind the OLS fitted linear model \\(y = \\hat{\\beta_0} + \\hat{\\beta_1} x\\) for January 2020, where \\(x\\) is Microsoft’s opening price and \\(y\\) is Apple’s opening price. Print your results to the screen\n\n# code here\n\n\n\nExercise 7\nRe-write the fitted equation replacing \\(\\beta_0\\) and \\(\\beta_1\\) with the OLS fitted values.\n\\[\n\\text{[your equation here]}\n\\]"
  },
  {
    "objectID": "ae/ae13.html",
    "href": "ae/ae13.html",
    "title": "The Normal Distribution",
    "section": "",
    "text": "Mid-semester grades posted\nRegression project due Saturday October 15"
  },
  {
    "objectID": "ae/ae13.html#bulletin",
    "href": "ae/ae13.html#bulletin",
    "title": "The Normal Distribution",
    "section": "",
    "text": "Mid-semester grades posted\nRegression project due Saturday October 15"
  },
  {
    "objectID": "ae/ae13.html#today",
    "href": "ae/ae13.html#today",
    "title": "The Normal Distribution",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\ndefine stable distribution, normal distribution, location and scale parameters\nplot normal distributions and calculate probabilities"
  },
  {
    "objectID": "ae/ae13.html#getting-started",
    "href": "ae/ae13.html#getting-started",
    "title": "The Normal Distribution",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae13.qmd\",\ndestfile = \"ae13.qmd\")"
  },
  {
    "objectID": "ae/ae13.html#load-packages",
    "href": "ae/ae13.html#load-packages",
    "title": "The Normal Distribution",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "ae/ae13.html#normal-distribution",
    "href": "ae/ae13.html#normal-distribution",
    "title": "The Normal Distribution",
    "section": "Normal distribution",
    "text": "Normal distribution\nThe normal distribution, also known as “Gaussian distribution” is a distribution of a continuous random variable. The sample space of a normal random variable is \\(\\{- \\infty, + \\infty \\}\\) and is defined by two parameters: a mean \\(\\mu\\) and a standard deviation \\(\\sigma\\). The mean is known as the location parameter while the standard deviation is the scale.\nWe can sample N times from a normal with mean mu and standard deviation sigma using rnorm(n = N, mean = mu, sd = sigma).\n\nset.seed(123)\n# example\nsample = rnorm(1000, mean = 0, sd = 1)\n\nmean(sample)\nsd(sample)\n\nhist(sample)\n\nLet’s visualize the normal function curve using the code below.\n\nmu1 = 100\ns1 = 6\n\nmu2 = 105\ns2 = 3\n\nggplot(data = data.frame(x = c(mu1 - s1*3, mu1 + s1*3)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = mu1, sd = s1),\n                color = \"steelblue\") +\n  stat_function(fun = dnorm, args = list(mean = mu2, sd = s2),\n                color = \"orange\") + \n  theme_bw() +\n  labs(title = \"Two normal curves\")\n\n\n\n\n\nExercise 1\nTry setting mu2 to 99 and s2 to 4. What do you notice? Play with a few more settings and describe what the mean and standard deviation do to the shape of the curve.\n\n\nExercise 2\nStart with the code below of a histogram but change your geometry to geom_histogram(aes(y = ..density..)). This will re-scale your histogram so that the area under the curve is 1. Next, use the stat_function code above as a template to superimpose a normal distribution on top of your histogram. Adjust the mean and standard deviation until you obtain a good looking fit. What do you notice?\n\nset.seed(1013)\nx = rbinom(n = 10000, size = 100, prob = 0.8)\ndf = data.frame(x) \n\ndf %&gt;%\n  ggplot(aes(x = x)) + \n  geom_histogram(bins = 35) +\n  theme_bw() +\n  labs(x = \"X\", title = \"Distribution of heads in 100 biased coin flips\")"
  },
  {
    "objectID": "ae/ae13.html#properties-of-a-gaussian",
    "href": "ae/ae13.html#properties-of-a-gaussian",
    "title": "The Normal Distribution",
    "section": "Properties of a Gaussian",
    "text": "Properties of a Gaussian\nIf \\(X\\) is a random variable, and \\(X\\) is normally distributed, then it the distribution of \\(X\\) is fully specified by the location (mean) and scale (standard deviation) parameters, \\(\\mu\\) and \\(\\sigma\\) respectively. In mathematical notation we write\n\\[\nX \\sim N(\\mu, \\sigma)\n\\] and this reads: “X is normally distributed with mean mu and standard deviation sigma.” It is worth noting that in some contexts (see e.g. “notation” on wikipedia) the second term of a Normal distribution refers to the variance or \\(\\sigma^2\\).\nA very useful property of Normal distribution is that it is stable. What this means is that linear combinations of normal random variables are themselves normal. In other words, if \\(X\\) and \\(Y\\) are normal random variables then \\(aX + bY\\) is normal for all \\(a\\) and \\(b\\). The two properties to remember when adding Gaussian random variables are:\n\nmean(X + Y) = mean(X) + mean(Y)\nvariance(X + Y) = variance(X) + variance(Y) when X and Y are independent\n\nWe can see this in an example.\nLet \\(X \\sim N(5, 3)\\) and let \\(Y \\sim N(-5, 1)\\)\nAccording to our rules above \\(X + Y \\sim N(0, \\sqrt{10})\\). Let’s check ourselves with code:\n\nset.seed(1)\nnormal_df = data.frame(X = rnorm(1000, mean = 5, sd = 3),\n                 Y = rnorm(1000, mean = -5, sd = 1))\n\n\n\nnormal_df = normal_df %&gt;%\n  mutate(Z = X + Y)\n\nnormal_df %&gt;%\n  ggplot(aes(x = Z)) +\n  geom_histogram(aes(y = ..density..), fill = 'gold3', alpha = 0.5) +\n  theme_bw() +\n  labs(y = \"Density\", title = \"Matching densities\") +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = sqrt(10)),\n                color = \"darkblue\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nExercise 3\nLet \\(Z \\sim N(0, 1)\\) and \\(X \\sim N(10, 4)\\)\n\\(aZ + b \\stackrel{d}{=} X\\) for some \\(a\\) and \\(b\\). What are \\(a\\) and \\(b\\)? Fill them in for the ? in the code below and the uncomment the code. Here “\\(\\stackrel{d}{=}\\) means”equal in distribution”.\n\nset.seed(1)\nnormal_df = data.frame(z = rnorm(1000, mean = 0, sd = 1),\n                 x = rnorm(1000, mean = 10, sd = 4))\n\nsample_mean = normal_df %&gt;%\n  summarize(sample_mean = mean(x)) %&gt;%\n  pull()\n\nnormal_df %&gt;%\n  ggplot(aes(x = x)) +\n  geom_histogram(aes(y = ..density..), fill = 'gold3', alpha = 0.5) +\n  geom_histogram(aes(x = z, y = ..density..), fill = 'steelblue', alpha = 0.7) +\n  #geom_density(aes(x = z*? + ?), color = 'red') + \n  theme_bw() +\n  labs(x = \"\", y = \"\", title = \"Sampling from two normals\")"
  },
  {
    "objectID": "ae/ae13.html#computing-probabilities",
    "href": "ae/ae13.html#computing-probabilities",
    "title": "The Normal Distribution",
    "section": "Computing probabilities",
    "text": "Computing probabilities\npnorm “probability normal” takes three arguments:\n\nq, mean and sd\n\nand pnorm(q = q, mean = mu, sd = sigma) answers the question:\nIf \\(X \\sim N(\\mu, \\sigma)\\), what is \\(p(X &lt; q)\\)?\nFor example, imagine that the resting heart rates in the classroom are normally distributed with mean 70 beats per minute (bpm) and standard deviation 5 bpm. What’s the probability a randomly selected individual has a heart rate less than 63 bpm?\nIn math: let \\(X\\) be the bpm of an individual in this class. Assume \\(X \\sim N(70, 5)\\). What is \\(p(X &lt; 63)\\)? Given heartbeats are normally distributed, randomly selecting an individual from the classroom is called “drawing from a normal distribution”.\nWe can compute this easily:\n\npnorm(63, 70, 5)\n\n[1] 0.08075666\n\n\n0.08 or about 8% chance. In picture, the probability is the proportion of area under the curve shaded:\n\n\n\n\n\n\nExercise 4\nWhat is the probability a randomly selected student in the class has a heart beat greater than 75 bpm?\n\n# code here"
  },
  {
    "objectID": "ae/ae5.html",
    "href": "ae/ae5.html",
    "title": "EDA III: Joins",
    "section": "",
    "text": "Lab 02 due Thursday"
  },
  {
    "objectID": "ae/ae5.html#bulletin",
    "href": "ae/ae5.html#bulletin",
    "title": "EDA III: Joins",
    "section": "",
    "text": "Lab 02 due Thursday"
  },
  {
    "objectID": "ae/ae5.html#today",
    "href": "ae/ae5.html#today",
    "title": "EDA III: Joins",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\njoin data frames"
  },
  {
    "objectID": "ae/ae5.html#getting-started",
    "href": "ae/ae5.html#getting-started",
    "title": "EDA III: Joins",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console (bottom left of screen)\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae5.qmd\",\n  destfile = \"ae5.qmd\")"
  },
  {
    "objectID": "ae/ae5.html#load-packages",
    "href": "ae/ae5.html#load-packages",
    "title": "EDA III: Joins",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "ae/ae5.html#joins",
    "href": "ae/ae5.html#joins",
    "title": "EDA III: Joins",
    "section": "Joins",
    "text": "Joins\nThere are six join functions in the dplyr package. Join functions take two data frames as arguments and return a data frame.\nThe six joins can be broken down into two categories:\n\nMutating joins: joining data frames results in mutating new columns\nFiltering joins: joining data frames results in filtering observations in one data frame based on another.\n\nIn all of the following examples, x and y are two data frames:\n\nx = tibble(value = c(100, 200, 300),\n            xcol = c(\"x1\", \"x2\", \"x3\"))\ny = tibble(value = c(100, 200, 400),\n            ycol = c(\"y1\", \"y2\", \"y4\"))\nx\n\n# A tibble: 3 × 2\n  value xcol \n  &lt;dbl&gt; &lt;chr&gt;\n1   100 x1   \n2   200 x2   \n3   300 x3   \n\ny\n\n# A tibble: 3 × 2\n  value ycol \n  &lt;dbl&gt; &lt;chr&gt;\n1   100 y1   \n2   200 y2   \n3   400 y4   \n\n\n\nMutating joins\nThe most popular 2 joins:\n\nleft_join(x, y): keep all rows from x and adds columns from y\nright_join(x, y): keeps all rows from y and adds columns from x\n\nTwo more helpful joins:\n\ninner_join(x, y): join all rows from x where there are matching values in y.Returns all combinations in case of multiple matches\nfull_join(x, y): include all rows in x or y\n\nToy examples:\n\nx %&gt;%\n  left_join(y)\n\nJoining with `by = join_by(value)`\n\n\n# A tibble: 3 × 3\n  value xcol  ycol \n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1   100 x1    y1   \n2   200 x2    y2   \n3   300 x3    &lt;NA&gt; \n\n\n\nWhat do you think Joining, by = \"value\" means?\n\n\nx %&gt;%\n  right_join(y, by = \"value\")\n\n# A tibble: 3 × 3\n  value xcol  ycol \n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1   100 x1    y1   \n2   200 x2    y2   \n3   400 &lt;NA&gt;  y4   \n\n\n\nx %&gt;%\n  inner_join(y)\n\nJoining with `by = join_by(value)`\n\n\n# A tibble: 2 × 3\n  value xcol  ycol \n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1   100 x1    y1   \n2   200 x2    y2   \n\n\n\nfull_join(x, y)\n\nJoining with `by = join_by(value)`\n\n\n# A tibble: 4 × 3\n  value xcol  ycol \n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1   100 x1    y1   \n2   200 x2    y2   \n3   300 x3    &lt;NA&gt; \n4   400 &lt;NA&gt;  y4   \n\n\n\n\nFiltering joins\n\nsemi_join(x, y): return all rows from x with match in y\nanti_join(x, y): return all rows from x without a match in y\n\nIn both of these “filtering” cases we do not add any new columns to our first argument (the data frame x).\nToy examples:\n\nx %&gt;%\nsemi_join(y)\n\nJoining with `by = join_by(value)`\n\n\n# A tibble: 2 × 2\n  value xcol \n  &lt;dbl&gt; &lt;chr&gt;\n1   100 x1   \n2   200 x2   \n\n\n\nx %&gt;%\nanti_join(y)\n\nJoining with `by = join_by(value)`\n\n\n# A tibble: 1 × 2\n  value xcol \n  &lt;dbl&gt; &lt;chr&gt;\n1   300 x3   \n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can manually specify which columns to join by and the columns do not have to have the same name! See the example below.\n\n\nCheck out the new data frame x2:\n\nx2 = x %&gt;%\n  mutate(new_value = value) %&gt;%\n  select(new_value, xcol)\n\nx2\n\n# A tibble: 3 × 2\n  new_value xcol \n      &lt;dbl&gt; &lt;chr&gt;\n1       100 x1   \n2       200 x2   \n3       300 x3   \n\n\nWe can still join x2 with y but left_join(x2, y) won’t work. We have to manually specify which columns to join by:\n\nx2 %&gt;%\n  left_join(y, by = c(\"new_value\" = \"value\"))\n\n# A tibble: 3 × 3\n  new_value xcol  ycol \n      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1       100 x1    y1   \n2       200 x2    y2   \n3       300 x3    &lt;NA&gt;"
  },
  {
    "objectID": "ae/ae5.html#practice",
    "href": "ae/ae5.html#practice",
    "title": "EDA III: Joins",
    "section": "Practice",
    "text": "Practice\nWe’ll take a look at some New York flights data.\nThis data set contains on-time data for all flights that departed NYC (i.e. JFK, LGA or EWR) in 2013.\nThis data is a subset of the data set provided by the nycflights13 package.\n\nCodebook\nflights2 contains\n\nyear: departure year\ntime_hour: departure time\ndep_delay: departure delay in minutes\narr_delay: arrival delay in minutes\norigin: origin\ndest: destination\ncarrier two letter carrier abbreviation\n\nairlines contains\n\ncarrier: two letter carrier abbrevation\nname: full carrier name\n\nairports contains\n\nfaa: FAA airport code\nname: name of airport\nlat: latitude\nlon: longitude\n\n\nflights = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/flights.csv\")\n\nRows: 10000 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): origin, dest, tailnum, carrier\ndbl  (3): year, dep_delay, arr_delay\ndttm (1): time_hour\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nairlines = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/airlines.csv\")\n\nRows: 16 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): carrier, name\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nairports = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/airports.csv\")\n\nRows: 1458 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): faa, name\ndbl (2): lat, lon\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nExercise 1\nflights contains the two letter carrier abbreviations and airlines contains a dictionary. It would be nice if this information was in one data frame so we could read off easily e.g. which airlines are most likely to have arrival delays.\nUse an appropriate join to add the column of airlines to flights and save the resulting data frame as flights2.\n\n# code here\n\n\n\nExercise 2\nUsing flights2, report the average arrival delay for each carrier. Print only the 5 carrier airline with the worst arrival delays on average. No joins required here.\n\n# code here\n\n\n\nExercise 3\nCreate a new data set called dest_delays that reports the median arrival delay at each destination airport.\n\n# code here\n\n\n\nExercise 4\nWhich destination has the worst arrival delay? (Note: you will need to join dest_delays with airports to answer this question)\n\n# code here\n\n\n\nOptional bonus\n\nIs there anything else you might want to learn from the data before declaring one airport is most likely to have delayed arrival or one carrier is most likely to result in a delayed flight?"
  },
  {
    "objectID": "ae/ae11.html",
    "href": "ae/ae11.html",
    "title": "Prediction",
    "section": "",
    "text": "Lab 04 now due Friday October 7\nFriday is last day for project group swaps"
  },
  {
    "objectID": "ae/ae11.html#bulletin",
    "href": "ae/ae11.html#bulletin",
    "title": "Prediction",
    "section": "",
    "text": "Lab 04 now due Friday October 7\nFriday is last day for project group swaps"
  },
  {
    "objectID": "ae/ae11.html#today",
    "href": "ae/ae11.html#today",
    "title": "Prediction",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nbe able to make new predictions from your fitted linear models\nvisualize the fit of your model"
  },
  {
    "objectID": "ae/ae11.html#getting-started",
    "href": "ae/ae11.html#getting-started",
    "title": "Prediction",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae11.qmd\",\ndestfile = \"ae11.qmd\")"
  },
  {
    "objectID": "ae/ae11.html#load-packages-and-data",
    "href": "ae/ae11.html#load-packages-and-data",
    "title": "Prediction",
    "section": "Load packages and data",
    "text": "Load packages and data\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "ae/ae11.html#notes",
    "href": "ae/ae11.html#notes",
    "title": "Prediction",
    "section": "Notes",
    "text": "Notes\n\nPrediction\npredict() is a powerful function that takes two arguments:\n\nyour model fit\nnew data you want to make predictions from\n\nThere are several ways you can use the predict() function.\nFor standard linear regression,\npredict(model_fit, test_data) # returns predicted outcome\nFor logistic regression you can use the code above to obtain the predicted outcome (0 or 1) or alternatively use one of the formulations below to quickly grab the log-odds or the probability of the outcome “1”.\npredict(model_fit$fit, test_data) # returns log-odds\npredict(model_fit, test_data, type = \"prob\") # returns probability of a 1."
  },
  {
    "objectID": "ae/ae11.html#practice",
    "href": "ae/ae11.html#practice",
    "title": "Prediction",
    "section": "Practice",
    "text": "Practice\nLoad data:\n\nparkinsons_train = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/parkinsons_train.csv\")\nparkinsons_test = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/parkinsons_test.csv\")\n\nThis dataset comes from Little et al. (2008). The data includes various measurements of dysphonia (disorders of the voice) from 32 people, 24 with Parkinson’s disease (PD). Multiple measurements were taken per individual. The measurements we examine in this subset of the data include:\n\nname: patient ID\njitter: a measure of relative variation in fundamental frequency\nshimmer: a measure of variation in amplitude (dB)\nPPE: pitch period entropy\nHNR: a ratio of total components vs. noise in the voice recording\nstatus: health status (1 for PD, 0 for healthy)\n\n\nExercise 1\nWrite down a main effects model to predict Parkinson’s status from HNR, shimmer, jitter and PPE.\n\n\nExercise 2\nFit your model from the previous exercise using the parkinsons_train data set.\n\n# code here\n\n\n\nExercise 3\nUse your model to predict PD status in the parkinsons_test data set with a decision boundary of p = 0.5. How many false positives do you observe? How many false negatives?\nNext change the decision boundary to 0.25. How many false positives and false negatives do you observe?\nWhich decision boundary do you prefer?\n\n# code here\n\n\n\nVisualizing model fits\nThere are many ways you can visualize a fitted model. Plotting the hyperplane is limited to simple two-variable (predictor, outcome) and three-variable (predictor, predictor, outcome) plots. Here we explore some useful visualizations for high-dimensional multivariate models.\n\nExample: logistic regression\n\nCreate a stacked bar plot with status on the x-axis and fill by whether or not the predicted status is correct or incorrect.\n\n\n# code here\n\n\n\nExample: ordinary least squares regression:\nScenario: we are trying to predict vocal amplitude variation (shimmer) from jitter and pitch period entropy (PPE).\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\n\\] where\n\\(y\\): shimmer \\(x_1\\): jitter \\(x_2\\): PPE\n\nmyPredictiveModel = linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(shimmer ~ jitter + PPE, data = parkinsons_train)\n\nEven if we don’t have a test data set, we could still create a new column of predictions like before:\n\n# predict based on new data\npredict_train = parkinsons_train %&gt;%\n  mutate(myPrediction = predict(myPredictiveModel, parkinsons_train)$.pred)\n\nFrom here we can plot \\(\\hat{y}\\) vs \\(y\\):\n\npredict_train %&gt;%\n  ggplot(aes(x = shimmer, y = myPrediction)) +\n  geom_point() +\n  labs(x = \"True Shimmer (dB)\", y = \"Predicted shimmer (dB)\", title = \"Predicted vs true shimmer values\") +\n  geom_abline(slope = 1, intercept = 0, color = \"steelblue\")\n\n\n\n\nAlternatively, we could create a residual plot. Residual plots can be used to assess whether a linear model is appropriate.\nA common assumption of linear regression models is that the error term, \\(\\epsilon\\), has constant variance everywhere.\n\nIf the linear model is appropriate, a residual plot should show this.\nPatterned or nonconstant residual spread may sometimes be indicative a model is missing predictors or missing interactions.\n\n\n\nExercise 4\nCreate a new column residuals in predict_train and save your data frame as predict_train_2\n\n# code here\n\n\npredict_train_2 %&gt;%\n  ggplot(aes(x = myPrediction, y = residuals)) + \n  geom_point() +\n  geom_hline(yintercept = 0) +\n  labs(x = \"Predicted shimmer (dB)\", y = \"Residual\")"
  },
  {
    "objectID": "ae/ae3.html",
    "href": "ae/ae3.html",
    "title": "Exploratory Data Analysis I",
    "section": "",
    "text": "Lab 1 due Thursday at 11:59pm on gradescope\nHow to render to pdf directly"
  },
  {
    "objectID": "ae/ae3.html#bulletin",
    "href": "ae/ae3.html#bulletin",
    "title": "Exploratory Data Analysis I",
    "section": "",
    "text": "Lab 1 due Thursday at 11:59pm on gradescope\nHow to render to pdf directly"
  },
  {
    "objectID": "ae/ae3.html#today",
    "href": "ae/ae3.html#today",
    "title": "Exploratory Data Analysis I",
    "section": "Today",
    "text": "Today\nWe’ll begin today by visiting the last figure of ae2.\nBy the end of today you will…\n\nharness the power of filter() using logic\ncreate and interpret scatter plots, bar plots, stacked bar plots, facet plots and be able to look up and use other ggplot geometries"
  },
  {
    "objectID": "ae/ae3.html#getting-started",
    "href": "ae/ae3.html#getting-started",
    "title": "Exploratory Data Analysis I",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae3.qmd\",\ndestfile = \"ae3.qmd\")"
  },
  {
    "objectID": "ae/ae3.html#load-packages",
    "href": "ae/ae3.html#load-packages",
    "title": "Exploratory Data Analysis I",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(viridis) # we'll use to customize colors"
  },
  {
    "objectID": "ae/ae3.html#load-data",
    "href": "ae/ae3.html#load-data",
    "title": "Exploratory Data Analysis I",
    "section": "Load data",
    "text": "Load data\n\ndata(penguins)\n\nType ?palmerpenguins to learn more about this package. Or better yet, check it out here."
  },
  {
    "objectID": "ae/ae3.html#logic-in-r",
    "href": "ae/ae3.html#logic-in-r",
    "title": "Exploratory Data Analysis I",
    "section": "Logic in R",
    "text": "Logic in R\nThe table of logical operators below will be helpful as you work with filtering.\n\n\n\noperator\ndefinition\n\n\n\n\n&lt;\nis less than?\n\n\n&lt;=\nis less than or equal to?\n\n\n&gt;\nis greater than?\n\n\n&gt;=\nis greater than or equal to?\n\n\n==\nis exactly equal to?\n\n\n!=\nis not equal to?\n\n\nis.na(x)\nis x NA?\n\n\n!is.na(x)\nis x not NA?\n\n\nx %in% y\nis x in y?\n\n\n!(x %in% y)\nis x not in y?\n\n\nx & y\nis x AND y?\n\n\nx \\| y\nis x OR y?\n\n\n!x\nis not x?\n\n\n\nThe above operations return TRUE (1) or FALSE (0).\n\nExamples\nHow many penguins have flipper length &gt; 200 mm?\n\npenguins %&gt;%\n  filter(flipper_length_mm &gt; 200)\n\n# A tibble: 148 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Dream               35.7          18                 202        3550\n 2 Adelie  Dream               41.1          18.1               205        4300\n 3 Adelie  Dream               40.8          18.9               208        4300\n 4 Adelie  Biscoe              41            20                 203        4725\n 5 Adelie  Torgersen           41.4          18.5               202        3875\n 6 Adelie  Torgersen           44.1          18                 210        4000\n 7 Adelie  Dream               41.5          18.5               201        4000\n 8 Gentoo  Biscoe              46.1          13.2               211        4500\n 9 Gentoo  Biscoe              50            16.3               230        5700\n10 Gentoo  Biscoe              48.7          14.1               210        4450\n# ℹ 138 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nWe could also pipe into nrow() to quickly grab the number of rows. Try it!\n\nHow many female penguins have flipper length &gt; 200 mm?\n\npenguins %&gt;%\n  filter(flipper_length_mm &gt; 200 & (sex == \"female\"))\n\n# A tibble: 60 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Dream            35.7          18                 202        3550\n 2 Gentoo  Biscoe           46.1          13.2               211        4500\n 3 Gentoo  Biscoe           48.7          14.1               210        4450\n 4 Gentoo  Biscoe           46.5          13.5               210        4550\n 5 Gentoo  Biscoe           45.4          14.6               211        4800\n 6 Gentoo  Biscoe           43.3          13.4               209        4400\n 7 Gentoo  Biscoe           40.9          13.7               214        4650\n 8 Gentoo  Biscoe           45.5          13.7               214        4650\n 9 Gentoo  Biscoe           45.8          14.6               210        4200\n10 Gentoo  Biscoe           42            13.5               210        4150\n# ℹ 50 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nFor how many penguins was flipper length not measured (i.e. reported as NA)?\n\npenguins %&gt;%\n  filter(is.na(flipper_length_mm))\n\n# A tibble: 2 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen             NA            NA                NA          NA\n2 Gentoo  Biscoe                NA            NA                NA          NA\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nHow many penguins are of species Adelie or Chinstrap?\n\npenguins %&gt;%\n  filter(species %in% c(\"Adelie\", \"Chinstrap\"))\n\n# A tibble: 220 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 210 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nExercise 1:\nDouble check the Adelie and Chinstrap total using count().\n\n# code here\n\n\n\nExercise 2:\nWhat is the mean bill length of Adelie penguins? Hint: filter and then use summarize() as seen in lab 1.\n\n# code here\n\n\n\nExercise 3:\nHow many penguins have a bill length greater than 40 mm or a bill depth less than 15 mm?\n\n# code here\n\n\n\nExercise 4:\nWhat proportion of penguins are from the island Torgersen?\n\n# code here"
  },
  {
    "objectID": "ae/ae3.html#plots",
    "href": "ae/ae3.html#plots",
    "title": "Exploratory Data Analysis I",
    "section": "Plots",
    "text": "Plots\nThe procedure used to construct plots can be summarized using the code below.\n\nggplot(data = [data set], \n       mapping = aes(x = [x-variable], y = [y-variable])) +\n   geom_xxx() +\n   geom_xxx() + \n  other options\n\n\nExample: bar plot\n\nggplot(data = penguins, \n       mapping = aes(x = species)) +\n  geom_bar() +\n  labs(x = \"Species\", y = \"Count\", title = \"Palmer penguin species\")\n\n\n\n\n\n\nExample: stacked bar plot\n\npenguins %&gt;%\n  filter(!is.na(sex)) %&gt;%\nggplot(mapping = aes(x = species, fill = sex)) +\n  geom_bar(position = \"fill\") +\n  labs(x = \"Species\", y = \"Count\", title = \"Palmer penguin species\")\n\n\n\n\n\ntry with and without position = \"fill\"\n\n\n\nAesthetics\nAn aesthetic is a visual property in your plot that is derived from the data.\n\nshape\ncolor\nsize\nalpha (transparency)\n\nWe can map a variable in our data set to a color, a size, a transparency, and so on. The aesthetics that can be used with each geom_ can be found in the documentation.\nHere we are going to use the viridis package, which has more color-blind accessible colors. scale_color_viridis specifies which colors you want to use. You can learn more about the options here.\nOther sources that can be helpful in devising accessible color schemes include Color Brewer, the Wes Anderson package, and the cividis package.\nThis visualization shows a scatterplot of bill length (x variable) and flipper length (y variable). Using the viridis function, we make points for male penguins purple and female penguins yellow. We also add axes labels and a title.\n\nggplot(data = penguins, \n       mapping = aes(x = bill_length_mm, y = flipper_length_mm,\n                     color = sex)) + \n   geom_point() + \n   labs(title = \"Flipper length vs bill length\",\n        x = \"Bill length (mm)\", y = \"Flipper length (mm)\") + \n        scale_color_viridis(discrete=TRUE, option = \"D\", name=\"Sex\")\n\nWarning: Removed 11 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\nExercise 5:\nCan you remove the NAs from the above visualization?\nQuestion: What will the visualization look like below? Write your answer down before running the code.\n\nggplot(data = penguins, \n       mapping = aes(x = bill_length_mm, y = flipper_length_mm,\n                     shape = sex)) + \n   geom_point() + \n   labs(title = \"Flipper length vs bill length\",\n        x = \"Bill length (mm)\", y = \"Flipper length (mm)\") + \n        scale_color_viridis(discrete=TRUE, option = \"D\", name=\"Sex\")\n\n\n\n\nFaceting\nWe can use smaller plots to display different subsets of the data using faceting. This is helpful to examine conditional relationships.\n\npenguins %&gt;%\n  ggplot(aes(x = bill_length_mm, flipper_length_mm, color = island)) +\n  geom_point() +\n  facet_wrap(~ species) +\n  labs(x = \"Bill length (mm)\", y = \"Flipper length (mm)\", color = \"Island\")\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\npenguins %&gt;%\n  ggplot(aes(x = bill_length_mm, flipper_length_mm, color = species)) +\n  geom_point() +\n  facet_wrap(~ island) +\n  labs(x = \"Bill length (mm)\", y = \"Flipper length (mm)\", color = \"Island\") +\n  scale_color_viridis(discrete = TRUE)\n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "ae/ae3.html#ggplot-activity",
    "href": "ae/ae3.html#ggplot-activity",
    "title": "Exploratory Data Analysis I",
    "section": "ggplot activity",
    "text": "ggplot activity\n\n# code here"
  },
  {
    "objectID": "ae/ae3.html#additional-resources",
    "href": "ae/ae3.html#additional-resources",
    "title": "Exploratory Data Analysis I",
    "section": "Additional resources",
    "text": "Additional resources\n\nFind more ggplot geometries at https://ggplot2.tidyverse.org/reference/"
  },
  {
    "objectID": "ae/ae18.html",
    "href": "ae/ae18.html",
    "title": "Intro to hypothesis tests",
    "section": "",
    "text": "Lab 7 due Thursday"
  },
  {
    "objectID": "ae/ae18.html#bulletin",
    "href": "ae/ae18.html#bulletin",
    "title": "Intro to hypothesis tests",
    "section": "",
    "text": "Lab 7 due Thursday"
  },
  {
    "objectID": "ae/ae18.html#today",
    "href": "ae/ae18.html#today",
    "title": "Intro to hypothesis tests",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nBe familiar with the terms “p-value”, “null-distribution”, “null hypothesis”, “alternative hypothesis”\nCompute a p-value"
  },
  {
    "objectID": "ae/ae18.html#getting-started",
    "href": "ae/ae18.html#getting-started",
    "title": "Intro to hypothesis tests",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae18.qmd\",\ndestfile = \"ae18.qmd\")"
  },
  {
    "objectID": "ae/ae18.html#load-packages",
    "href": "ae/ae18.html#load-packages",
    "title": "Intro to hypothesis tests",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "ae/ae18.html#introduction-to-hypothesis-testing",
    "href": "ae/ae18.html#introduction-to-hypothesis-testing",
    "title": "Intro to hypothesis tests",
    "section": "Introduction to hypothesis testing",
    "text": "Introduction to hypothesis testing\n\nIs this a fair coin?\nWe’ll record 1 if the coin is “Heads” and 0 if the coin lands “Tails”.\n\n# coin_flips = c()\n\nIf the coin is fair, what is the probability of seeing the outcome we saw? To answer this question we’ll setup a statistical model:\n\\[\n\\text{\\# heads} \\sim Binomial(n, p)\n\\]\nwhere \\(p\\) is the probability of a heads and \\(n\\) is the total number of coin flips.\n\nExercise 2\n\nIf the coin is fair, what would \\(p\\) be?\nUsing R, flip a fair coin 6 times and count the number of heads. Next, repeat this experiment 1000 times and count the proportion of times you observe 6 heads.\n\n\n# code here\n\n\nWhat is the probability of observing 6 heads in 6 coin flips?"
  },
  {
    "objectID": "ae/ae18.html#hypothesis-testing-framework",
    "href": "ae/ae18.html#hypothesis-testing-framework",
    "title": "Intro to hypothesis tests",
    "section": "Hypothesis testing framework",
    "text": "Hypothesis testing framework\nYou may not have realized it but you just performed a hypothesis test!\nYou setup a null hypothesis: \\(H_0\\). The null hypothesis is a hypothesis you set up and then try and knock down. Conceptually, it’s the “nothing special is going on” hypothesis. Formally, the null hypothesis makes a claim or assumption about a population parameter.\nIn this case, it was the assumption that the coin is fair. Mathematically, we write this:\n\\[\nH_0: p = 0.5\n\\]\nConceptually, \\(p\\) is the probability of flipping a head if we flipped the coin infinitely many times.\nSince we computed the probability of observing all heads, we were fundamentally interested in if \\(p &gt; 0.5\\). This is our alternative hypothesis, \\(H_A\\). In mathematical notation, we write\n\\[\nH_A: p &gt; 0.5\n\\]\nNext, we simulated under the null. This means that we simulated what the coin flips would have looked like if the null was true. In this context, this means we simulated as if the coin was fair.\nFinally, we check to see where our actual observed data places under the null distribution. If it’s way out in the tail, we reject the null. If its not way out in the tail, we fail to reject the null.\nHow can we make “way out in the tail” more precise? Well, it’s arbitrary and context-dependent. In some contexts it is popular to use a cut-off of \\(0.05\\). This cutoff is called “the significance level” and is also known as \\(\\alpha\\).\n\nExercise 3\nAssume we continue flipping our coin for a total of 30 coin flips and observe 23 heads and 7 tails. What is the probability of seeing 23 or more heads if the coin is fair?\n\n# code here"
  },
  {
    "objectID": "ae/ae18.html#p-values",
    "href": "ae/ae18.html#p-values",
    "title": "Intro to hypothesis tests",
    "section": "p-values",
    "text": "p-values\nYou might not realize it, but you just computed a p-value… again!\nA p-value is a probability. It’s the tail probability associated with your alternative hypothesis.\nThe alternative hypothesis must always relate to the null. Here we had three options:\n\n\\(H_A: p &lt; 0.5\\), the coin is biased to land tails\n\\(H_A: p &gt; 0.5\\), the coin is biased to land heads\n\\(H_A: p \\neq 0.5\\), the coin is biased\n\nLet’s look offline at what each one would like here.\n\nExercise 4\nCompute the p-value associated with each of the alternative hypotheses above.\n\n# code here\n\nMake a conclusion based on a significance level of 0.05. In other words,\n\nif p &lt; 0.05, reject the null.\nif p &gt; 0.05, we fail to reject the null.\n\nAs you can see our conclusion depends on our alternative hypothesis. For this reason, it is important to set up an alternative hypothesis before looking at the data.\n\n\nRecap\nWe were interested in whether or not a coin was fair. We let \\(p\\) be the probability of landing heads. Fundamentally, we were interested in whether or not \\(p = 0.5\\). This was our null hypothesis:\n\\[\nH_0: p = .5\n\\]\nand our alternative, was that the coin was biased heads: \\[\nH_A: p &gt; 0.5\n\\]\nIn one example our data consisted of 30 coin flips and 23 heads. The proportion of heads that we observed, \\(\\hat{p} = 23/30 = .77\\).\nDo these 30 coin flips give us enough evidence to reject the null in favor of the alternative?\nTo answer this question, we computed the p-value: \\(Pr(\\hat{p} \\geq .77 | H_0 \\text{ true})\\). In words, the probability that our statistic of interest, (\\(\\hat{p}\\)), is greater than or equal to what we saw given that the null is true.\nNotice that the p-value is defined by three things:\n\nour observed statistic (0.77)\nthe null hypothesis (\\(H_0\\))\nthe alternative hypothesis (\\(H_A\\)), this tells us the direction (\\(&gt;=\\)) to shade.\n\nWe compared the p-value to some pre-defined cutoff, \\(\\alpha\\). In our example we set our cutoff at \\(\\alpha = 0.05\\). If p-value \\(&lt; \\alpha\\), we reject the null. If p-value \\(&gt; \\alpha\\), we fail to reject the null.\n\n\nThe tidy way\n\ncoin_flips = data.frame(one_flip = sample(c(rep(\"H\",23), rep(\"T\", 7)), size = 30))\n\nglimpse(coin_flips)\n\nRows: 30\nColumns: 1\n$ one_flip &lt;chr&gt; \"H\", \"H\", \"H\", \"H\", \"T\", \"H\", \"H\", \"T\", \"H\", \"H\", \"H\", \"T\", \"…\n\n\n\nset.seed(2022)\nnull_dist = \n  coin_flips %&gt;% \n  specify(response = one_flip, success = \"H\") %&gt;%\n  hypothesize(null = \"point\", p = 0.5) %&gt;%\n  generate(reps = 10000, type = \"draw\") %&gt;%\n  calculate(stat = \"prop\")\n\nobs_stat = 23/30\n\nvisualize(null_dist) +\n  shade_p_value(obs_stat, direction = \"right\")\n\n\n\nnull_dist %&gt;%\nget_p_value(obs_stat, direction = \"right\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1  0.0027\n\n\np-value of \\(0.0027 &lt; 0.05\\). We reject the null hypothesis that the coin is fair."
  },
  {
    "objectID": "ae/ae9.html",
    "href": "ae/ae9.html",
    "title": "Model Selection",
    "section": "",
    "text": "Please take the first five minutes of class to fill out this survey\nMy Wednesday office hours are now zoom only for the rest of the semester. Monday will remain hybrid."
  },
  {
    "objectID": "ae/ae9.html#bulletin",
    "href": "ae/ae9.html#bulletin",
    "title": "Model Selection",
    "section": "",
    "text": "Please take the first five minutes of class to fill out this survey\nMy Wednesday office hours are now zoom only for the rest of the semester. Monday will remain hybrid."
  },
  {
    "objectID": "ae/ae9.html#today",
    "href": "ae/ae9.html#today",
    "title": "Model Selection",
    "section": "Today",
    "text": "Today\nBy the end of today you will…\n\nselect between linear models with different numbers of predictors"
  },
  {
    "objectID": "ae/ae9.html#getting-started",
    "href": "ae/ae9.html#getting-started",
    "title": "Model Selection",
    "section": "Getting started",
    "text": "Getting started\nDownload this application exercise by pasting the code below into your console (bottom left of screen)\ndownload.file(\"https://sta101-fa22.netlify.app/static/appex/ae9.qmd\",\ndestfile = \"ae9.qmd\")"
  },
  {
    "objectID": "ae/ae9.html#load-packages-and-data",
    "href": "ae/ae9.html#load-packages-and-data",
    "title": "Model Selection",
    "section": "Load packages and data",
    "text": "Load packages and data\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "ae/ae9.html#notes",
    "href": "ae/ae9.html#notes",
    "title": "Model Selection",
    "section": "Notes",
    "text": "Notes\n\nThe problem with \\(R^2\\)\n\\(R^2\\) tell us the proportion of variability in the data our model explains. If we add predictors to our model, we will always improve \\(R^2\\) (regardless of whether the predictor is good or not).\nTo see this…\n\noffline example\ntake away: a line can go through any two points, a plane can go through any three points, etc. In general an \\(n\\) dimensional object can go through \\(n\\) points.\n\nFor this reason, \\(R^2\\) is not a good way to select between two models that have a different number of predictors. Instead, we prefer to use Akaike Information Criterion (AIC).\n\n\nAIC\n\\[\n\\text{AIC} = 2k - 2 \\log (\\text{likelihood})\n\\]\nwhere \\(k\\) is the number of estimated parameters (\\(\\beta\\)s) in the model. Notice this will be 1 + the number of predictors. and \\(\\hat{L}\\) is “likelihood” of the data given the fitted model.\nWe’ll return to the idea of a likelihood later in the semester. For now, it suffices to know that the likelihood is a measure of how well a given model fits the data. Specifically, higher likelihoods imply better fits. Since the AIC score has a negative in front of the log likelihood, lower scores are better fits. However, \\(k\\) penalizes adding new predictors to the model.\nTake-away: lower AIC is better fit.\nYou can find AIC using glance(fitted-model). (Assuming you named your fitted model fitted-model)"
  },
  {
    "objectID": "ae/ae9.html#building-a-model",
    "href": "ae/ae9.html#building-a-model",
    "title": "Model Selection",
    "section": "Building a model",
    "text": "Building a model\nScenario: you have an outcome \\(y\\) you want to predict. You have several variables you’ve measured that you could use as predictors in your linear model. Each predictor is expensive to collect future measurements of. You want your model to only include the most useful predictors.\n\nBackward elimination\nBackward elimination starts with the full model (the model that includes all potential predictor variables). Variables are eliminated one-at-a-time from the model until we cannot improve the model any further.1\nProcedure:\n\nStart with a model that has all predictors we consider and compute the AIC.\nNext fit every possible model with 1 less predictor.\nCompare AIC scores to select the best model with 1 less predictor.\nRepeat steps 2 and 3 until you score the model with no predictors.\nCompare AIC among all tested models to select the best model.\n\n\n\nForward selection\nForward selection is the reverse of the backward elimination technique. Instead, of eliminating variables one-at-a-time, we add variables one-at-a-time until we cannot find any variables that improve the model any further.\nProcedure:\n\nStart with a model that has no predictors.\nNext fit every possible model with 1 additional predictor and score each model.\nCompare AIC scores to select the best model with 1 additional predictor.\nRepeat steps 2 and 3 until you score the model with all available predictors.\nCompare AIC among all tested models to select the best model."
  },
  {
    "objectID": "ae/ae9.html#example",
    "href": "ae/ae9.html#example",
    "title": "Model Selection",
    "section": "Example",
    "text": "Example\n\nExercise\n\nWill forward selection and backward elimination always yield the same model? Type your answer below before running any code.\nNext, see if you are right in using the data set below.\n\nSolution below\n\ntest_df = read_csv(\"https://sta101-fa22.netlify.app/static/appex/data/test_df.csv\")\n\nRows: 20 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): y, x1, x2, x3\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nIn the following two examples, we will use stepwise selection to build a main effects model.\nPerform 1 step of forward selection. What variable will be in the final forward selection model?\n\nAnswer: \\(x_1\\)\n\n\nlinear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(y ~ x1, data = test_df) %&gt;%\n  glance() %&gt;%\n  pull(AIC)\n\n[1] 93.08637\n\nlinear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(y ~ x2, data = test_df) %&gt;%\n  glance() %&gt;%\n  pull(AIC)\n\n[1] 131.8917\n\nlinear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(y ~ x3, data = test_df) %&gt;%\n  glance() %&gt;%\n  pull(AIC)\n\n[1] 152.9043\n\n\nNext, perform 1 step of backward elimination. Which variable will not be in the final backward elimination model?\n\nAnswer: \\(x_1\\)\n\n\nlinear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(y ~ x2 + x3, data = test_df) %&gt;%\n  glance() %&gt;%\n  pull(AIC)\n\n[1] 35.25949\n\nlinear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(y ~ x1 + x3, data = test_df) %&gt;%\n  glance() %&gt;%\n  pull(AIC)\n\n[1] 94.2536\n\nlinear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(y ~ x1 + x2, data = test_df) %&gt;%\n  glance() %&gt;%\n  pull(AIC)\n\n[1] 87.1686"
  },
  {
    "objectID": "ae/ae9.html#footnotes",
    "href": "ae/ae9.html#footnotes",
    "title": "Model Selection",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nthis definition is from Introduction to Modern Statistics.↩︎"
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "Course overview",
    "section": "",
    "text": "This is the homepage for STA 101 - Data Analysis and Statistical Inference taught by Dr. Mine Çetinkaya-Rundel in Fall 2023 at Duke University. All course materials will be posted on this site.\nYou can find the course syllabus here and the course schedule here."
  },
  {
    "objectID": "course-overview.html#class-meetings",
    "href": "course-overview.html#class-meetings",
    "title": "Course overview",
    "section": "Class meetings",
    "text": "Class meetings\n\n\n\nMeeting\nLocation\nTime\n\n\n\n\nLectures\nGross Hall 103\nMon & Wed 1:25 - 2:40 pm\n\n\nLab 1\nPerkins LINK 087 (Classroom 3)\nFri 8:30 - 9:45 am\n\n\nLab 2\nPerkins LINK 087 (Classroom 3)\nFri 10:05 - 11:20 am\n\n\nLab 3\nPerkins LINK 087 (Classroom 3)\nFri 11:45 am - 1:00 pm\n\n\nLab 4\nPerkins LINK 087 (Classroom 3)\nFri 1:25 - 2:40 pm"
  },
  {
    "objectID": "course-overview.html#license",
    "href": "course-overview.html#license",
    "title": "Course overview",
    "section": "License",
    "text": "License\n\nThis online work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International. Visit here for more information about the license."
  }
]