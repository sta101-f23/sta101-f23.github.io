{
  "hash": "1c4427b03b0d48c0568404624342a6bd",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Confidence intervals with bootstrapping\nsubtitle: Lecture 13\nformat: revealjs\nauto-stretch: false\n---\n\n\n\n# Warm up\n\n## Announcements\n\n-   ...\n\n## Packages\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n✔ broom        1.0.5     ✔ rsample      1.2.0\n✔ dials        1.2.0     ✔ tune         1.1.2\n✔ infer        1.0.4     ✔ workflows    1.1.3\n✔ modeldata    1.2.0     ✔ workflowsets 1.0.1\n✔ parsnip      1.1.1     ✔ yardstick    1.2.0\n✔ recipes      1.0.8     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(openintro)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: airports\nLoading required package: cherryblossom\nLoading required package: usdata\n\nAttaching package: 'openintro'\n\nThe following object is masked from 'package:modeldata':\n\n    ames\n```\n\n\n:::\n:::\n\n\n\n# **Inference with mathematical models**\n\n## That familiar shape...\n\n::: task\nDescribe the shape of the distributions.\n:::\n\n![](images/FourCaseStudies.png){fig-align=\"center\" width=\"700\"}\n\n## It's not happenstance!\n\nIt's the **Central Limit Theorem**, which says that the distribution of the sample statistic is normal, if certain conditions are met.\n\n. . .\n\n::: task\nTwo questions:\n\n1.  What do we mean by the distribution of the sample statistic?\n2.  What conditions need to be met?\n:::\n\n## The distribution of the sample statistic\n\n::: incremental\n-   You can build the distribution of the sample statistic by repeatedly taking samples of size n (original sample size) from the population and calculating and recording the sample statistic for each of these samples.\n\n-   But, you would never do this in reality!\n\n-   You'd either use simulation (randomization, bootstrapping, stuff we've done so far!) or you would leverage mathematical theory to know what to expect if we *had* taken repeated samples.\n:::\n\n## The (technical) conditions \n\n1.  **Independent observations:** Observations in the sample are independent. Independence is guaranteed when we take a random sample from a population. Independence can also be guaranteed if we randomly divide individuals into treatment and control groups.\n\n2.  **Large enough sample:** The sample size cannot be too small. What qualifies as \\\"small\\\" differs from one context (i.e., from sample statistic to sample statistic).\n\n## More to the CLT\n\n-   There is more to the CLT than just the **shape** of the distribution -- normal.\n\n-   The CLT says that the **center** of the sampling distribution will be at the true population parameter.\n\n-   The CLT also says something about the **spread** of the sampling distribution, measured by the **standard error**. For each sample statistic ($\\bar{x}$ -- the sample mean, $\\hat{p}$ -- the sample proportion, $\\bar{x}_1 - \\bar{x}_2$ -- the difference in sample means, etc.) the CLT provides a formula for its standard error.\n\n    -   You won't be asked to memorize these formulas.\n\n    -   In fact, you'll rarely use the CLT to calculate the variability of sample statistics, you'll simulate their distributions directly.\n\n# Normal distribution\n\n## Normal distributions\n\n::: task\nHow are these normal distributions similar? How are they different? Which one is $N(\\mu = 0, \\sigma. 1)$ and which $N(\\mu = 19, \\sigma = 4)$?\n:::\n\n![](images/twoSampleNormals.png){fig-align=\"center\" width=\"600\"}\n\n![](images/twoSampleNormalsStacked.png){fig-align=\"center\" width=\"599\"}\n\n## **The 68-95-99.7 rule**\n\nThe normal distribution is not just *any* unimodal and symmetric distribution, it follows the **68-95-99.7 rule**.\n\n![](images/er6895997.png){fig-align=\"center\"}\n\n## Using the normal distribution...\n\n-   To make decisions $\\rightarrow$ hypothesis testing\n\n    -   Use properties of the normal distribution to determine the probability of the observed sample statistic (or something more extreme, in the direction of the alternative hypothesis), i.e. the p-value\n\n. . .\n\n-   To make estimations $\\rightarrow$ confidence intervals\n\n    -   Use properties of the normal distribution to calculate the bounds of the confidence interval, adding and subtracting a margin of error to the observed sample statistic\n\n. . .\n\nBut before then...\n\n## Application exercise\n\nGo to Posit Cloud and continue the project titled `ae-11-Bone density`.\n",
    "supporting": [
      "15-inference-mathematical-models_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}